"""KV Cache SSD ä¼ è¾“æ—¶é—´ä¸“ç”¨åˆ†æå™¨
ä¸“é—¨è®¡ç®—å’Œç»Ÿè®¡ï¼š
1. SSD â†’ DRAM çš„åŠ è½½æ—¶é—´
2. DRAM â†’ GPU çš„ä¼ è¾“æ—¶é—´  
3. GPU â†’ DRAM çš„ä¿å­˜æ—¶é—´
4. DRAM â†’ SSD çš„å¸è½½æ—¶é—´
"""
import math
import argparse, time, pathlib
from contextlib import contextmanager
import torch, csv, os
from llama3.generator import LLaMA
from llama3.layers import PerformanceTracker    
from llama3.kv_offload import KVOffloader, BLOCK 
from torch.cuda import Event, current_stream
import threading

# ---------- å…¨å±€ç»Ÿè®¡æ•°æ® ----------
KV_TRANSFER_STATS = {
    # SSD ç›¸å…³
    'ssd_to_dram_us': 0,        # SSD â†’ DRAM åŠ è½½æ—¶é—´
    'dram_to_ssd_us': 0,        # DRAM â†’ SSD å¸è½½æ—¶é—´
    'ssd_to_dram_bytes': 0,     # SSD â†’ DRAM ä¼ è¾“å­—èŠ‚æ•°
    'dram_to_ssd_bytes': 0,     # DRAM â†’ SSD ä¼ è¾“å­—èŠ‚æ•°
    'ssd_to_dram_count': 0,     # SSD â†’ DRAM æ“ä½œæ¬¡æ•°
    'dram_to_ssd_count': 0,     # DRAM â†’ SSD æ“ä½œæ¬¡æ•°
    
    # DRAM â†” GPU ç›¸å…³
    'dram_to_gpu_us': 0,        # DRAM â†’ GPU ä¼ è¾“æ—¶é—´
    'gpu_to_dram_us': 0,        # GPU â†’ DRAM ä¼ è¾“æ—¶é—´
    'dram_to_gpu_bytes': 0,     # DRAM â†’ GPU ä¼ è¾“å­—èŠ‚æ•°
    'gpu_to_dram_bytes': 0,     # GPU â†’ DRAM ä¼ è¾“å­—èŠ‚æ•°
    'dram_to_gpu_count': 0,     # DRAM â†’ GPU æ“ä½œæ¬¡æ•°
    'gpu_to_dram_count': 0,     # GPU â†’ DRAM æ“ä½œæ¬¡æ•°
    
    # å…¶ä»–ç»Ÿè®¡
    'kv_cache_hit_count': 0,    # Cache å‘½ä¸­æ¬¡æ•°
    'kv_cache_miss_count': 0,   # Cache æœªå‘½ä¸­æ¬¡æ•°
}

_PATCHES_APPLIED = False

# ---------- è®¡æ—¶å·¥å…· ----------
@contextmanager
def precise_timer(stat_key: str, byte_key: str = None, count_key: str = None, bytes_transferred: int = 0):
    """ç²¾ç¡®è®¡æ—¶å™¨ï¼ŒåŒæ—¶ç»Ÿè®¡æ—¶é—´ã€å­—èŠ‚æ•°å’Œæ“ä½œæ¬¡æ•°"""
    start_time = time.perf_counter()
    yield
    end_time = time.perf_counter()
    
    elapsed_us = int((end_time - start_time) * 1e6)
    KV_TRANSFER_STATS[stat_key] += elapsed_us
    
    if byte_key and bytes_transferred > 0:
        KV_TRANSFER_STATS[byte_key] += bytes_transferred
    
    if count_key:
        KV_TRANSFER_STATS[count_key] += 1

@contextmanager
def cuda_precise_timer(stat_key: str, byte_key: str = None, count_key: str = None, bytes_transferred: int = 0):
    """CUDA ç²¾ç¡®è®¡æ—¶å™¨"""
    if not torch.cuda.is_available():
        with precise_timer(stat_key, byte_key, count_key, bytes_transferred):
            yield
        return
    
    start_event = Event(enable_timing=True)
    end_event = Event(enable_timing=True)
    
    start_event.record()
    yield
    end_event.record()
    torch.cuda.synchronize()
    
    elapsed_us = int(start_event.elapsed_time(end_event) * 1000)
    KV_TRANSFER_STATS[stat_key] += elapsed_us
    
    if byte_key and bytes_transferred > 0:
        KV_TRANSFER_STATS[byte_key] += bytes_transferred
    
    if count_key:
        KV_TRANSFER_STATS[count_key] += 1

def sizeof_fmt(num, suffix='B'):
    for unit in ['', 'K', 'M', 'G', 'T', 'P']:
        if abs(num) < 1024.0:
            return f"{num:6.1f} {unit}{suffix}"
        num /= 1024.0
    return f"{num:.1f} E{suffix}"

def bandwidth_fmt(bytes_val, time_us):
    """è®¡ç®—å¹¶æ ¼å¼åŒ–å¸¦å®½"""
    if time_us <= 0:
        return "N/A"
    
    time_s = time_us / 1e6
    bandwidth_bps = bytes_val / time_s
    return sizeof_fmt(bandwidth_bps, 'B/s')

# ---------- KV Offloader è¯¦ç»† Patch ----------
# ---------- KV Offloader è¯¦ç»† Patchï¼ˆå…¼å®¹ OptimizedKVOffloader & æ—§ç‰ˆï¼‰ ----------
def patch_kv_offloader_detailed():
    """
    æ‰“è¡¥ä¸ä»¥ç»Ÿè®¡ GPUâ†”DRAM åŠ SSDâ†”DRAM ä¼ è¾“æ—¶é—´ã€‚
    ä»…ä¿®æ”¹å‡½æ•°å°è£…ï¼Œä¸æ”¹ä»»ä½•è¾“å‡ºéƒ¨åˆ†ã€‚
    """
    global _PATCHES_APPLIED
    if _PATCHES_APPLIED:
        return

    import llama3.kv_offload as kvmod          # ç»Ÿä¸€å…¥å£
    KVCls = getattr(kvmod, "OptimizedKVOffloader",
                    getattr(kvmod, "KVOffloader", None))
    if KVCls is None:
        raise RuntimeError("âš ï¸  æœªæ‰¾åˆ° KV offloader ç±»")

    # -------- ä¿å­˜åŸå§‹æ–¹æ³• --------
    orig_push  = KVCls.push
    orig_fetch = KVCls.fetch
    orig_spill = getattr(KVCls, "_spill_to_ssd",  None)
    # æ–°ç‰ˆåŒæ­¥åŠ è½½å« _load_from_ssd_syncï¼Œæ—§ç‰ˆå« _load_from_ssd
    orig_load  = getattr(KVCls, "_load_from_ssd_sync",
                         getattr(KVCls, "_load_from_ssd", None))

    # ç»Ÿä¸€å—å¤§å°ï¼Œç”¨äºä¼°ç®—å­—èŠ‚
    def _blk_bytes(self):
        return getattr(self, "block_nbytes",
               getattr(self, "blk_bytes", 0))

    # ---------- GPU â†’ DRAM ----------
    def wrapped_push(self, layer, blk, k, v):
        bytes_tx = k.nbytes + v.nbytes
        with cuda_precise_timer('gpu_to_dram_us',
                                'gpu_to_dram_bytes',
                                'gpu_to_dram_count',
                                bytes_tx):
            return orig_push(self, layer, blk, k, v)

    # ---------- DRAM (+SSD) â†’ GPU ----------
    # ---------- DRAM (+SSD) â†’ GPU ----------
    def wrapped_fetch(self, layer, blocks):
        blk_list = (blocks.tolist() if torch.is_tensor(blocks) else list(blocks))
        need_ssd = [b for b in blk_list if getattr(self, "on_ssd")[layer][b]]

        # ---------- å¦‚éœ€ä» SSD è¯»å–ï¼Œå¼ºåˆ¶åŒæ­¥ ----------
        if need_ssd:
            bytes_from_ssd = len(need_ssd) * _blk_bytes(self)
            with precise_timer('ssd_to_dram_us',
                               'ssd_to_dram_bytes',
                               'ssd_to_dram_count',
                               bytes_from_ssd):
                for b in need_ssd:           # é€å—åŒæ­¥è¯»å–ï¼Œä¿è¯å¯ç”¨
                    if hasattr(self, "_load_from_ssd_sync"):
                        self._load_from_ssd_sync(layer, b)
                    else:                    # å…¼å®¹æ—§å
                        self._load_from_ssd(layer, b)
            KV_TRANSFER_STATS['kv_cache_miss_count'] += 1
        else:
            KV_TRANSFER_STATS['kv_cache_hit_count']  += 1

        # ---------- ç°åœ¨å—å·²åœ¨ DRAMï¼Œå®‰å…¨æ‰§è¡ŒåŸ fetch ----------
        k, v = orig_fetch(self, layer, blocks)

        # ---------- DRAM â†’ GPU è®¡æ—¶ ----------
        if k is not None and v is not None:
            with cuda_precise_timer('dram_to_gpu_us',
                                    'dram_to_gpu_bytes',
                                    'dram_to_gpu_count',
                                    k.nbytes + v.nbytes):
                pass
        return k, v


    # ---------- DRAM â†’ SSD ----------
    def wrapped_spill(self, layer, blk, *args, **kwargs):
        with precise_timer('dram_to_ssd_us',
                           'dram_to_ssd_bytes',
                           'dram_to_ssd_count',
                           _blk_bytes(self)):
            return orig_spill(self, layer, blk, *args, **kwargs)

    # ---------- SSD â†’ DRAMï¼ˆæ˜¾å¼åŒæ­¥åŠ è½½ APIï¼‰ ----------
    def wrapped_load(self, layer, blk, *args, **kwargs):
        with precise_timer('ssd_to_dram_us',
                           'ssd_to_dram_bytes',
                           'ssd_to_dram_count',
                           _blk_bytes(self)):
            return orig_load(self, layer, blk, *args, **kwargs)

    # -------- åº”ç”¨è¡¥ä¸ï¼ˆä»…å½“å¯¹åº”æ–¹æ³•å­˜åœ¨ï¼‰--------
    KVCls.push   = wrapped_push
    KVCls.fetch  = wrapped_fetch
    if orig_spill:
        KVCls._spill_to_ssd      = wrapped_spill
    if orig_load:
        if hasattr(KVCls, "_load_from_ssd_sync"):
            KVCls._load_from_ssd_sync = wrapped_load
        else:
            KVCls._load_from_ssd      = wrapped_load

    _PATCHES_APPLIED = True
    print("âœ… KV offloader detailed patches applied (auto-compatible)")

def _make_perf_tracker_thread_safe():
    """
    æŠŠ llama3.layers.PERF_TRACKER.lock æ¢æˆ RLockï¼Œ
    å¹¶è®© add_layer_stat åœ¨æ‹¿ä¸åˆ°é”æ—¶ç›´æ¥ç•¥è¿‡è¿™ä¸€æ¬¡ç´¯åŠ ï¼Œ
    ä»¥å… GPU åŒæ­¥æœŸé—´é€ æˆå…¨å±€é˜»å¡ã€‚
    """
    import llama3.layers as layermod
    import threading

    tracker = layermod.PERF_TRACKER
    tracker.lock = threading.RLock()          # 1. æ¢æˆå¯é‡å…¥é”

    orig_add = tracker.add_layer_stat

    def safe_add(self, layer_id, stat_name, value):
        # 2. å°è¯• 0.5ms æ‹¿é”ï¼›æ‹¿ä¸åˆ°å°±æ”¾å¼ƒæœ¬æ¬¡ç»Ÿè®¡
        locked = self.lock.acquire(timeout=5e-4)
        if not locked:
            return
        try:
            return orig_add(layer_id, stat_name, value)
        finally:
            self.lock.release()

    # 3. æ›¿æ¢æ–¹æ³•
    layermod.PerformanceTracker.add_layer_stat = safe_add

_make_perf_tracker_thread_safe()

# ---------- ç»Ÿè®¡æŠ¥å‘Š ----------
def print_transfer_report():
    """æ‰“å°è¯¦ç»†çš„ä¼ è¾“ç»Ÿè®¡æŠ¥å‘Š"""
    stats = KV_TRANSFER_STATS
    
    print("\n" + "="*80)
    print("ğŸ·ï¸  KV Cache Transfer Analysis Report")
    print("="*80)
    
    # SSD â†” DRAM ç»Ÿè®¡
    print(f"\nğŸ’¾ SSD â†” DRAM Transfers:")
    print(f"   SSD â†’ DRAM:")
    print(f"     Operations: {stats['ssd_to_dram_count']:,}")
    print(f"     Total time: {stats['ssd_to_dram_us']/1000:.1f} ms")
    print(f"     Total bytes: {sizeof_fmt(stats['ssd_to_dram_bytes'])}")
    if stats['ssd_to_dram_us'] > 0:
        print(f"     Bandwidth: {bandwidth_fmt(stats['ssd_to_dram_bytes'], stats['ssd_to_dram_us'])}")
        print(f"     Avg time/op: {stats['ssd_to_dram_us']/max(1, stats['ssd_to_dram_count']):.0f} Î¼s")
    
    print(f"\n   DRAM â†’ SSD:")
    print(f"     Operations: {stats['dram_to_ssd_count']:,}")
    print(f"     Total time: {stats['dram_to_ssd_us']/1000:.1f} ms")
    print(f"     Total bytes: {sizeof_fmt(stats['dram_to_ssd_bytes'])}")
    if stats['dram_to_ssd_us'] > 0:
        print(f"     Bandwidth: {bandwidth_fmt(stats['dram_to_ssd_bytes'], stats['dram_to_ssd_us'])}")
        print(f"     Avg time/op: {stats['dram_to_ssd_us']/max(1, stats['dram_to_ssd_count']):.0f} Î¼s")
    
    # DRAM â†” GPU ç»Ÿè®¡
    print(f"\nğŸ–¥ï¸  DRAM â†” GPU Transfers:")
    print(f"   DRAM â†’ GPU:")
    print(f"     Operations: {stats['dram_to_gpu_count']:,}")
    print(f"     Total time: {stats['dram_to_gpu_us']/1000:.1f} ms")
    print(f"     Total bytes: {sizeof_fmt(stats['dram_to_gpu_bytes'])}")
    if stats['dram_to_gpu_us'] > 0:
        print(f"     Bandwidth: {bandwidth_fmt(stats['dram_to_gpu_bytes'], stats['dram_to_gpu_us'])}")
        print(f"     Avg time/op: {stats['dram_to_gpu_us']/max(1, stats['dram_to_gpu_count']):.0f} Î¼s")
    
    print(f"\n   GPU â†’ DRAM:")
    print(f"     Operations: {stats['gpu_to_dram_count']:,}")
    print(f"     Total time: {stats['gpu_to_dram_us']/1000:.1f} ms")
    print(f"     Total bytes: {sizeof_fmt(stats['gpu_to_dram_bytes'])}")
    if stats['gpu_to_dram_us'] > 0:
        print(f"     Bandwidth: {bandwidth_fmt(stats['gpu_to_dram_bytes'], stats['gpu_to_dram_us'])}")
        print(f"     Avg time/op: {stats['gpu_to_dram_us']/max(1, stats['gpu_to_dram_count']):.0f} Î¼s")
    
    # Cache æ•ˆç‡ç»Ÿè®¡
    total_cache_ops = stats['kv_cache_hit_count'] + stats['kv_cache_miss_count']
    if total_cache_ops > 0:
        hit_ratio = stats['kv_cache_hit_count'] / total_cache_ops * 100
        print(f"\nğŸ¯ Cache Efficiency:")
        print(f"     Cache hits: {stats['kv_cache_hit_count']:,} ({hit_ratio:.1f}%)")
        print(f"     Cache misses: {stats['kv_cache_miss_count']:,} ({100-hit_ratio:.1f}%)")
        print(f"     Hit ratio: {hit_ratio:.1f}%")
    
    # æ€»ä½“ç»Ÿè®¡
    total_time = (stats['ssd_to_dram_us'] + stats['dram_to_ssd_us'] + 
                  stats['dram_to_gpu_us'] + stats['gpu_to_dram_us'])
    total_bytes = (stats['ssd_to_dram_bytes'] + stats['dram_to_ssd_bytes'] + 
                   stats['dram_to_gpu_bytes'] + stats['gpu_to_dram_bytes'])
    
    print(f"\nğŸ“Š Overall Transfer Summary:")
    print(f"     Total transfer time: {total_time/1000:.1f} ms")
    print(f"     Total bytes transferred: {sizeof_fmt(total_bytes)}")
    if total_time > 0:
        print(f"     Overall bandwidth: {bandwidth_fmt(total_bytes, total_time)}")
    
    # æ—¶é—´åˆ†å¸ƒ
    if total_time > 0:
        print(f"\nâ±ï¸  Time Distribution:")
        print(f"     SSD operations: {(stats['ssd_to_dram_us'] + stats['dram_to_ssd_us'])/total_time*100:.1f}%")
        print(f"     GPU operations: {(stats['dram_to_gpu_us'] + stats['gpu_to_dram_us'])/total_time*100:.1f}%")
    
    print("="*80)

# ---------- ä¸»å‡½æ•° ----------
def main():
    ap = argparse.ArgumentParser(description="KV Cache SSD Transfer Profiler")
    ap.add_argument("--model-path", required=True, help="Path to LLaMA model")
    ap.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    ap.add_argument("--prompt", default="The quick brown fox jumps over the lazy dog. This is a longer prompt to generate more KV cache activity.")
    ap.add_argument("--max-gen-len", type=int, default=128, help="Number of tokens to generate")
    ap.add_argument("--topk-blk", type=int, default=4, help="Number of KV blocks to keep in GPU")
    ap.add_argument("--csv", help="Save results to CSV file")
    ap.add_argument("--runs", type=int, default=1, help="Number of inference runs")
    args = ap.parse_args()

    ckpt = pathlib.Path(args.model_path)
    
    print(f"ğŸš€ KV Cache SSD Transfer Profiler")
    print(f"ğŸ“ Model: {ckpt.name}")
    print(f"ğŸ–¥ï¸  Device: {args.device}")
    print(f"ğŸ“ Prompt length: {len(args.prompt)} chars")
    print(f"ğŸ”¢ Generation length: {args.max_gen_len}")
    print(f"ğŸ’¾ KV blocks in GPU: {args.topk_blk}")
    print(f"ğŸ”„ Number of runs: {args.runs}")
    
    # åº”ç”¨ patches
    patch_kv_offloader_detailed()
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ Loading model...")
    llama = LLaMA.build(ckpt, load_model=True, device="cpu")
    
    # é…ç½® KV cache
    for layer in llama.model.layers:
        if hasattr(layer, 'attention') and hasattr(layer.attention, 'topk_blk'):
            layer.attention.topk_blk = args.topk_blk
    
    # è½¬ç§»åˆ° GPU
    if args.device.startswith("cuda"):
        print("ğŸ”„ Transferring to GPU...")
        llama.model.to(args.device)
        torch.cuda.synchronize()
    
    llama.args.device = args.device
    
    # é…ç½® offloader streams
    for blk in llama.model.layers:
        if hasattr(blk, 'attention') and hasattr(blk.attention, 'offloader'):
            off = blk.attention.offloader
            off.device = args.device
            if torch.cuda.is_available():
                off.copy_stream = torch.cuda.Stream(device=args.device)
    
    print("âœ… Model ready for inference")
    
    # æ‰§è¡Œå¤šæ¬¡æ¨ç†
    total_inference_time = 0
    
    for run in range(args.runs):
        print(f"\nğŸ§  Running inference {run+1}/{args.runs}...")
        
        # é‡ç½®å½“å‰è¿è¡Œçš„ç»Ÿè®¡
        run_stats = {k: v for k, v in KV_TRANSFER_STATS.items()}
        
        start_time = time.perf_counter()
        result = llama.text_completion([args.prompt], max_gen_len=args.max_gen_len)
        end_time = time.perf_counter()
        
        run_time = (end_time - start_time) * 1000
        total_inference_time += run_time
        
        # è®¡ç®—è¿™æ¬¡è¿è¡Œçš„ä¼ è¾“ç»Ÿè®¡
        run_transfer_time = 0
        for key in ['ssd_to_dram_us', 'dram_to_ssd_us', 'dram_to_gpu_us', 'gpu_to_dram_us']:
            run_transfer_time += KV_TRANSFER_STATS[key] - run_stats[key]
        
        print(f"   Inference time: {run_time:.1f} ms")
        print(f"   Transfer time: {run_transfer_time/1000:.1f} ms ({run_transfer_time/run_time/10:.1f}%)")
        
        if args.runs == 1:  # åªæœ‰ä¸€æ¬¡è¿è¡Œæ—¶æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬
            print(f"   Generated: {result[0][len(args.prompt):len(args.prompt)+50]}...")
    
    # æ‰“å°è¯¦ç»†æŠ¥å‘Š
    print_transfer_report()
    
    # æœ€ç»ˆç»Ÿè®¡
    avg_inference_time = total_inference_time / args.runs
    tokens_per_sec = args.max_gen_len / (avg_inference_time / 1000)
    
    print(f"\nğŸ“ˆ Performance Summary:")
    print(f"   Average inference time: {avg_inference_time:.1f} ms")
    print(f"   Tokens per second: {tokens_per_sec:.1f}")
    
    # ä¿å­˜åˆ° CSV
    if args.csv:
        headers = ['model', 'prompt_len', 'gen_len', 'topk_blk', 'runs', 'avg_inference_ms'] + list(KV_TRANSFER_STATS.keys())
        row = [ckpt.name, len(args.prompt), args.max_gen_len, args.topk_blk, args.runs, avg_inference_time] + list(KV_TRANSFER_STATS.values())
        
        file_exists = os.path.exists(args.csv)
        with open(args.csv, "a", newline="") as f:
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(headers)
            writer.writerow(row)
        print(f"\nğŸ“Š Results saved to {args.csv}")

if __name__ == "__main__":
    main()