"""KV Cache SSD ä¼ è¾“æ—¶é—´ä¸“ç”¨åˆ†æå™¨

è¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºåˆ†æ LLaMA æ¨¡å‹ KV Cache ä¼ è¾“æ€§èƒ½çš„å·¥å…·ã€‚ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

1. ç²¾ç¡®æµ‹é‡å„ç§ä¼ è¾“æ—¶é—´ï¼š
   - SSD â†’ DRAM çš„åŠ è½½æ—¶é—´
   - DRAM â†’ GPU çš„ä¼ è¾“æ—¶é—´  
   - GPU â†’ DRAM çš„ä¿å­˜æ—¶é—´
   - DRAM â†’ SSD çš„å¸è½½æ—¶é—´

2. ç»Ÿè®¡åˆ†æï¼š
   - ä¼ è¾“å¸¦å®½è®¡ç®—
   - Cache å‘½ä¸­ç‡ç»Ÿè®¡
   - å†…å­˜ä½¿ç”¨ç›‘æ§
   - æ‰¹å¤„ç†æ€§èƒ½åˆ†æ

3. å¯¼å‡ºåŠŸèƒ½ï¼š
   - è¯¦ç»†çš„æ§åˆ¶å°æŠ¥å‘Š
   - CSV æ ¼å¼æ•°æ®å¯¼å‡º
   - ç»“æ„åŒ–æ—¥å¿—è®°å½•

ä½¿ç”¨æ–¹æ³•ï¼š
    python profile_pipeline.py --model-path /path/to/model --prompt "Your prompt here"
    python profile_pipeline.py --model-path /path/to/model --prompt-file prompts.txt --csv results.csv

Author: LLaMA3 Project Team
License: MIT
"""
import math
import sys
import argparse
import time
import pathlib
from contextlib import contextmanager
import csv
import os
import threading
import logging
from typing import List, Iterator, Any, Optional, Dict, Tuple

# å®‰å…¨å¯¼å…¥ torch å’Œç›¸å…³æ¨¡å—
try:
    import torch
    from torch.cuda import Event, current_stream
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    Event = None
    current_stream = None

# å®‰å…¨å¯¼å…¥ llama3 æ¨¡å—
try:
    from llama3.generator import LLaMA
    from llama3.layers import PerformanceTracker    
    from llama3.kv_offload import KVOffloader
    LLAMA_AVAILABLE = True
except ImportError:
    LLAMA_AVAILABLE = False
    LLaMA = None
    PerformanceTracker = None
    KVOffloader = None

# ---------- å…¨å±€ç»Ÿè®¡æ•°æ® ----------
# ä½¿ç”¨ dict è€Œä¸æ˜¯æ™®é€šå˜é‡ä»¥æé«˜çº¿ç¨‹å®‰å…¨æ€§
KV_TRANSFER_STATS = {
    # SSD ç›¸å…³
    'ssd_to_dram_us': 0,        # SSD â†’ DRAM åŠ è½½æ—¶é—´
    'dram_to_ssd_us': 0,        # DRAM â†’ SSD å¸è½½æ—¶é—´
    'ssd_to_dram_bytes': 0,     # SSD â†’ DRAM ä¼ è¾“å­—èŠ‚æ•°
    'dram_to_ssd_bytes': 0,     # DRAM â†’ SSD ä¼ è¾“å­—èŠ‚æ•°
    'ssd_to_dram_count': 0,     # SSD â†’ DRAM æ“ä½œæ¬¡æ•°
    'dram_to_ssd_count': 0,     # DRAM â†’ SSD æ“ä½œæ¬¡æ•°
    
    # DRAM â†” GPU ç›¸å…³
    'dram_to_gpu_us': 0,        # DRAM â†’ GPU ä¼ è¾“æ—¶é—´
    'gpu_to_dram_us': 0,        # GPU â†’ DRAM ä¼ è¾“æ—¶é—´
    'dram_to_gpu_bytes': 0,     # DRAM â†’ GPU ä¼ è¾“å­—èŠ‚æ•°
    'gpu_to_dram_bytes': 0,     # GPU â†’ DRAM ä¼ è¾“å­—èŠ‚æ•°
    'dram_to_gpu_count': 0,     # DRAM â†’ GPU æ“ä½œæ¬¡æ•°
    'gpu_to_dram_count': 0,     # GPU â†’ DRAM æ“ä½œæ¬¡æ•°
    
    # å…¶ä»–ç»Ÿè®¡
    'kv_cache_hit_count': 0,    # Cache å‘½ä¸­æ¬¡æ•°
    'kv_cache_miss_count': 0,   # Cache æœªå‘½ä¸­æ¬¡æ•°
    
    # æ–°å¢æ€§èƒ½ç›‘æ§
    'total_memory_allocated': 0, # æ€»å†…å­˜åˆ†é…é‡
    'peak_memory_usage': 0,      # å³°å€¼å†…å­˜ä½¿ç”¨
    'memory_fragmentation': 0,   # å†…å­˜ç¢ç‰‡åŒ–ç¨‹åº¦
}

# ---------- Batch statistics ----------
BATCH_STATS = {
    'total_batches':   0,        # æ€»æ‰¹æ¬¡æ•°
    'total_prompts':   0,        # ç´¯è®¡ prompt æ¡æ•°
    'max_batch_size':  0,
    'min_batch_size':  1 << 30,
    'batch_sizes':     [],       # æ¯æ‰¹å®é™…å¤§å°ï¼Œå¯åšåˆ†å¸ƒåˆ†æ
}

_PATCHES_APPLIED = False

# ---------- æ—¥å¿—é…ç½® ----------
def setup_logging(verbose: bool = False) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—é…ç½®
    
    Args:
        verbose: æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º
        
    Returns:
        é…ç½®å¥½çš„ Logger å®ä¾‹
    """
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('profile_pipeline.log', mode='a')
        ]
    )
    return logging.getLogger(__name__)

# ---------- è¾…åŠ©å‡½æ•° ----------
def chunks(lst: List[Any], n: int) -> Iterator[List[Any]]:
    """å°†åˆ—è¡¨åˆ†å‰²æˆæŒ‡å®šå¤§å°çš„å—
    
    Args:
        lst: è¦åˆ†å‰²çš„åˆ—è¡¨
        n: æ¯ä¸ªå—çš„å¤§å°
        
    Yields:
        å¤§å°ä¸º n çš„åˆ—è¡¨å—ï¼ˆæœ€åä¸€å—å¯èƒ½å°äº nï¼‰
    """
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def check_dependencies(logger: logging.Logger) -> None:
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–æ˜¯å¦å¯ç”¨
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨
        
    Raises:
        RuntimeError: å½“ç¼ºå°‘å¿…è¦ä¾èµ–æ—¶æŠ›å‡º
    """
    errors = []
    
    if not TORCH_AVAILABLE:
        errors.append("PyTorch not available. Please install PyTorch.")
    
    if not LLAMA_AVAILABLE:
        errors.append("LLaMA3 modules not available. Please check your LLaMA3 installation.")
    
    if torch.cuda.is_available():
        try:
            torch.cuda.device_count()
            logger.info(f"CUDA available with {torch.cuda.device_count()} GPU(s)")
        except Exception as e:
            logger.warning(f"CUDA available but initialization failed: {e}")
    else:
        logger.info("CUDA not available, will use CPU")
    
    if errors:
        for error in errors:
            logger.error(f"âŒ {error}")
        raise RuntimeError(f"Missing dependencies: {'; '.join(errors)}")
    
    logger.info("âœ… All dependencies checked successfully")

def validate_and_process_prompts(prompts: List[str], max_seq_len: int = 2048, logger: logging.Logger = None) -> List[str]:
    """éªŒè¯å’Œå¤„ç†è¾“å…¥æç¤ºï¼Œç¡®ä¿å…¼å®¹æ€§
    
    Args:
        prompts: åŸå§‹æç¤ºåˆ—è¡¨
        max_seq_len: æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦
        logger: æ—¥å¿—è®°å½•å™¨
        
    Returns:
        å¤„ç†åçš„æç¤ºåˆ—è¡¨
    """
    if logger is None:
        logger = logging.getLogger(__name__)
    
    processed_prompts = []
    
    for i, prompt in enumerate(prompts):
        # æ£€æŸ¥prompté•¿åº¦
        original_length = len(prompt)
        
        # å¦‚æœpromptè¿‡é•¿ï¼Œè¿›è¡Œæˆªæ–­
        if original_length > max_seq_len * 3:  # ç²—ç•¥ä¼°ç®—ï¼Œ3ä¸ªå­—ç¬¦â‰ˆ1ä¸ªtoken
            logger.warning(f"Prompt {i+1} is very long ({original_length} chars), truncating...")
            prompt = prompt[:max_seq_len * 3]
            
        # æ£€æŸ¥æ˜¯å¦ä¸ºæµ‹è¯•æ•°æ®æ ¼å¼ï¼ˆå¦‚ p1_0 p1_1 ...ï¼‰
        if prompt.startswith(('p1_', 'p2_', 'p3_')) and ' ' in prompt:
            tokens = prompt.split()
            if len(tokens) > max_seq_len // 2:  # å¦‚æœtokenæ•°é‡è¿‡å¤š
                logger.warning(f"Prompt {i+1} appears to be test data with {len(tokens)} tokens, truncating to {max_seq_len//2}")
                # æˆªæ–­åˆ°åˆç†é•¿åº¦
                truncated_tokens = tokens[:max_seq_len//2]
                prompt = " ".join(truncated_tokens)
        
        # æ·»åŠ åŸºæœ¬å†…å®¹éªŒè¯
        if not prompt.strip():
            logger.warning(f"Prompt {i+1} is empty, using default text")
            prompt = "Hello world"
            
        processed_prompts.append(prompt)
        
        if len(prompt) != original_length:
            logger.debug(f"Prompt {i+1}: {original_length} -> {len(prompt)} chars")
    
    return processed_prompts

def safe_batch_processing(prompts: List[str], batch_size: int, max_seq_len: int = 2048, logger: logging.Logger = None) -> Iterator[List[str]]:
    """å®‰å…¨çš„æ‰¹å¤„ç†å‡½æ•°ï¼Œç¡®ä¿æ¯ä¸ªbatchç¬¦åˆæ¨¡å‹é™åˆ¶
    
    æ³¨æ„ï¼šç”±äºå½“å‰LLaMAå®ç°çš„é™åˆ¶ï¼Œæš‚æ—¶å¼ºåˆ¶batch_size=1ä»¥é¿å…shape mismatché”™è¯¯
    
    Args:
        prompts: å¤„ç†åçš„æç¤ºåˆ—è¡¨
        batch_size: æœŸæœ›çš„æ‰¹å¤„ç†å¤§å°
        max_seq_len: æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦
        logger: æ—¥å¿—è®°å½•å™¨
        
    Yields:
        å®‰å…¨çš„promptæ‰¹æ¬¡
    """
    if logger is None:
        logger = logging.getLogger(__name__)
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨çœŸæ­£çš„multi-batchæ”¯æŒ
    # åŸºäºæ¨¡å‹ä¿®æ”¹ï¼Œç°åœ¨æ”¯æŒçœŸæ­£çš„multi-batchå¤„ç†
    effective_batch_size = batch_size
    if batch_size > 1:
        logger.info(f"Using multi-batch processing with batch_size={batch_size}. "
                   f"This requires the updated LLaMA model implementation.")
    
    current_batch = []
    current_batch_tokens = 0
    
    for i, prompt in enumerate(prompts):
        # ä¼°ç®—å½“å‰promptçš„tokenæ•°é‡ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰
        estimated_tokens = len(prompt.split()) if ' ' in prompt else len(prompt) // 3
        
        # æ£€æŸ¥æ·»åŠ æ­¤promptæ˜¯å¦ä¼šè¶…å‡ºé™åˆ¶
        if (len(current_batch) >= effective_batch_size or 
            (current_batch_tokens + estimated_tokens > max_seq_len and current_batch)):
            
            if current_batch:
                logger.debug(f"Yielding batch with {len(current_batch)} prompts, ~{current_batch_tokens} tokens")
                yield current_batch
                current_batch = []
                current_batch_tokens = 0
        
        # å¦‚æœå•ä¸ªpromptè¿‡é•¿ï¼Œå‘å‡ºè­¦å‘Šä½†ä»ç„¶å¤„ç†
        if estimated_tokens > max_seq_len:
            logger.warning(f"Prompt {i+1} has ~{estimated_tokens} tokens, exceeds max_seq_len {max_seq_len}")
        
        current_batch.append(prompt)
        current_batch_tokens += estimated_tokens
    
    # å¤„ç†æœ€åä¸€ä¸ªbatch
    if current_batch:
        logger.debug(f"Yielding final batch with {len(current_batch)} prompts, ~{current_batch_tokens} tokens")
        yield current_batch

# ---------- è®¡æ—¶å·¥å…· ----------
@contextmanager
def precise_timer(stat_key: str, byte_key: str = None, count_key: str = None, bytes_transferred: int = 0):
    """ç²¾ç¡®è®¡æ—¶å™¨ï¼ŒåŒæ—¶ç»Ÿè®¡æ—¶é—´ã€å­—èŠ‚æ•°å’Œæ“ä½œæ¬¡æ•°"""
    start_time = time.perf_counter()
    success = False
    try:
        yield
        success = True
    except Exception as e:
        logging.getLogger(__name__).warning(f"Operation failed during timing: {e}")
        raise
    finally:
        try:
            end_time = time.perf_counter()
            elapsed_us = int((end_time - start_time) * 1e6)
            
            # åªæœ‰æ“ä½œæˆåŠŸæ—¶æ‰è®°å½•ç»Ÿè®¡
            if success and stat_key in KV_TRANSFER_STATS:
                KV_TRANSFER_STATS[stat_key] += elapsed_us
                
                if byte_key and bytes_transferred > 0 and byte_key in KV_TRANSFER_STATS:
                    KV_TRANSFER_STATS[byte_key] += bytes_transferred
                
                if count_key and count_key in KV_TRANSFER_STATS:
                    KV_TRANSFER_STATS[count_key] += 1
        except Exception as e:
            logging.getLogger(__name__).error(f"Error recording timing stats: {e}")

@contextmanager
def cuda_precise_timer(stat_key: str, byte_key: str = None, count_key: str = None, bytes_transferred: int = 0):
    """CUDA ç²¾ç¡®è®¡æ—¶å™¨"""
    if not torch.cuda.is_available():
        with precise_timer(stat_key, byte_key, count_key, bytes_transferred):
            yield
        return
    
    start_event = Event(enable_timing=True)
    end_event = Event(enable_timing=True)
    
    try:
        start_event.record()
        yield
        end_event.record()
        torch.cuda.synchronize()
        
        elapsed_us = int(start_event.elapsed_time(end_event) * 1000)
        KV_TRANSFER_STATS[stat_key] += elapsed_us
        
        if byte_key and bytes_transferred > 0:
            KV_TRANSFER_STATS[byte_key] += bytes_transferred
        
        if count_key:
            KV_TRANSFER_STATS[count_key] += 1
    except Exception as e:
        print(f"âš ï¸  CUDA timing error: {e}")
        # å›é€€åˆ° CPU è®¡æ—¶
        with precise_timer(stat_key, byte_key, count_key, bytes_transferred):
            pass

def sizeof_fmt(num: float, suffix: str = 'B') -> str:
    """æ ¼å¼åŒ–å­—èŠ‚æ•°æ˜¾ç¤º"""
    for unit in ['', 'K', 'M', 'G', 'T', 'P']:
        if abs(num) < 1024.0:
            return f"{num:6.1f} {unit}{suffix}"
        num /= 1024.0
    return f"{num:.1f} E{suffix}"

def bandwidth_fmt(bytes_val: int, time_us: int) -> str:
    """è®¡ç®—å¹¶æ ¼å¼åŒ–å¸¦å®½"""
    if time_us <= 0:
        return "N/A"
    
    time_s = time_us / 1e6
    bandwidth_bps = bytes_val / time_s
    return sizeof_fmt(bandwidth_bps, 'B/s')

def update_memory_stats():
    """æ›´æ–°å†…å­˜ä½¿ç”¨ç»Ÿè®¡"""
    if not torch.cuda.is_available():
        return
    
    try:
        current_memory = torch.cuda.memory_allocated()
        max_memory = torch.cuda.max_memory_allocated()
        
        KV_TRANSFER_STATS['total_memory_allocated'] = current_memory
        KV_TRANSFER_STATS['peak_memory_usage'] = max(
            KV_TRANSFER_STATS['peak_memory_usage'], 
            max_memory
        )
        
        # ç®€å•çš„å†…å­˜ç¢ç‰‡åŒ–æ£€æµ‹
        reserved = torch.cuda.memory_reserved()
        if reserved > 0:
            fragmentation = (reserved - current_memory) / reserved
            KV_TRANSFER_STATS['memory_fragmentation'] = fragmentation
            
    except Exception as e:
        logging.getLogger(__name__).debug(f"Memory stats update failed: {e}")

# ---------- KV Offloader è¯¦ç»† Patch ----------
def patch_kv_offloader_detailed():
    """
    æ‰“è¡¥ä¸ä»¥ç»Ÿè®¡ GPUâ†”DRAM åŠ SSDâ†”DRAM ä¼ è¾“æ—¶é—´ã€‚
    ä»…ä¿®æ”¹å‡½æ•°å°è£…ï¼Œä¸æ”¹ä»»ä½•è¾“å‡ºéƒ¨åˆ†ã€‚
    """
    global _PATCHES_APPLIED
    if _PATCHES_APPLIED:
        return

    try:
        import llama3.kv_offload as kvmod
    except ImportError:
        print("âš ï¸  æ— æ³•å¯¼å…¥ llama3.kv_offload æ¨¡å—")
        return

    # å°è¯•æ‰¾åˆ° KV offloader ç±»
    KVCls = None
    for cls_name in ["OptimizedKVOffloader", "KVOffloader"]:
        KVCls = getattr(kvmod, cls_name, None)
        if KVCls is not None:
            break
    
    if KVCls is None:
        print("âš ï¸  æœªæ‰¾åˆ° KV offloader ç±»")
        return

    # -------- ä¿å­˜åŸå§‹æ–¹æ³• --------
    orig_push = getattr(KVCls, 'push', None)
    orig_fetch = getattr(KVCls, 'fetch', None)
    orig_spill = getattr(KVCls, "_spill_to_ssd", None)
    
    # æ–°ç‰ˆåŒæ­¥åŠ è½½å« _load_from_ssd_syncï¼Œæ—§ç‰ˆå« _load_from_ssd
    orig_load = getattr(KVCls, "_load_from_ssd_sync", 
                       getattr(KVCls, "_load_from_ssd", None))

    # ç»Ÿä¸€å—å¤§å°ï¼Œç”¨äºä¼°ç®—å­—èŠ‚
    def _blk_bytes(self):
        return getattr(self, "block_nbytes",
               getattr(self, "blk_bytes", 0))

    # ---------- GPU â†’ DRAM ----------
    def wrapped_push(self, layer, blk, k, v):
        if not orig_push:
            return None
        
        try:
            bytes_tx = 0
            if hasattr(k, 'nbytes') and hasattr(v, 'nbytes'):
                bytes_tx = k.nbytes + v.nbytes
            
            with cuda_precise_timer('gpu_to_dram_us',
                                    'gpu_to_dram_bytes',
                                    'gpu_to_dram_count',
                                    bytes_tx):
                result = orig_push(self, layer, blk, k, v)
                update_memory_stats()  # æ›´æ–°å†…å­˜ç»Ÿè®¡
                return result
        except Exception as e:
            logging.getLogger(__name__).warning(f"Error in wrapped_push: {e}")
            return orig_push(self, layer, blk, k, v)

    # ---------- DRAM (+SSD) â†’ GPU ----------
    def wrapped_fetch(self, layer, blocks):
        if not orig_fetch:
            return None, None
        
        try:
            # å®‰å…¨åœ°è½¬æ¢ blocks ä¸ºåˆ—è¡¨
            if torch.is_tensor(blocks):
                blk_list = blocks.tolist()
            elif hasattr(blocks, '__iter__'):
                blk_list = list(blocks)
            else:
                blk_list = [blocks]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä» SSD åŠ è½½
            need_ssd = []
            if hasattr(self, "on_ssd") and layer < len(self.on_ssd):
                need_ssd = [b for b in blk_list 
                           if b < len(self.on_ssd[layer]) and self.on_ssd[layer][b]]

            # å¦‚éœ€ä» SSD è¯»å–ï¼Œå¼ºåˆ¶åŒæ­¥
            if need_ssd:
                bytes_from_ssd = len(need_ssd) * _blk_bytes(self)
                with precise_timer('ssd_to_dram_us',
                                   'ssd_to_dram_bytes',
                                   'ssd_to_dram_count',
                                   bytes_from_ssd):
                    for b in need_ssd:
                        if hasattr(self, "_load_from_ssd_sync"):
                            self._load_from_ssd_sync(layer, b)
                        elif hasattr(self, "_load_from_ssd"):
                            self._load_from_ssd(layer, b)
                KV_TRANSFER_STATS['kv_cache_miss_count'] += 1
            else:
                KV_TRANSFER_STATS['kv_cache_hit_count'] += 1

            # ç°åœ¨å—å·²åœ¨ DRAMï¼Œå®‰å…¨æ‰§è¡ŒåŸ fetch
            k, v = orig_fetch(self, layer, blocks)

            # DRAM â†’ GPU è®¡æ—¶
            if k is not None and v is not None:
                bytes_tx = 0
                if hasattr(k, 'nbytes') and hasattr(v, 'nbytes'):
                    bytes_tx = k.nbytes + v.nbytes
                
                with cuda_precise_timer('dram_to_gpu_us',
                                        'dram_to_gpu_bytes',
                                        'dram_to_gpu_count',
                                        bytes_tx):
                    pass
            return k, v
        except Exception as e:
            print(f"âš ï¸  Error in wrapped_fetch: {e}")
            return orig_fetch(self, layer, blocks)

    # ---------- DRAM â†’ SSD ----------
    def wrapped_spill(self, layer, blk, *args, **kwargs):
        if not orig_spill:
            return None
        
        try:
            with precise_timer('dram_to_ssd_us',
                               'dram_to_ssd_bytes',
                               'dram_to_ssd_count',
                               _blk_bytes(self)):
                return orig_spill(self, layer, blk, *args, **kwargs)
        except Exception as e:
            print(f"âš ï¸  Error in wrapped_spill: {e}")
            return orig_spill(self, layer, blk, *args, **kwargs)

    # ---------- SSD â†’ DRAMï¼ˆæ˜¾å¼åŒæ­¥åŠ è½½ APIï¼‰ ----------
    def wrapped_load(self, layer, blk, *args, **kwargs):
        if not orig_load:
            return None
        
        try:
            with precise_timer('ssd_to_dram_us',
                               'ssd_to_dram_bytes',
                               'ssd_to_dram_count',
                               _blk_bytes(self)):
                return orig_load(self, layer, blk, *args, **kwargs)
        except Exception as e:
            print(f"âš ï¸  Error in wrapped_load: {e}")
            return orig_load(self, layer, blk, *args, **kwargs)

    # -------- åº”ç”¨è¡¥ä¸ï¼ˆä»…å½“å¯¹åº”æ–¹æ³•å­˜åœ¨ï¼‰--------
    if orig_push:
        KVCls.push = wrapped_push
    if orig_fetch:
        KVCls.fetch = wrapped_fetch
    if orig_spill:
        KVCls._spill_to_ssd = wrapped_spill
    if orig_load:
        if hasattr(KVCls, "_load_from_ssd_sync"):
            KVCls._load_from_ssd_sync = wrapped_load
        else:
            KVCls._load_from_ssd = wrapped_load

    _PATCHES_APPLIED = True
    print("âœ… KV offloader detailed patches applied (auto-compatible)")

# ---------- LLaMA batch counter patch ----------
def patch_llama_batch_counter():
    """
    ç»™ llama3.generator.LLaMA.text_completion æ‰“è¡¥ä¸ï¼Œ
    åœ¨æ¯æ¬¡è°ƒç”¨æ—¶ç»Ÿè®¡ batch_size å¹¶å†™å…¥ BATCH_STATSã€‚
    """
    try:
        import llama3.generator as genmod
    except ImportError:
        print("âš ï¸  æ— æ³•å¯¼å…¥ llama3.generator æ¨¡å—")
        return

    if getattr(genmod, "_BATCH_PATCHED", False):
        return

    if not hasattr(genmod, 'LLaMA') or not hasattr(genmod.LLaMA, 'text_completion'):
        print("âš ï¸  LLaMA.text_completion æ–¹æ³•ä¸å­˜åœ¨")
        return

    orig_tc = genmod.LLaMA.text_completion

    def wrapped(self, prompts, *a, **kw):
        try:
            bsz = len(prompts) if prompts else 0
            BATCH_STATS['total_batches'] += 1
            BATCH_STATS['total_prompts'] += bsz
            BATCH_STATS['batch_sizes'].append(bsz)
            BATCH_STATS['max_batch_size'] = max(BATCH_STATS['max_batch_size'], bsz)
            BATCH_STATS['min_batch_size'] = min(BATCH_STATS['min_batch_size'], bsz)
            return orig_tc(self, prompts, *a, **kw)
        except Exception as e:
            print(f"âš ï¸  Error in batch counter: {e}")
            return orig_tc(self, prompts, *a, **kw)

    genmod.LLaMA.text_completion = wrapped
    genmod._BATCH_PATCHED = True
    print("âœ… LLaMA batch-size counter patched")

def _make_perf_tracker_thread_safe():
    """
    æŠŠ llama3.layers.PERF_TRACKER.lock æ¢æˆ RLockï¼Œ
    å¹¶è®© add_layer_stat åœ¨æ‹¿ä¸åˆ°é”æ—¶ç›´æ¥ç•¥è¿‡è¿™ä¸€æ¬¡ç´¯åŠ ï¼Œ
    ä»¥å… GPU åŒæ­¥æœŸé—´é€ æˆå…¨å±€é˜»å¡ã€‚
    """
    try:
        import llama3.layers as layermod
    except ImportError:
        print("âš ï¸  æ— æ³•å¯¼å…¥ llama3.layers æ¨¡å—")
        return

    if not hasattr(layermod, 'PERF_TRACKER'):
        print("âš ï¸  PERF_TRACKER ä¸å­˜åœ¨")
        return

    tracker = layermod.PERF_TRACKER
    tracker.lock = threading.RLock()

    if not hasattr(tracker, 'add_layer_stat'):
        return

    orig_add = tracker.add_layer_stat

    def safe_add(self, layer_id, stat_name, value):
        try:
            locked = self.lock.acquire(timeout=5e-4)
            if not locked:
                return
            try:
                return orig_add(layer_id, stat_name, value)
            finally:
                self.lock.release()
        except Exception as e:
            print(f"âš ï¸  Error in safe_add: {e}")

    layermod.PerformanceTracker.add_layer_stat = safe_add

# ---------- ç»Ÿè®¡æŠ¥å‘Š ----------
def print_transfer_report():
    """æ‰“å°è¯¦ç»†çš„ä¼ è¾“ç»Ÿè®¡æŠ¥å‘Š"""
    stats = KV_TRANSFER_STATS
    
    # ------- Batch summary -------
    if BATCH_STATS['total_batches']:
        avg_bs = BATCH_STATS['total_prompts'] / BATCH_STATS['total_batches']
        print("\nğŸ“‘ Batch Statistics")
        print(f"   Total batches : {BATCH_STATS['total_batches']}")
        print(f"   Total prompts : {BATCH_STATS['total_prompts']}")
        print(f"   Avg batch size: {avg_bs:.1f}")
        print(f"   Max batch size: {BATCH_STATS['max_batch_size']}")
        print(f"   Min batch size: {BATCH_STATS['min_batch_size']}")
        
    print("\n" + "="*80)
    print("ğŸ·ï¸  KV Cache Transfer Analysis Report")
    print("="*80)
    
    # SSD â†” DRAM ç»Ÿè®¡
    print(f"\nğŸ’¾ SSD â†” DRAM Transfers:")
    print(f"   SSD â†’ DRAM:")
    print(f"     Operations: {stats['ssd_to_dram_count']:,}")
    print(f"     Total time: {stats['ssd_to_dram_us']/1000:.1f} ms")
    print(f"     Total bytes: {sizeof_fmt(stats['ssd_to_dram_bytes'])}")
    if stats['ssd_to_dram_us'] > 0:
        print(f"     Bandwidth: {bandwidth_fmt(stats['ssd_to_dram_bytes'], stats['ssd_to_dram_us'])}")
        print(f"     Avg time/op: {stats['ssd_to_dram_us']/max(1, stats['ssd_to_dram_count']):.0f} Î¼s")
    
    print(f"\n   DRAM â†’ SSD:")
    print(f"     Operations: {stats['dram_to_ssd_count']:,}")
    print(f"     Total time: {stats['dram_to_ssd_us']/1000:.1f} ms")
    print(f"     Total bytes: {sizeof_fmt(stats['dram_to_ssd_bytes'])}")
    if stats['dram_to_ssd_us'] > 0:
        print(f"     Bandwidth: {bandwidth_fmt(stats['dram_to_ssd_bytes'], stats['dram_to_ssd_us'])}")
        print(f"     Avg time/op: {stats['dram_to_ssd_us']/max(1, stats['dram_to_ssd_count']):.0f} Î¼s")
    
    # DRAM â†” GPU ç»Ÿè®¡
    print(f"\nğŸ–¥ï¸  DRAM â†” GPU Transfers:")
    print(f"   DRAM â†’ GPU:")
    print(f"     Operations: {stats['dram_to_gpu_count']:,}")
    print(f"     Total time: {stats['dram_to_gpu_us']/1000:.1f} ms")
    print(f"     Total bytes: {sizeof_fmt(stats['dram_to_gpu_bytes'])}")
    if stats['dram_to_gpu_us'] > 0:
        print(f"     Bandwidth: {bandwidth_fmt(stats['dram_to_gpu_bytes'], stats['dram_to_gpu_us'])}")
        print(f"     Avg time/op: {stats['dram_to_gpu_us']/max(1, stats['dram_to_gpu_count']):.0f} Î¼s")
    
    print(f"\n   GPU â†’ DRAM:")
    print(f"     Operations: {stats['gpu_to_dram_count']:,}")
    print(f"     Total time: {stats['gpu_to_dram_us']/1000:.1f} ms")
    print(f"     Total bytes: {sizeof_fmt(stats['gpu_to_dram_bytes'])}")
    if stats['gpu_to_dram_us'] > 0:
        print(f"     Bandwidth: {bandwidth_fmt(stats['gpu_to_dram_bytes'], stats['gpu_to_dram_us'])}")
        print(f"     Avg time/op: {stats['gpu_to_dram_us']/max(1, stats['gpu_to_dram_count']):.0f} Î¼s")
    
    # Cache æ•ˆç‡ç»Ÿè®¡
    total_cache_ops = stats['kv_cache_hit_count'] + stats['kv_cache_miss_count']
    if total_cache_ops > 0:
        hit_ratio = stats['kv_cache_hit_count'] / total_cache_ops * 100
        print(f"\nğŸ¯ Cache Efficiency:")
        print(f"     Cache hits: {stats['kv_cache_hit_count']:,} ({hit_ratio:.1f}%)")
        print(f"     Cache misses: {stats['kv_cache_miss_count']:,} ({100-hit_ratio:.1f}%)")
        print(f"     Hit ratio: {hit_ratio:.1f}%")
    
    # æ€»ä½“ç»Ÿè®¡
    total_time = (stats['ssd_to_dram_us'] + stats['dram_to_ssd_us'] + 
                  stats['dram_to_gpu_us'] + stats['gpu_to_dram_us'])
    total_bytes = (stats['ssd_to_dram_bytes'] + stats['dram_to_ssd_bytes'] + 
                   stats['dram_to_gpu_bytes'] + stats['gpu_to_dram_bytes'])
    
    print(f"\nğŸ“Š Overall Transfer Summary:")
    print(f"     Total transfer time: {total_time/1000:.1f} ms")
    print(f"     Total bytes transferred: {sizeof_fmt(total_bytes)}")
    if total_time > 0:
        print(f"     Overall bandwidth: {bandwidth_fmt(total_bytes, total_time)}")
    
    # æ—¶é—´åˆ†å¸ƒ
    if total_time > 0:
        print(f"\nâ±ï¸  Time Distribution:")
        print(f"     SSD operations: {(stats['ssd_to_dram_us'] + stats['dram_to_ssd_us'])/total_time*100:.1f}%")
        print(f"     GPU operations: {(stats['dram_to_gpu_us'] + stats['gpu_to_dram_us'])/total_time*100:.1f}%")
    
    # å†…å­˜ä½¿ç”¨ç»Ÿè®¡
    if stats['peak_memory_usage'] > 0:
        print(f"\nğŸ§  Memory Usage:")
        print(f"     Peak GPU memory: {sizeof_fmt(stats['peak_memory_usage'])}")
        print(f"     Current allocated: {sizeof_fmt(stats['total_memory_allocated'])}")
        if stats['memory_fragmentation'] > 0:
            print(f"     Memory fragmentation: {stats['memory_fragmentation']*100:.1f}%")
    
    print("="*80)

# ---------- ä¸»å‡½æ•° ----------
def main():
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(verbose=False)
    logger.info("ğŸš€ Starting KV Cache SSD Transfer Profiler")
    
    # æ£€æŸ¥ä¾èµ–
    try:
        check_dependencies(logger)
    except RuntimeError as e:
        logger.error(f"Dependency check failed: {e}")
        sys.exit(1)
    
    ap = argparse.ArgumentParser(description="KV Cache SSD Transfer Profiler")
    ap.add_argument("--model-path", required=True, help="Path to LLaMA model")
    ap.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    ap.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")
    
    src = ap.add_mutually_exclusive_group(required=True)
    src.add_argument("--prompt", help="Input prompt text")
    src.add_argument("--prompt-file", help="File containing prompts, one per line")
    
    ap.add_argument("--max-gen-len", type=int, default=128, help="Number of tokens to generate")
    ap.add_argument("--batch-size", type=int, default=32, help="æ¯æ‰¹å¹¶è¡Œæ¡æ•°ï¼›>1 æ—¶è„šæœ¬ä¼šæŠŠ prompts åˆ‡å—å¾ªç¯æ¨ç†")
    ap.add_argument("--topk-blk", type=int, default=4, help="Number of KV blocks to keep in GPU")
    ap.add_argument("--max-seq-len", type=int, default=2048, help="Maximum sequence length for model")
    ap.add_argument("--csv", help="Save results to CSV file")
    ap.add_argument("--runs", type=int, default=1, help="Number of inference runs")
    args = ap.parse_args()
    
    # æ ¹æ®å‚æ•°é‡æ–°é…ç½®æ—¥å¿—
    if args.verbose:
        logger = setup_logging(verbose=True)
        logger.debug("Verbose logging enabled")

    ckpt = pathlib.Path(args.model_path)
    
    if not ckpt.exists():
        logger.error(f"âŒ Model path does not exist: {ckpt}")
        sys.exit(1)
    
    logger.info(f"ğŸš€ KV Cache SSD Transfer Profiler")
    logger.info(f"ğŸ“ Model: {ckpt.name}")
    logger.info(f"ğŸ–¥ï¸  Device: {args.device}")
    
    # ------------- æ”¶é›† prompt -----------------
    prompts = []
    if args.prompt_file:
        if not os.path.exists(args.prompt_file):
            logger.error(f"âŒ Prompt file does not exist: {args.prompt_file}")
            sys.exit(1)
        
        try:
            with open(args.prompt_file, encoding="utf-8") as f:
                prompts = [line.rstrip("\n") for line in f if line.strip()]
            logger.debug(f"Loaded {len(prompts)} prompts from file")
        except Exception as e:
            logger.error(f"âŒ Error reading prompt file: {e}")
            sys.exit(1)
    else:
        prompts = [args.prompt]

    # ----------- ç©ºåˆ—è¡¨ä¿æŠ¤ -----------
    if not prompts:
        src_name = args.prompt_file or "command line"
        logger.error(f"âŒ No valid prompt found in {src_name}")
        sys.exit(1)

    # ----------- PromptéªŒè¯å’Œå¤„ç† -----------
    logger.info("ğŸ” Validating and processing prompts...")
    try:
        original_count = len(prompts)
        prompts = validate_and_process_prompts(prompts, max_seq_len=args.max_seq_len, logger=logger)
        if len(prompts) != original_count:
            logger.warning(f"Prompt count changed: {original_count} -> {len(prompts)}")
    except Exception as e:
        logger.error(f"âŒ Error processing prompts: {e}")
        sys.exit(1)

    # ------------ æ‰“å°åŸºæœ¬ä¿¡æ¯ -----------------
    max_len = max(len(p) for p in prompts)
    total_batches = math.ceil(len(prompts) / args.batch_size)
    print(f"ğŸ“ Prompts: {len(prompts)} | Max length: {max_len} chars")
    print(f"ğŸ”¢ Batch size: {args.batch_size} | Total batches: {total_batches}")
    print(f"ğŸ”¢ Generation length: {args.max_gen_len}")
    print(f"ğŸ’¾ KV blocks in GPU: {args.topk_blk}")
    print(f"ğŸ”„ Number of runs: {args.runs}")
    
    # åº”ç”¨ patches
    _make_perf_tracker_thread_safe()
    patch_kv_offloader_detailed()
    patch_llama_batch_counter()
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ Loading model...")
    try:
        llama = LLaMA.build(ckpt, load_model=True, device="cpu")
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        sys.exit(1)
    
    # é…ç½® KV cache
    try:
        for layer in llama.model.layers:
            if hasattr(layer, 'attention') and hasattr(layer.attention, 'topk_blk'):
                layer.attention.topk_blk = args.topk_blk
    except Exception as e:
        print(f"âš ï¸  Error configuring KV cache: {e}")
    
    # è½¬ç§»åˆ° GPU
    if args.device.startswith("cuda"):
        if not torch.cuda.is_available():
            print("âŒ CUDA not available but cuda device specified")
            sys.exit(1)
        
        print("ğŸ”„ Transferring to GPU...")
        try:
            llama.model.to(args.device)
            torch.cuda.synchronize()
        except Exception as e:
            print(f"âŒ Error transferring to GPU: {e}")
            sys.exit(1)
    
    llama.args.device = args.device
    
    # é…ç½® offloader streams
    try:
        for blk in llama.model.layers:
            if hasattr(blk, 'attention') and hasattr(blk.attention, 'offloader'):
                off = blk.attention.offloader
                off.device = args.device
                if torch.cuda.is_available():
                    off.copy_stream = torch.cuda.Stream(device=args.device)
    except Exception as e:
        print(f"âš ï¸  Error configuring offloader streams: {e}")
    
    print("âœ… Model ready for inference")
    
    # æ‰§è¡Œå¤šæ¬¡æ¨ç†
    total_inference_time = 0
    
    for run in range(args.runs):
        print(f"\nğŸ§  Running inference {run+1}/{args.runs}...")
        
        # é‡ç½®å½“å‰è¿è¡Œçš„ç»Ÿè®¡
        run_stats = {k: v for k, v in KV_TRANSFER_STATS.items()}
        
        start_time = time.perf_counter()
        
        try:
            all_outs = []
            batch_count = 0
            for prompt_batch in safe_batch_processing(prompts, args.batch_size, max_seq_len=args.max_seq_len, logger=logger):
                batch_count += 1
                logger.debug(f"Processing batch {batch_count} with {len(prompt_batch)} prompts")
                
                _, outs = llama.text_completion(
                    prompt_batch,
                    max_gen_len=args.max_gen_len
                )
                all_outs.extend(outs)
                
                # æ›´æ–°å†…å­˜ç»Ÿè®¡
                update_memory_stats()
                
        except Exception as e:
            logger.error(f"âŒ Error during inference: {e}")
            print(f"âŒ Error during inference: {e}")
            continue
        
        end_time = time.perf_counter()
        
        inference_time = end_time - start_time
        total_inference_time += inference_time
        
        # è®¡ç®—æœ¬æ¬¡è¿è¡Œçš„å¢é‡ç»Ÿè®¡
        run_delta = {}
        for key in KV_TRANSFER_STATS:
            run_delta[key] = KV_TRANSFER_STATS[key] - run_stats[key]
        
        print(f"âœ… Run {run+1} completed in {inference_time:.2f}s")
        print(f"   Generated {len(all_outs)} responses")
        
        # æ˜¾ç¤ºæœ¬æ¬¡è¿è¡Œçš„å…³é”®ç»Ÿè®¡
        if run_delta['ssd_to_dram_count'] > 0 or run_delta['dram_to_gpu_count'] > 0:
            print(f"   SSDâ†’DRAM: {run_delta['ssd_to_dram_count']} ops, {run_delta['ssd_to_dram_us']/1000:.1f}ms")
            print(f"   DRAMâ†’GPU: {run_delta['dram_to_gpu_count']} ops, {run_delta['dram_to_gpu_us']/1000:.1f}ms")
        
        # ä»…åœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶æ‰“å°éƒ¨åˆ†ç”Ÿæˆç»“æœ
        if run == 0 and all_outs:
            print(f"\nğŸ“ Sample outputs (first run):")
            for i, out in enumerate(all_outs[:min(3, len(all_outs))]):
                print(f"   [{i+1}] {out[:100]}{'...' if len(out) > 100 else ''}")
            if len(all_outs) > 3:
                print(f"   ... and {len(all_outs) - 3} more")
    
    # æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š
    print(f"\nğŸ All runs completed!")
    print(f"Total inference time: {total_inference_time:.2f}s")
    print(f"Average time per run: {total_inference_time/args.runs:.2f}s")
    
    # æ‰“å°è¯¦ç»†çš„ä¼ è¾“ç»Ÿè®¡æŠ¥å‘Š
    print_transfer_report()
    
    # ä¿å­˜ CSV ç»“æœ
    if args.csv:
        save_csv_results(args.csv, args, total_inference_time)
    
    print("\nğŸ¯ Profiling completed successfully!")

def save_csv_results(csv_path: str, args, total_inference_time: float) -> None:
    """ä¿å­˜ç»Ÿè®¡ç»“æœåˆ° CSV æ–‡ä»¶
    
    Args:
        csv_path: CSV æ–‡ä»¶ä¿å­˜è·¯å¾„
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        total_inference_time: æ€»æ¨ç†æ—¶é—´ï¼ˆç§’ï¼‰
        
    Note:
        CSV æ–‡ä»¶åŒ…å«å®Œæ•´çš„æ€§èƒ½ç»Ÿè®¡æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
        - é…ç½®å‚æ•°
        - ä¼ è¾“ç»Ÿè®¡
        - ç¼“å­˜ç»Ÿè®¡  
        - æ‰¹å¤„ç†ç»Ÿè®¡
        - å†…å­˜ä½¿ç”¨ç»Ÿè®¡
    """
    try:
        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # å†™å…¥å¤´éƒ¨ä¿¡æ¯
            writer.writerow(['Metric', 'Value', 'Unit'])
            writer.writerow(['Model Path', args.model_path, ''])
            writer.writerow(['Device', args.device, ''])
            writer.writerow(['Batch Size', args.batch_size, ''])
            writer.writerow(['Max Gen Length', args.max_gen_len, ''])
            writer.writerow(['TopK Blocks', args.topk_blk, ''])
            writer.writerow(['Runs', args.runs, ''])
            writer.writerow(['Total Inference Time', f'{total_inference_time:.3f}', 'seconds'])
            writer.writerow([])  # ç©ºè¡Œåˆ†éš”
            
            # å†™å…¥ä¼ è¾“ç»Ÿè®¡
            stats = KV_TRANSFER_STATS
            writer.writerow(['Transfer Statistics', '', ''])
            
            # SSD ç»Ÿè®¡
            writer.writerow(['SSD to DRAM Operations', stats['ssd_to_dram_count'], 'count'])
            writer.writerow(['SSD to DRAM Time', f"{stats['ssd_to_dram_us']/1000:.3f}", 'ms'])
            writer.writerow(['SSD to DRAM Bytes', stats['ssd_to_dram_bytes'], 'bytes'])
            
            writer.writerow(['DRAM to SSD Operations', stats['dram_to_ssd_count'], 'count'])
            writer.writerow(['DRAM to SSD Time', f"{stats['dram_to_ssd_us']/1000:.3f}", 'ms'])
            writer.writerow(['DRAM to SSD Bytes', stats['dram_to_ssd_bytes'], 'bytes'])
            
            # GPU ç»Ÿè®¡
            writer.writerow(['DRAM to GPU Operations', stats['dram_to_gpu_count'], 'count'])
            writer.writerow(['DRAM to GPU Time', f"{stats['dram_to_gpu_us']/1000:.3f}", 'ms'])
            writer.writerow(['DRAM to GPU Bytes', stats['dram_to_gpu_bytes'], 'bytes'])
            
            writer.writerow(['GPU to DRAM Operations', stats['gpu_to_dram_count'], 'count'])
            writer.writerow(['GPU to DRAM Time', f"{stats['gpu_to_dram_us']/1000:.3f}", 'ms'])
            writer.writerow(['GPU to DRAM Bytes', stats['gpu_to_dram_bytes'], 'bytes'])
            
            # Cache ç»Ÿè®¡
            writer.writerow(['Cache Hits', stats['kv_cache_hit_count'], 'count'])
            writer.writerow(['Cache Misses', stats['kv_cache_miss_count'], 'count'])
            
            # å†…å­˜ç»Ÿè®¡
            writer.writerow(['Peak Memory Usage', stats['peak_memory_usage'], 'bytes'])
            writer.writerow(['Current Memory Allocated', stats['total_memory_allocated'], 'bytes'])
            writer.writerow(['Memory Fragmentation', f"{stats['memory_fragmentation']*100:.2f}", 'percent'])
            
            # Batch ç»Ÿè®¡
            if BATCH_STATS['total_batches'] > 0:
                writer.writerow([])
                writer.writerow(['Batch Statistics', '', ''])
                writer.writerow(['Total Batches', BATCH_STATS['total_batches'], 'count'])
                writer.writerow(['Total Prompts', BATCH_STATS['total_prompts'], 'count'])
                writer.writerow(['Max Batch Size', BATCH_STATS['max_batch_size'], 'count'])
                writer.writerow(['Min Batch Size', BATCH_STATS['min_batch_size'], 'count'])
                
                avg_batch_size = BATCH_STATS['total_prompts'] / BATCH_STATS['total_batches']
                writer.writerow(['Avg Batch Size', f'{avg_batch_size:.2f}', 'count'])
        
        print(f"ğŸ“Š Results saved to: {csv_path}")
        
    except Exception as e:
        print(f"âš ï¸  Error saving CSV: {e}")

if __name__ == "__main__":
    main()