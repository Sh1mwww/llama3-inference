#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Multi-batch inference launcher for llama3.generator.LLaMA

ç”¨æ³•ç¤ºä¾‹
--------
# å•æ–‡ä»¶ã€å¤šè¡Œ promptï¼ŒæŒ‰ batch_size é€å…¥ GPU
python scripts/run_inference.py \
    --model-path /path/to/model \
    --prompt-file ./prompts/web_questions.txt \
    --batch-size 32 \
    --max-gen-len 64

# å‘½ä»¤è¡Œç›´æ¥ç»™å¤šæ¡ promptï¼ˆä¸æ—§ç‰ˆå…¼å®¹ï¼‰
python scripts/run_inference.py \
    --model-path /path/to/model \
    --prompt "ä½ å¥½" "è¯·ç”¨ä¸­æ–‡è§£é‡Šç‰›é¡¿ç¬¬äºŒå®šå¾‹" \
    --batch-size 8
"""
import argparse
import itertools
import math
import sys
from pathlib import Path
from typing import List

import torch

from llama3.generator import LLaMA


# ------------------------- Helpers ------------------------- #
def chunks(lst: List[str], n: int):
    """Yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i : i + n]


def read_prompts_from_file(path: Path) -> List[str]:
    with path.open("r", encoding="utf-8") as f:
        lines = [ln.rstrip("\n") for ln in f]
    # è¿‡æ»¤ç©ºè¡Œ
    return [ln for ln in lines if ln.strip()]


# ---------------------- Main routine ---------------------- #
def main():
    p = argparse.ArgumentParser(
        description="Run LLaMA-3 inference with optional multi-batch splitting"
    )
    p.add_argument(
        "--model-path",
        required=True,
        help="ç›®å½•ä¸‹éœ€åŒ…å« *.pth / tokenizer.model / params.json",
    )
    p.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    prompt_group = p.add_mutually_exclusive_group(required=True)
    prompt_group.add_argument(
        "--prompt",
        nargs="+",
        help="ä¸€æ¬¡è¾“å…¥å¤šæ¡ prompt, æ¯æ¡ç”¨å¼•å·åŒ…ä½ï¼ˆæ—§æ¥å£ï¼Œå…¼å®¹å•/å¤š Batchï¼‰",
    )
    prompt_group.add_argument(
        "--prompt-file",
        help="æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼›æ¯è¡Œè§†ä½œä¸€æ¡ promptï¼ˆæ¨èå¤§è§„æ¨¡æ¨ç†æ—¶ä½¿ç”¨ï¼‰",
    )
    p.add_argument("--batch-size", type=int, default=32,help="æ¯ä¸ª Batch å®é™…å¹¶è¡Œæ¡æ•°")
    p.add_argument("--max-seq-len", type=int, default=2048,help="æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆé¢„ç•™æ˜¾å­˜ç”¨ï¼‰")
    p.add_argument("--max-batch-size", type=int, default=512,help="ç¼“å­˜é¢„åˆ†é…çš„æœ€å¤§æ‰¹å®¹é‡ï¼Œè¦ â‰¥ --batch-size")
    args = p.parse_args()

    # -------- Collect prompts -------- #
    if args.prompt_file:
        prompt_path = Path(args.prompt_file).expanduser().resolve()
        if not prompt_path.is_file():
            sys.exit(f"[ERROR] prompt-file not found: {prompt_path}")
        prompts = read_prompts_from_file(prompt_path)
    else:
        prompts = args.prompt or []

    if not prompts:
        sys.exit("[ERROR] No prompt specified.")

    print(
        f"[INFO] Prompts: {len(prompts)}  |  Batch size: {args.batch_size}  |  "
        f"Total batches: {math.ceil(len(prompts)/args.batch_size)}"
    )

    # -------- Build model (ä¸€æ¬¡åŠ è½½å³å¯) -------- #
    llama = LLaMA.build(args.model_path, load_model=True, device=args.device)

    # -------- Run batched inference -------- #
    all_outputs: List[str] = []
    idx_offset = 0
    for batch_idx, prompt_batch in enumerate(chunks(prompts, args.batch_size), 1):
        print(f"\nğŸŸ¢ Running batch {batch_idx}  (size={len(prompt_batch)}) â€¦")
        _, outs = llama.text_completion(
            prompt_batch, max_gen_len=args.max_gen_len
        )
        all_outputs.extend(outs)

        # å®æ—¶æ‰“å°æœ¬æ‰¹æ¬¡ç»“æœ
        for i, (inp, out) in enumerate(zip(prompt_batch, outs), start=idx_offset):
            print(f"\nâ–¼ [{i}] Prompt: {inp}")
            print(f"â–² [{i}] Completion: {out}")
            print("-" * 60)
        idx_offset += len(prompt_batch)

    # -------- å¯é€‰ï¼šå°†å…¨éƒ¨ç»“æœå†™æ–‡ä»¶ï¼ˆç¤ºä¾‹ï¼‰ -------- #
    # with open("inference_outputs.txt", "w", encoding="utf-8") as f:
    #     for i, (inp, out) in enumerate(zip(prompts, all_outputs)):
    #         f.write(f"[{i}] PROMPT:\n{inp}\n\n[{i}] COMPLETION:\n{out}\n\n{'='*80}\n")

    print(f"\nâœ… Finished. Generated {len(all_outputs)} completions.")


if __name__ == "__main__":
    main()
