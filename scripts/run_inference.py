"""
# å•æ–‡ä»¶ã€å¤šè¡Œ promptï¼ŒæŒ‰ batch_size é€å…¥ GPU
python scripts/run_inference.py \
    --model-path /path/to/model \
    --prompt-file ./prompts/web_questions.txt \
    --batch-size 32 \
    --max-gen-len 64

# å‘½ä»¤è¡Œç›´æ¥ç»™å¤šæ¡ prompt
python scripts/run_inference.py \
    --model-path /path/to/model \
    --prompt "ä½ å¥½" "è¯·ç”¨ä¸­æ–‡è§£é‡Šç‰›é¡¿ç¬¬äºŒå®šå¾‹" \
    --batch-size 8
"""
import argparse
import math
import sys
import time
from pathlib import Path
from typing import List, Iterator

import torch

from llama3.generator import LLaMA


# ------------------------- Helpers ------------------------- #
def chunks(lst: List[str], n: int):
    """Yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i : i + n]


def read_prompts_from_file(path: Path) -> List[str]:
    with path.open("r", encoding="utf-8") as f:
        lines = [ln.rstrip("\n") for ln in f]
    # è¿‡æ»¤ç©ºè¡Œ
    return [ln for ln in lines if ln.strip()]


def safe_batch_processing(prompts: List[str], batch_size: int, max_seq_len: int = 2048) -> Iterator[List[str]]:
    """å®‰å…¨çš„æ‰¹å¤„ç†å‡½æ•°ï¼Œç¡®ä¿æ¯ä¸ªbatchç¬¦åˆæ¨¡å‹é™åˆ¶"""
    current_batch = []
    current_batch_tokens = 0
    
    for i, prompt in enumerate(prompts):
        # ä¼°ç®—å½“å‰promptçš„tokenæ•°é‡ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰
        estimated_tokens = len(prompt.split()) if ' ' in prompt else len(prompt) // 3
        
        # æ£€æŸ¥æ·»åŠ æ­¤promptæ˜¯å¦ä¼šè¶…å‡ºé™åˆ¶
        if (len(current_batch) >= batch_size or 
            (current_batch_tokens + estimated_tokens > max_seq_len and current_batch)):
            
            if current_batch:
                yield current_batch
                current_batch = []
                current_batch_tokens = 0
        
        # å¦‚æœå•ä¸ªpromptè¿‡é•¿ï¼Œå‘å‡ºè­¦å‘Šä½†ä»ç„¶å¤„ç†
        if estimated_tokens > max_seq_len:
            print(f"[WARNING] Prompt {i+1} has ~{estimated_tokens} tokens, exceeds max_seq_len {max_seq_len}")
        
        current_batch.append(prompt)
        current_batch_tokens += estimated_tokens
    
    # å¤„ç†æœ€åä¸€ä¸ªbatch
    if current_batch:
        yield current_batch


# ---------------------- Main routine ---------------------- #
def main():
    p = argparse.ArgumentParser(
        description="Run LLaMA-3 inference with optional multi-batch splitting"
    )
    p.add_argument(
        "--model-path",
        required=True,
        help="ç›®å½•ä¸‹éœ€åŒ…å« *.pth / tokenizer.model / params.json",
    )
    p.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    prompt_group = p.add_mutually_exclusive_group(required=True)
    prompt_group.add_argument(
        "--prompt",
        nargs="+",
        help="ä¸€æ¬¡è¾“å…¥å¤šæ¡ prompt, æ¯æ¡ç”¨å¼•å·åŒ…ä½ï¼ˆæ—§æ¥å£ï¼Œå…¼å®¹å•/å¤š Batchï¼‰",
    )
    prompt_group.add_argument(
        "--prompt-file",
        help="æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼›æ¯è¡Œè§†ä½œä¸€æ¡ promptï¼ˆæ¨èå¤§è§„æ¨¡æ¨ç†æ—¶ä½¿ç”¨ï¼‰",
    )
    p.add_argument("--batch-size", type=int, default=1, help="æ¯ä¸ª Batch å®é™…å¹¶è¡Œæ¡æ•°")
    p.add_argument("--max-seq-len", type=int, default=2048, help="æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆé¢„ç•™æ˜¾å­˜ç”¨ï¼‰")
    p.add_argument("--max-batch-size", type=int, default=512, help="ç¼“å­˜é¢„åˆ†é…çš„æœ€å¤§æ‰¹å®¹é‡ï¼Œè¦ â‰¥ --batch-size")
    p.add_argument("--max-gen-len", type=int, default=64, help="ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦")
    args = p.parse_args()

    # Validate arguments
    if args.max_batch_size < args.batch_size:
        sys.exit(f"[ERROR] max-batch-size ({args.max_batch_size}) must be >= batch-size ({args.batch_size})")
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = Path(args.model_path)
    if not model_path.exists():
        sys.exit(f"[ERROR] Model path does not exist: {model_path}")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if args.device.startswith("cuda") and not torch.cuda.is_available():
        print(f"[WARNING] CUDA not available, falling back to CPU")
        args.device = "cpu"

    # -------- Collect prompts -------- #
    if args.prompt_file:
        prompt_path = Path(args.prompt_file).expanduser().resolve()
        if not prompt_path.is_file():
            sys.exit(f"[ERROR] prompt-file not found: {prompt_path}")
        prompts = read_prompts_from_file(prompt_path)
    else:
        prompts = args.prompt or []

    if not prompts:
        sys.exit("[ERROR] No prompt specified.")

    print(
        f"[INFO] Prompts: {len(prompts)}  |  Batch size: {args.batch_size}  |  "
        f"Total batches: {math.ceil(len(prompts)/args.batch_size)}"
    )

    # -------- Build model (å…ˆCPUåGPUï¼Œé¿å…å¡ä½) -------- #
    print("[INFO] Loading model on CPU first...")
    llama = LLaMA.build(args.model_path, load_model=True, device="cpu")
    
    # å¦‚æœéœ€è¦GPUï¼Œå•ç‹¬è½¬ç§»
    if args.device.startswith("cuda"):
        if not torch.cuda.is_available():
            print("[ERROR] CUDA not available but cuda device specified")
            sys.exit(1)
        
        print(f"[INFO] Transferring model to {args.device}...")
        try:
            llama.model.to(args.device)
            llama.args.device = args.device
            torch.cuda.synchronize()
            print(f"[INFO] Model successfully moved to {args.device}")
        except Exception as e:
            print(f"[ERROR] Failed to move model to GPU: {e}")
            sys.exit(1)
    else:
        llama.args.device = args.device

    # -------- Run batched inference -------- #
    all_outputs: List[str] = []
    idx_offset = 0
    total_batches = math.ceil(len(prompts) / args.batch_size)
    
    print(f"[INFO] Starting inference with {total_batches} batches...")
    
    try:
        for batch_idx, prompt_batch in enumerate(safe_batch_processing(prompts, args.batch_size, args.max_seq_len), 1):
            print(f"\nğŸŸ¢ Running batch {batch_idx}/{total_batches} (size={len(prompt_batch)}) â€¦")
            
            start_time = time.time()
            _, outs = llama.text_completion(
                prompt_batch, max_gen_len=args.max_gen_len
            )
            batch_time = time.time() - start_time
            
            all_outputs.extend(outs)
            print(f"âœ… Batch {batch_idx} completed in {batch_time:.2f}s")

            # å®æ—¶æ‰“å°æœ¬æ‰¹æ¬¡ç»“æœ
            for i, (inp, out) in enumerate(zip(prompt_batch, outs), start=idx_offset):
                print(f"\nâ–¼ [{i}] Prompt: {inp}")
                print(f"â–² [{i}] Completion: {out}")
                print("-" * 60)
            idx_offset += len(prompt_batch)
            
    except Exception as e:
        print(f"[ERROR] Inference failed: {e}")
        sys.exit(1)

    # -------- å¯é€‰ï¼šå°†å…¨éƒ¨ç»“æœå†™æ–‡ä»¶ï¼ˆç¤ºä¾‹ï¼‰ -------- #
    # with open("inference_outputs.txt", "w", encoding="utf-8") as f:
    #     for i, (inp, out) in enumerate(zip(prompts, all_outputs)):
    #         f.write(f"[{i}] PROMPT:\n{inp}\n\n[{i}] COMPLETION:\n{out}\n\n{'='*80}\n")

    print(f"\nâœ… Finished. Generated {len(all_outputs)} completions.")


if __name__ == "__main__":
    main()
