#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠŠ Meta åˆ†ç‰‡ (consolidated.00.pth ... consolidated.xx.pth) åˆä½µæˆå–®å€‹ consolidated.pthï¼Œ
å°‡æ¬Šé‡å‘½åæ˜ å°„ç‚ºæœ¬å°ˆæ¡ˆå…§éƒ¨æ ¼å¼ï¼Œä¸¦å°æ³¨æ„åŠ›æŠ•å½±çŸ©é™£ + FFN çŸ©é™£åšè·¨åˆ†ç‰‡æ‹¼æ¥é‡å»ºå®Œæ•´çŸ©é™£ã€‚

ç”¨æ³•:
  python merge_consolidated_to_single.py /home/roger/.llama/checkpoints/Llama3.1-70B /data1/70b.consolidated.pth
"""

import re
import os
import sys
import glob
import json
import torch
from pathlib import Path
from typing import Dict, Tuple

# ---- å‘½åæ˜ å°„ --------------------------------------------------------------

def _hf_to_internal_name(name: str) -> str:
    n = name
    if n.startswith("model."):
        n = n[len("model."):]
    n = n.replace(".self_attn.q_proj.", ".attention.wq.")
    n = n.replace(".self_attn.k_proj.", ".attention.wk.")
    n = n.replace(".self_attn.v_proj.", ".attention.wv.")
    n = n.replace(".self_attn.o_proj.", ".attention.wo.")
    n = n.replace(".input_layernorm.", ".attention_norm.")
    n = n.replace(".post_attention_layernorm.", ".ffn_norm.")
    # FFN: gate->w1, up->w3, down->w2ï¼ˆèˆ‡æœ¬å°ˆæ¡ˆä¸€è‡´ï¼‰
    n = n.replace(".mlp.gate_proj.", ".feed_forward.w1.")
    n = n.replace(".mlp.up_proj.",   ".feed_forward.w3.")
    n = n.replace(".mlp.down_proj.", ".feed_forward.w2.")
    if n == "model.embed_tokens.weight":
        n = "embed_tokens.weight"
    if n == "lm_head.weight":
        n = "output.weight"
    return n

# ---- è®€é…ç½®ï¼Œè‡³å°‘æ‹¿åˆ° dimï¼ˆ=8192 for 70Bï¼‰ ------------------------------------

def _load_model_args_from_dir(root: Path) -> Dict[str, int]:
    for fname in ("params.json", "config.json"):
        p = root / fname
        if p.exists():
            js = json.loads(p.read_text(encoding="utf-8"))
            dim        = int(js.get("dim") or js.get("hidden_size"))
            n_heads    = int(js.get("n_heads") or js.get("num_attention_heads"))
            n_kv_heads = int(js.get("n_kv_heads") or js.get("num_key_value_heads") or n_heads)
            head_dim   = dim // n_heads
            # å˜—è©¦è®€å‡º intermediate_sizeï¼›è®€ä¸åˆ°ä¹Ÿç„¡å¦¨ï¼Œå¾Œé¢æœƒç”¨â€œè§€å¯Ÿåˆ‡ç‰‡+ç›¸åŠ â€æ¨æ–·
            inter = js.get("intermediate_size") or js.get("ffn_hidden_size")
            inter = int(inter) if inter is not None else None
            return {
                "dim": dim, "n_heads": n_heads, "n_kv_heads": n_kv_heads,
                "head_dim": head_dim, "intermediate_size": inter
            }
    raise FileNotFoundError(f"params.json / config.json not found in {root}")

# ---- æ‹¼æ¥ç­–ç•¥ï¼šæŒ‰åƒæ•¸åæ±ºå®šâ€œå„ªå…ˆæ‹¼æ¥ç¶­åº¦â€ -------------------------------------

ATTN_TAG_RE = re.compile(r"\.attention\.(wq|wk|wv|wo)\.weight$")
FFN_TAG_RE  = re.compile(r"\.feed_forward\.(w1|w2|w3)\.weight$")

def _preferred_axis(name: str, dim: int) -> Tuple[int, bool]:
    """
    è¿”å› (å„ªå…ˆæ‹¼æ¥çš„ç¶­åº¦, æ˜¯å¦éœ€è¦åœ¨åˆ¤æ–·å‰è½‰ç½®) çš„é è¨­ç­–ç•¥ï¼Œç”¨æ–¼â€œåˆ†ç‰‡å¸¸è¦‹æƒ…å½¢â€ï¼š
      - ATTN:
          wq: å„ªå…ˆæ²¿ 0 æ‹¼ï¼ˆè¡Œæ‹¼ï¼‰
          wk/wv: å„ªå…ˆæ²¿ 0 æ‹¼
          wo: å„ªå…ˆæ²¿ 1 æ‹¼ï¼ˆåˆ—æ‹¼ï¼‰
      - FFN:
          w1/w3: å„ªå…ˆæ²¿ 0 æ‹¼ï¼ˆè¡Œæ‹¼ï¼‰
          w2   : å„ªå…ˆæ²¿ 1 æ‹¼ï¼ˆåˆ—æ‹¼ï¼‰
    è‹¥æŸç‰‡èˆ‡å„ªå…ˆç­–ç•¥ä¸åŒ¹é…ï¼ˆæ¯”å¦‚ (8192, 1024) å° wqï¼‰ï¼Œå‰‡æœƒåœ¨å¯¦éš›ç´¯ç©æ™‚åˆ¤æ–·ä¸¦å…è¨±è½‰ç½®ã€‚
    """
    if ATTN_TAG_RE.search(name):
        tag = ATTN_TAG_RE.search(name).group(1)
        if tag in ("wq", "wk", "wv"):
            return 0, False
        else:  # wo
            return 1, False
    if FFN_TAG_RE.search(name):
        tag = FFN_TAG_RE.search(name).group(1)
        if tag in ("w1", "w3"):
            return 0, False
        else:  # w2
            return 1, False
    # å…¶å®ƒæ¬Šé‡ä¸æ‹¼æ¥
    return -1, False

# ---- åˆä½µä¸»ç¨‹å¼ -------------------------------------------------------------

def main():
    if len(sys.argv) != 3:
        print(__doc__)
        sys.exit(1)

    root = Path(sys.argv[1]).resolve()
    out_pth = Path(sys.argv[2]).resolve()
    if not root.is_dir():
        print(f"âŒ Not a directory: {root}")
        sys.exit(1)

    args = _load_model_args_from_dir(root)
    DIM = args["dim"]
    print(f"ğŸ“‹ args: dim={args['dim']} n_heads={args['n_heads']} n_kv_heads={args['n_kv_heads']} head_dim={args['head_dim']} "
          f"intermediate_size={args['intermediate_size']}")

    shards = sorted(glob.glob(str(root / "consolidated.*.pth")))
    if not shards:
        # å…œåº•ï¼š*.pthï¼ˆæ’é™¤æˆ‘å€‘è‡ªå·±åˆä½µå‡ºçš„ consolidated.pthï¼‰
        shards = [p for p in sorted(glob.glob(str(root / "*.pth"))) if Path(p).name != "consolidated.pth"]
    if not shards:
        print(f"âŒ No consolidated.*.pth found in {root}")
        sys.exit(1)

    # æ¡¶ï¼šname -> dict(axis=?, parts=[Tensor...], need_dim_check=True/False)
    buckets: Dict[str, Dict] = {}
    merged: Dict[str, torch.Tensor] = {}

    def _accumulate_slice(name: str, t: torch.Tensor):
        # é 2D ç›´æ¥æ”¾ mergedï¼ˆé€šå¸¸æ˜¯ bias æˆ– embedding ç­‰ï¼‰
        if t.ndim != 2:
            if name not in merged:
                merged[name] = t.detach().cpu()
            else:
                # å†æ¬¡å‡ºç¾ä¸”å½¢ç‹€ä¸€æ¨£ -> å¿½ç•¥ï¼›ä¸ä¸€æ¨£ -> å ±éŒ¯
                if tuple(merged[name].shape) != tuple(t.shape):
                    raise RuntimeError(f"Duplicate non-2D key with different shapes: {name} {merged[name].shape} vs {t.shape}")
            return

        # åƒ…å° ATTN/FFN æ¬Šé‡åšæ‹¼æ¥é‚è¼¯
        m_attn = ATTN_TAG_RE.search(name)
        m_ffn  = FFN_TAG_RE.search(name)
        if not (m_attn or m_ffn):
            # å…¶å®ƒ 2D æ¬Šé‡ï¼ˆembed/output/LayerNorm gamma ç­‰ï¼‰ç›´æ¥æ”¶ä¸‹ï¼ˆé€šå¸¸æ¯ç‰‡åªå‡ºç¾åœ¨ä¸€å€‹ shardï¼‰
            if name not in merged:
                merged[name] = t.detach().cpu()
            else:
                if tuple(merged[name].shape) != tuple(t.shape):
                    raise RuntimeError(f"Duplicate key with different shapes: {name} {merged[name].shape} vs {t.shape}")
            return

        pref_axis, _ = _preferred_axis(name, DIM)
        r, c = int(t.shape[0]), int(t.shape[1])

        # è‹¥å·²å­˜åœ¨å®Œæ•´çŸ©é™£ï¼ˆä¹‹å‰æŸç‰‡å°±çµ¦äº†å®Œæ•´çš„ï¼‰ï¼Œç›´æ¥ä¿ç•™
        if name in merged:
            if tuple(merged[name].shape) != (r, c):
                # å¦‚æœå·²ç¶“æœ‰å®Œæ•´ï¼Œä¸”ç•¶å‰åˆä¾†ä¸€å€‹åˆ‡ç‰‡ -> è¦–ç‚ºé‡è¤‡ï¼Œå¿½ç•¥
                if name in buckets:
                    # æ­£å¸¸æƒ…æ³ä¸æœƒåŒæ™‚æ—¢æœ‰å®Œæ•´åˆæœ‰æ¡¶ï¼Œä½†è‹¥å‡ºç¾ï¼Œå„ªå…ˆå®Œæ•´ï¼Œå¿½ç•¥åˆ‡ç‰‡
                    return
                # å¦å‰‡å½¢ç‹€è¡çª
                raise RuntimeError(f"Duplicate {name} with different shapes: {merged[name].shape} vs {t.shape}")
            return

        # åˆå§‹åŒ–æ¡¶
        B = buckets.get(name)
        if B is None:
            axis = pref_axis
            need_T = False
            # å˜—è©¦æŒ‰â€œå„ªå…ˆç­–ç•¥â€å°é½Š
            if axis == 0:
                # æœŸæœ›â€œåˆ—æ•¸ = DIMâ€ï¼Œè‹¥ä¸æ˜¯ï¼Œè©¦è‘—è½‰ç½®ï¼›å†ä¸è¡Œå°±é€€è€Œæ±‚å…¶æ¬¡è®“ä¸‹æ–¹é‚è¼¯åˆ¤æ–·
                if c != DIM and r == DIM:
                    need_T = True
                elif c != DIM and r != DIM:
                    # è®“å¾ŒçºŒé‚è¼¯å»åˆ¤æ–·ï¼ˆå°æŸäº› ATTN åˆ‡ç‰‡ e.g. wo å¯èƒ½å„ªå…ˆæ˜¯ axis=1ï¼‰
                    pass
            elif axis == 1:
                # æœŸæœ›â€œè¡Œæ•¸ = DIMâ€ï¼Œè‹¥ä¸æ˜¯ï¼Œè©¦è‘—è½‰ç½®
                if r != DIM and c == DIM:
                    need_T = True

            if need_T:
                t = t.T.contiguous()
                r, c = int(t.shape[0]), int(t.shape[1])

            # è‹¥ä¾ç„¶ä¸åŒ¹é…â€œaxis æ‰€å°æ‡‰çš„å›ºå®šé‚Š = DIMâ€ï¼Œå˜—è©¦åè½‰ axis
            if axis == 0 and c != DIM:
                axis = 1
            if axis == 1 and r != DIM:
                axis = 0

            B = {"axis": axis, "parts": []}
            buckets[name] = B

        # åˆ°é€™è£¡ï¼Œè‹¥éœ€è¦å†åšä¸€æ¬¡è½‰ç½®è®“â€œå›ºå®šé‚Š = DIMâ€
        axis = B["axis"]
        if axis == 0 and c != DIM and r == DIM:
            t = t.T.contiguous(); r, c = c, r
        elif axis == 1 and r != DIM and c == DIM:
            t = t.T.contiguous(); r, c = c, r

        # æœ€çµ‚æª¢æŸ¥å›ºå®šé‚Šæ˜¯å¦æ˜¯ DIM
        if axis == 0 and c != DIM:
            raise RuntimeError(f"[{name}] expect concat axis=0 with columns==DIM({DIM}), got shape {t.shape}")
        if axis == 1 and r != DIM:
            raise RuntimeError(f"[{name}] expect concat axis=1 with rows==DIM({DIM}), got shape {t.shape}")

        B["parts"].append(t.detach().cpu())

    # é€åˆ†ç‰‡ç´¯ç©
    for sp in shards:
        print(f"  â€¢ loading {sp}")
        sd = torch.load(sp, map_location="cpu")
        for k, v in sd.items():
            if not isinstance(v, torch.Tensor):
                continue
            name = _hf_to_internal_name(k)
            _accumulate_slice(name, v)

    # å°‡æ¡¶è£¡çš„åˆ‡ç‰‡åš torch.catï¼ˆæ³¨æ„åŠ› + FFN éƒ½æœƒä¾†åˆ°é€™è£¡ï¼‰
    for name, meta in buckets.items():
        axis  = meta["axis"]
        parts = meta["parts"]
        if len(parts) == 1:
            merged[name] = parts[0]
            continue
        cat = torch.cat(parts, dim=axis).contiguous()
        merged[name] = cat

    # ä¿å­˜
    out_pth.parent.mkdir(parents=True, exist_ok=True)
    torch.save(merged, str(out_pth))
    print(f"âœ… saved: {out_pth}")

    # å¿«é€Ÿæª¢æŸ¥å‰å…©å±¤ ATTN/FFN å½¢ç‹€
    def shp(d, key): return tuple(merged[d].shape) if d in merged else None
    for L in [0, 1]:
        print(f"Layer {L}:",
              "wq", shp(f"layers.{L}.attention.wq.weight", ""),
              "wk", shp(f"layers.{L}.attention.wk.weight", ""),
              "wv", shp(f"layers.{L}.attention.wv.weight", ""),
              "wo", shp(f"layers.{L}.attention.wo.weight", ""))
        print(f"          ",
              "w1", shp(f"layers.{L}.feed_forward.w1.weight", ""),
              "w2", shp(f"layers.{L}.feed_forward.w2.weight", ""),
              "w3", shp(f"layers.{L}.feed_forward.w3.weight", ""))
    print("ğŸ¯ æç¤ºï¼šå° 70B æœŸæœ› wq/wo=(8192,8192) wk/wv=(1024,8192)ï¼›w1/w3=(28672,8192) w2=(8192,28672)")
if __name__ == "__main__":
    main()
