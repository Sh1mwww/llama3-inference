#!/usr/bin/env python3
"""
å•å…ƒæµ‹è¯•ï¼šéªŒè¯æ‰€æœ‰æ”¹è¿›æ˜¯å¦æ­£å¸¸å·¥ä½œï¼ˆä¸éœ€è¦å®é™…æ¨¡å‹æƒé‡ï¼‰
"""
import torch
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

def test_event_pool_improvements():
    """æµ‹è¯•äº‹ä»¶æ± æ”¹è¿›"""
    print("\n" + "="*80)
    print("æµ‹è¯• 1: äº‹ä»¶æ± æ”¹è¿›ï¼ˆstream_mnt.pyï¼‰")
    print("="*80)

    try:
        from llama3 import stream_mnt

        if not torch.cuda.is_available():
            print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡äº‹ä»¶æ± æµ‹è¯•")
            return None

        device = "cuda:0"

        # è·å– streams
        streams = stream_mnt.get_streams(device)
        print(f"âœ… Streams åˆ›å»ºæˆåŠŸ")

        # æµ‹è¯•äº‹ä»¶æ± 
        pool = stream_mnt._get_event_pool(device)
        print(f"âœ… äº‹ä»¶æ± è·å–æˆåŠŸ")

        # æµ‹è¯•æ–°çš„ record_event_on API
        if streams.compute_mha:
            eid, evt = stream_mnt.record_event_on(streams.compute_mha, device=device)
            print(f"âœ… äº‹ä»¶è®°å½•æˆåŠŸ (ID: {eid})")

            # æµ‹è¯•é‡Šæ”¾
            stream_mnt.release_event(eid, device=device)
            print(f"âœ… äº‹ä»¶é‡Šæ”¾æˆåŠŸ")

        # æµ‹è¯• GC
        freed = stream_mnt.gc_event_pool(device=device)
        print(f"âœ… äº‹ä»¶æ±  GC æˆåŠŸ (å›æ”¶: {freed} ä¸ªäº‹ä»¶)")

        print(f"\nâœ… äº‹ä»¶æ± æ”¹è¿›æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"\nâŒ äº‹ä»¶æ± æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_kv_offloader_improvements():
    """æµ‹è¯• KV Offloader DRAM é…é¢æ”¹è¿›"""
    print("\n" + "="*80)
    print("æµ‹è¯• 2: KV Offloader DRAM é…é¢ä¼˜åŒ–")
    print("="*80)

    try:
        from llama3.kv_offload import KVOffloader, BLOCK
        from llama3.config import KVCacheArgs

        # è®¾ç½®æµ‹è¯•é…ç½®
        KVCacheArgs.dram_limit_gb = 2.0
        KVCacheArgs.dram_sizing_batch = 8  # ä½¿ç”¨æ”¹è¿›çš„é…é¢ä¼°ç®—

        print(f"é…ç½®:")
        print(f"  - dram_limit_gb: {KVCacheArgs.dram_limit_gb}")
        print(f"  - dram_sizing_batch: {KVCacheArgs.dram_sizing_batch}")

        # åˆ›å»º KVOffloader
        device = "cuda:0" if torch.cuda.is_available() else "cpu"

        print(f"\nåˆ›å»º KVOffloader (device: {device})...")
        offloader = KVOffloader(
            layers=32,
            heads=32,
            dim=128,
            max_seq=2048,
            max_batch=32,  # å®é™…ä¼šç”¨ dram_sizing_batch=8 æ¥ä¼°ç®—
            device=device,
            dtype_bytes=2,  # fp16
            streams=None,
        )

        print(f"\nâœ… KVOffloader åˆ›å»ºæˆåŠŸ")
        print(f"  - DRAM limit blocks: {offloader.dram_limit_blk}")
        print(f"  - Block size: {offloader.block_nbytes / (1024**2):.2f} MB")

        # éªŒè¯é…é¢è®¡ç®—æ˜¯å¦ä½¿ç”¨äº† dram_sizing_batch
        expected_token_nbytes = (8 * 32 * 128) * 2 * 2  # alloc_bsz=8
        assert offloader.token_nbytes == expected_token_nbytes, \
            f"token_nbytes ä¸æ­£ç¡®: {offloader.token_nbytes} != {expected_token_nbytes}"
        print(f"âœ… é…é¢è®¡ç®—æ­£ç¡®ä½¿ç”¨ dram_sizing_batch")

        # éªŒè¯ dram_limit_blk ä¸ä¸º 0
        assert offloader.dram_limit_blk > 0, \
            f"dram_limit_blk ä¸º 0ï¼Œé…é¢ä¼°ç®—å¯èƒ½æœ‰é—®é¢˜"
        print(f"âœ… dram_limit_blk æ­£å¸¸ (> 0)")

        print(f"\nâœ… DRAM é…é¢ä¼˜åŒ–æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"\nâŒ KV Offloader æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_queue_blocking_put():
    """æµ‹è¯•å†™é˜Ÿåˆ—é˜»å¡å¼ put"""
    print("\n" + "="*80)
    print("æµ‹è¯• 3: å†™é˜Ÿåˆ—é˜»å¡å¼å¤„ç†")
    print("="*80)

    try:
        from queue import Queue, Full
        import time

        # åˆ›å»ºå°é˜Ÿåˆ—æµ‹è¯•
        q = Queue(maxsize=2)

        # å¡«æ»¡é˜Ÿåˆ—
        q.put("item1")
        q.put("item2")

        print(f"âœ… é˜Ÿåˆ—å·²å¡«æ»¡ (2/2)")

        # æµ‹è¯•é˜»å¡å¼ put å¸¦è¶…æ—¶
        start = time.time()
        try:
            q.put("item3", timeout=0.5)
            print(f"âŒ åº”è¯¥è§¦å‘ Full å¼‚å¸¸")
            return False
        except Full:
            elapsed = time.time() - start
            print(f"âœ… Full å¼‚å¸¸æ­£ç¡®è§¦å‘ (è¶…æ—¶: {elapsed:.2f}s)")

        # éªŒè¯ä»£ç ä¸­ç¡®å®å¯¼å…¥äº† Full
        import llama3.kv_offload
        import inspect
        source = inspect.getsource(llama3.kv_offload)

        if "from queue import" in source and "Full" in source:
            print(f"âœ… kv_offload.py æ­£ç¡®å¯¼å…¥ Full")
        else:
            print(f"âŒ kv_offload.py æœªæ­£ç¡®å¯¼å…¥ Full")
            return False

        if "put(" in source and "timeout=" in source:
            print(f"âœ… kv_offload.py ä½¿ç”¨é˜»å¡å¼ put")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°é˜»å¡å¼ put è°ƒç”¨")

        print(f"\nâœ… å†™é˜Ÿåˆ—æ”¹è¿›æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"\nâŒ é˜Ÿåˆ—æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_layers_event_sync():
    """æµ‹è¯• layers.py ä¸­çš„äº‹ä»¶åŒ–åŒæ­¥"""
    print("\n" + "="*80)
    print("æµ‹è¯• 4: Layers äº‹ä»¶åŒ–åŒæ­¥")
    print("="*80)

    try:
        from llama3.config import ModelArgs
        from llama3.layers import EncoderBlock

        if not torch.cuda.is_available():
            print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡ layers æµ‹è¯•")
            return None

        # åˆ›å»ºç®€å•çš„ args
        args = ModelArgs(
            dim=256,
            n_layers=2,
            n_heads=8,
            n_kv_heads=8,
            vocab_size=32000,
            multiple_of=256,
            ffn_dim_multiplier=None,
            norm_eps=1e-5,
            rope_theta=10000.0,
            use_scaled_rope=False,
            max_batch_size=4,
            max_seq_len=512,
            device="cuda:0",
            topk_blk=4,
        )

        print(f"åˆ›å»º EncoderBlock...")
        block = EncoderBlock(args, layer_id=0)

        # éªŒè¯åˆå§‹åŒ–äº† _gc_counter
        assert hasattr(block, '_gc_counter'), "EncoderBlock ç¼ºå°‘ _gc_counter"
        print(f"âœ… EncoderBlock æ­£ç¡®åˆå§‹åŒ– _gc_counter")

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ–°çš„äº‹ä»¶åŒ–ç­‰å¾…
        import inspect
        source = inspect.getsource(block.forward)

        if "stream_mnt.record_event_on" in source:
            print(f"âœ… EncoderBlock.forward ä½¿ç”¨äº‹ä»¶åŒ–ç­‰å¾…")
        else:
            print(f"âš ï¸  EncoderBlock.forward æœªä½¿ç”¨äº‹ä»¶åŒ–ç­‰å¾…ï¼ˆå¯èƒ½ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼‰")

        if "gc_event_pool" in source:
            print(f"âœ… EncoderBlock.forward å®šæœŸè§¦å‘ GC")
        else:
            print(f"âš ï¸  EncoderBlock.forward æœªè§¦å‘ GC")

        print(f"\nâœ… Layers äº‹ä»¶åŒ–åŒæ­¥æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"\nâŒ Layers æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("="*80)
    print("æ‰€æœ‰æ”¹è¿›çš„å•å…ƒæµ‹è¯•")
    print("="*80)
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPU: {torch.cuda.get_device_name(0)}")

    results = {}

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results['event_pool'] = test_event_pool_improvements()
    results['kv_offloader'] = test_kv_offloader_improvements()
    results['queue_blocking'] = test_queue_blocking_put()
    results['layers_sync'] = test_layers_event_sync()

    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“")
    print("="*80)

    for name, result in results.items():
        if result is True:
            status = "âœ… é€šè¿‡"
        elif result is False:
            status = "âŒ å¤±è´¥"
        else:
            status = "â­ï¸  è·³è¿‡"
        print(f"{name:20s}: {status}")

    passed = sum(1 for r in results.values() if r is True)
    failed = sum(1 for r in results.values() if r is False)
    skipped = sum(1 for r in results.values() if r is None)

    print(f"\næ€»è®¡: {passed} é€šè¿‡, {failed} å¤±è´¥, {skipped} è·³è¿‡")

    if failed == 0:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ‰€æœ‰æ”¹è¿›å·²æˆåŠŸåº”ç”¨ã€‚")
        return 0
    else:
        print(f"\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return 1

if __name__ == "__main__":
    exit(main())
