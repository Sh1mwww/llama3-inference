#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäº test_70b_prefill_ssd.py çš„å®Œæ•´æ¨ç†è„šæœ¬ï¼š
- ç»´æŒç›¸åŒçš„è¿è¡Œæ—¶é…ç½®ï¼ˆWSM/SSD æµå¼æƒé‡ã€KV æ± ã€env ç­‰ï¼‰
- å°†ç”Ÿæˆé•¿åº¦æ”¹ä¸º max_gen_len=32ï¼Œå¹¶çœŸæ­£ decode è¾“å‡ºæ–‡æœ¬
- æŒ‰éœ€æ±‚ï¼šåœ¨è®¡ç®—ç¬¬ i å±‚æ—¶ï¼Œä¿è¯ i+1..i+4 çš„ç»„çº§æƒé‡å·²åœ¨ GPUï¼ˆäº‹ä»¶å°±ç»ªï¼Œæ— é˜»å¡ç­‰å¾…ï¼‰ï¼›
  DRAM ä¾§ç»´æŒ i+4 .. i+4+cap çš„ç¯å½¢çª—å£ï¼ˆå¯¹ 80 å±‚å–æ¨¡ï¼‰
"""

import os
from pathlib import Path
import torch

import time
from dataclasses import dataclass, field

# ===================== æ€§èƒ½è®¡æ—¶å™¨ï¼ˆGPU/IO/Compute/é˜¶æ®µåˆ‡åˆ†ï¼‰ =====================
class _PerfRecorder:
    """
    - è®°å½•ï¼š
        * Host æ€»ä½“æ¨ç†è€—æ—¶ï¼ˆwallï¼‰
        * GPU H2D ä¼ è¾“åŒºé—´ï¼ˆåŸºäº WSM çš„éé˜»å¡ copy äº‹ä»¶ï¼‰
        * GPU è®¡ç®—åŒºé—´ï¼ˆæŒ‰æ¯å±‚ forward åŒ…è£¹ CUDA Eventï¼‰
        * Host ä¾§ I/O ç­‰å¾…æ—¶é—´ï¼ˆwait_event/synchronize çš„é˜»å¡æ—¶é—´ï¼‰
    - ç»™å‡ºï¼š
        * è®¡ç®—æ€»æ—¶é•¿ã€I/O æ€»æ—¶é•¿
        * I/O ä¸è®¡ç®—çš„äº¤å æ—¶é•¿ï¼ˆåŒä¸€ GPU æ—¶é—´çº¿ï¼‰
        * prefill / decode çš„ç²—ç²’åº¦æ—¶é—´ï¼ˆGPU è®¡ç®— + wall ä¼°è®¡ï¼‰
    """
    def __init__(self, device: str | torch.device):
        dev = torch.device(device) if not isinstance(device, torch.device) else device
        self.cuda = (dev.type == "cuda" and torch.cuda.is_available())
        self.dev  = dev
        self.reset()

    def reset(self):
        # Host è®¡æ—¶
        self.host_infer_start = None
        self.host_infer_end   = None
        self.host_io_wait_ms  = 0.0

        # GPU äº‹ä»¶
        self.t0_evt = None
        self.h2d_evt_pairs = []       # [(start_evt, end_evt, meta_dict)]
        self.compute_evt_pairs = []   # [(start_evt, end_evt, meta_dict{layer, phase, seq_len})]

        # ä¼°è®¡é˜¶æ®µåˆ†ç•Œ
        self.prefill_end_ms   = None
        self.decode_start_ms  = None

        # ç»Ÿè®¡ç¼“å­˜
        self._final = None

    # ---------- Host è®¡æ—¶ ----------
    def host_start(self):
        self.host_infer_start = time.perf_counter()

    def host_end(self):
        self.host_infer_end = time.perf_counter()

    def add_host_io_wait(self, ms: float):
        self.host_io_wait_ms += float(ms)

    # ---------- GPU è®¡æ—¶ ----------
    def record_t0(self):
        if not self.cuda: return
        self.t0_evt = torch.cuda.Event(enable_timing=True)
        self.t0_evt.record(torch.cuda.current_stream(self.dev))

    def record_h2d_pair(self, start_evt, end_evt, meta=None):
        if not self.cuda: return
        self.h2d_evt_pairs.append((start_evt, end_evt, meta or {}))
        # Debug: print first few recordings
        if len(self.h2d_evt_pairs) <= 3:
            print(f"[PERF DEBUG] H2D event recorded: {len(self.h2d_evt_pairs)}, meta={meta}")

    def record_compute_pair(self, start_evt, end_evt, meta=None):
        if not self.cuda: return
        self.compute_evt_pairs.append((start_evt, end_evt, meta or {}))
        # Debug: print first few recordings
        if len(self.compute_evt_pairs) <= 3:
            print(f"[PERF DEBUG] Compute event recorded: {len(self.compute_evt_pairs)}, meta={meta}")

    # ---------- å·¥å…·ï¼šåŒºé—´å¹¶é›†ä¸äº¤é›† ----------
    @staticmethod
    def _merge(intervals):
        if not intervals: return []
        intervals = sorted(intervals, key=lambda x: (x[0], x[1]))
        merged = []
        cs, ce = intervals[0]
        for s,e in intervals[1:]:
            if s <= ce:
                ce = max(ce, e)
            else:
                merged.append((cs, ce))
                cs, ce = s, e
        merged.append((cs, ce))
        return merged

    @staticmethod
    def _total_length(intervals):
        return sum(max(0.0, e - s) for s,e in intervals)

    @staticmethod
    def _intersect(a, b):
        i, j = 0, 0
        res = []
        while i < len(a) and j < len(b):
            s = max(a[i][0], b[j][0])
            e = min(a[i][1], b[j][1])
            if s < e:
                res.append((s,e))
            if a[i][1] < b[j][1]:
                i += 1
            else:
                j += 1
        return res

    # ---------- å®Œæˆç»Ÿè®¡ ----------
    def finalize(self):
        if self._final is not None:
            return self._final

        print(f"[PERF DEBUG] Finalizing... cuda={self.cuda}, h2d_pairs={len(self.h2d_evt_pairs)}, compute_pairs={len(self.compute_evt_pairs)}")

        if self.cuda:
            torch.cuda.synchronize(self.dev)

        # Host wall
        wall_ms = None
        if self.host_infer_start is not None and self.host_infer_end is not None:
            wall_ms = (self.host_infer_end - self.host_infer_start) * 1000.0

        # è‹¥æ—  CUDAï¼Œè¿”å›æœ€åŸºç¡€ç»Ÿè®¡
        if not self.cuda or self.t0_evt is None:
            self._final = {
                "host_wall_ms": wall_ms,
                "host_io_wait_ms": self.host_io_wait_ms,
                "gpu_compute_ms": None,
                "gpu_io_h2d_ms": None,
                "gpu_overlap_io_compute_ms": None,
                "prefill": {"gpu_compute_ms": None, "wall_est_ms": None},
                "decode":  {"gpu_compute_ms": None, "wall_est_ms": None},
            }
            return self._final

        # è½¬æ¢äº‹ä»¶ä¸ºä»¥ t0 ä¸ºåŸç‚¹çš„æ—¶é—´åŒºé—´
        def _evt_to_ms_pair(pair):
            s_evt, e_evt = pair
            s = self.t0_evt.elapsed_time(s_evt)
            e = self.t0_evt.elapsed_time(e_evt)
            if e < s: e = s
            return (float(s), float(e))

        h2d_intervals = []
        for s_evt, e_evt, meta in self.h2d_evt_pairs:
            h2d_intervals.append(_evt_to_ms_pair((s_evt, e_evt)))

        compute_intervals = []
        prefill_intervals = []
        decode_intervals  = []
        for s_evt, e_evt, meta in self.compute_evt_pairs:
            s,e = _evt_to_ms_pair((s_evt, e_evt))
            compute_intervals.append((s,e))
            phase = meta.get("phase")
            if phase == "prefill":
                prefill_intervals.append((s,e))
            elif phase == "decode":
                decode_intervals.append((s,e))

        # å¹¶é›† & é•¿åº¦
        h2d_union = self._merge(h2d_intervals)
        cmp_union = self._merge(compute_intervals)
        pre_union = self._merge(prefill_intervals) if prefill_intervals else []
        dec_union = self._merge(decode_intervals)  if decode_intervals else []

        h2d_total = self._total_length(h2d_union)
        cmp_total = self._total_length(cmp_union)
        pre_total = self._total_length(pre_union)
        dec_total = self._total_length(dec_union)

        # äº¤å 
        inter = self._intersect(h2d_union, cmp_union)
        inter_total = self._total_length(inter)

        # é˜¶æ®µè¾¹ç•Œï¼ˆwall ç²—ä¼°ï¼‰
        wall_prefill_ms = None
        wall_decode_ms  = None
        if pre_union:
            t0 = pre_union[0][0]
            te = pre_union[-1][1]
            wall_prefill_ms = te - t0
        if dec_union:
            t1 = dec_union[0][0]
            te = dec_union[-1][1]
            wall_decode_ms = te - t1

        self._final = {
            "host_wall_ms": wall_ms,
            "host_io_wait_ms": self.host_io_wait_ms,
            "gpu_compute_ms": cmp_total,
            "gpu_io_h2d_ms": h2d_total,
            "gpu_overlap_io_compute_ms": inter_total,
            "prefill": {"gpu_compute_ms": pre_total, "wall_est_ms": wall_prefill_ms},
            "decode":  {"gpu_compute_ms": dec_total, "wall_est_ms": wall_decode_ms},
        }
        return self._final

    def pretty_print(self, extra=None):
        R = self.finalize()
        print("\n==================== â±ï¸ Inference Profiling Report ====================")
        def fmt(x):
            return f"{x:.2f} ms" if isinstance(x, (int,float)) and x is not None else str(x)
        print(f"Host Wall (overall): {fmt(R['host_wall_ms'])}")
        print(f"Host I/O wait:       {fmt(R['host_io_wait_ms'])}")
        print(f"GPU Compute total:   {fmt(R['gpu_compute_ms'])}")
        print(f"GPU H2D I/O total:   {fmt(R['gpu_io_h2d_ms'])}")
        print(f"GPU Overlap(IOâˆ©Cmp): {fmt(R['gpu_overlap_io_compute_ms'])}")
        # äº¤å å æ¯”
        if R['gpu_io_h2d_ms'] and R['gpu_compute_ms'] and R['gpu_compute_ms']>0:
            overlap_ratio = R['gpu_overlap_io_compute_ms']/min(R['gpu_compute_ms']+1e-9, R['gpu_io_h2d_ms']+1e-9)
            print(f"Overlap ratio (~ against min(comp,io)): {overlap_ratio*100:.1f}%")

        print("\n---- Phase breakdown ----")
        print(f"Prefill - GPU compute: {fmt(R['prefill']['gpu_compute_ms'])} | Wall(est): {fmt(R['prefill']['wall_est_ms'])}")
        print(f"Decoder - GPU compute: {fmt(R['decode']['gpu_compute_ms'])} | Wall(est): {fmt(R['decode']['wall_est_ms'])}")

        if isinstance(extra, dict):
            for k,v in extra.items():
                print(f"{k}: {v}")
        print("=======================================================================\n")


# å…¨å±€ recorderï¼ˆåœ¨ main() ä¸­åˆå§‹åŒ–ï¼‰
G_PERF: _PerfRecorder | None = None


# ===== é¡¹ç›®å†…æ¨¡å— =====
from llama3.generator import LLaMA
from llama3.config import KVCacheArgs, load_runtime_config, runtime_config_to_dict
from llama3 import generator as _gen

# ========== build() åŒ…è£…ï¼šåªåŠ æ—¥å¿—ï¼Œä¸æ”¹åº“ ==========
_orig_build = _gen.LLaMA.build

def _debug_build(*args, **kw):
    mode       = kw.get("mode", None)
    load_model = kw.get("load_model", None)
    mode_cfg   = (kw.get("mode_config", {}) or {})
    raw_dev    = mode_cfg.get("raw_device")
    manifest   = mode_cfg.get("manifest_path") or mode_cfg.get("ssd_manifest_path")

    print(f"[MODE-DECISION] LLaMA.build(mode={mode}, load_model={load_model})")
    use_raw_ssd = (mode in {"ssd", "mixed"}) or (mode_cfg.get("weight_source") == "raw-ssd")
    print(f"[MODE-DECISION] use_raw_ssd={use_raw_ssd} raw_device={raw_dev} manifest={manifest}")

    llama = _orig_build(*args, **kw)

    has_wsm = hasattr(llama, "weight_streaming_manager")
    if has_wsm:
        wsm = llama.weight_streaming_manager
        ssd = bool(getattr(wsm, "ssd_enabled", False) or getattr(wsm, "ssd", None))
        print(f"[MODE-DECISION] built: WSM present, ssd_enabled={ssd}")
    else:
        print("[MODE-DECISION] built: NO WSM (å¯èƒ½æ˜¯ full-cpu/full-gpu/æ—§ streaming)")
    return llama

_gen.LLaMA.build = staticmethod(_debug_build)

# ===== WSM runtime monkey-patch: strict ready + CPU stub loader =====
import types

def _patched_wait_group_ready(self, layer_idx: int, group: str, compute_stream=None):
    """
    ç­‰å¾… (layer_idx, group) ç»„å°±ç»ªï¼›äº‹ä»¶ç»“æŸå**äºŒæ¬¡æ ¡éªŒ**æ˜¯å¦çœŸåœ¨ GPUã€‚
    è‹¥ä»ä¸åœ¨ï¼Œåˆ™å¼ºåˆ¶åŒæ­¥ ensure_group_on_gpu()ã€‚

    â­ åŒä¿é™©ï¼šdecoder åˆ‡æ¢æ£€æµ‹ï¼ˆä»é«˜å±‚å›åˆ° layer 0 ä¸”è¿˜æ²¡ prime è¿‡ï¼‰
    """
    # ===== åŒä¿é™©ï¼šdecoder åˆ‡æ¢æ£€æµ‹ =====
    # æ£€æµ‹ä»"é«˜å±‚å›åˆ° 0"ä¸”è¿˜æ²¡ prime è¿‡ â†’ è¯´æ˜è¿›å…¥ decoder é˜¶æ®µ
    if layer_idx == 0 and not getattr(self, "_decoder_prime_done", False):
        last_layer = getattr(self, "_last_executed_layer", -1)
        if last_layer > 0 and hasattr(self, "_prime_decoder_window"):
            if getattr(self, "verbose", False):
                print(f"[WSM FAILSAFE] Detected decoder start (L{last_layer}â†’L0); priming now")
            try:
                self._prime_decoder_window(first_n=4)
                self._decoder_prime_done = True
            except Exception as e:
                if getattr(self, "verbose", False):
                    print(f"[WSM FAILSAFE] Failed to prime decoder: {e}")
    # ===============================================

    # åŸå§‹ç­‰å¾…é€»è¾‘ï¼šè°ƒç”¨åŸå§‹çš„ï¼ˆæœªè¢« patch çš„ï¼‰wait_group_ready
    # æ³¨æ„ï¼š_original_wait_group_ready ä¼šåœ¨ patch æ—¶ä¿å­˜
    return self._original_wait_group_ready(layer_idx, group, compute_stream=compute_stream)


def _patched_ensure_module_on_gpu(self, m: torch.nn.Module, layer_idx: int | None = None, module_name: str | None = None):
    """
    æ‰©å±•ï¼šæŠŠ **0-size CPU stub** å½“ä½œ meta ä¸€æ ·å¤„ç†ï¼Œä¼˜å…ˆä» CPU cache å–å›å¹¶ä¸Šå¡ã€‚
    å…¶å®ƒæƒ…å†µä»å¤ç”¨åŸå…ˆçš„ _ensure_param_on_gpu() è·¯å¾„ã€‚
    """
    params_to_replace = {}
    params_full_names = {}

    def _full_name(layer_idx: int, module_name: str, local_param_name: str) -> str:
        if module_name in ("wq", "wk", "wv", "wo"):
            parent = "attention"
        elif module_name in ("w1", "w2", "w3"):
            parent = "feed_forward"
        else:
            parent = module_name or ""
        return f"layers.{layer_idx}.{parent}.{module_name}.{local_param_name}" if parent else f"layers.{layer_idx}.{module_name}.{local_param_name}"

    def _fetch_from_cpu_cache(name: str):
        if (layer_idx is not None) and (layer_idx in self.cpu_cache):
            return self.cpu_cache[layer_idx].get(name)
        return None

    for local_param_name, p in m.named_parameters(recurse=False):
        full_name = None
        if (layer_idx is not None) and (module_name is not None):
            full_name = _full_name(layer_idx, module_name, local_param_name)

        is_meta     = (p.device.type == "meta") or getattr(p, "is_meta", False)
        is_cpu_stub = (p.device.type == "cpu")  and (p.numel() == 0)

        if (is_meta or is_cpu_stub) and self.ssd_enabled and full_name:
            # ç¡®ä¿æœ¬å±‚å·²æœ‰ CPU cacheï¼ˆæ²¡æœ‰å°±ç«‹å³åŠ è½½ï¼‰
            if (layer_idx not in self.cpu_cache):
                try:
                    self._load_layer_to_cpu(int(layer_idx))
                except Exception:
                    pass

            cached = _fetch_from_cpu_cache(full_name)
            # å½¢çŠ¶ä¿®å¤ï¼šè‹¥ cache çš„ key ä¸æœŸæœ› shape ä¸é…ï¼Œå°è¯•åŒæ—åˆ«å
            expected = tuple(getattr(getattr(m, local_param_name), "shape", ()))
            chosen_name, chosen_tensor = None, None

            def _try_pick(names: list[str]):
                nonlocal chosen_name, chosen_tensor
                for nm in names:
                    t = _fetch_from_cpu_cache(nm)
                    if t is not None and (not expected or tuple(t.shape) == expected):
                        chosen_name, chosen_tensor = nm, t
                        break

            if cached is not None and (not expected or tuple(cached.shape) == expected):
                chosen_name, chosen_tensor = full_name, cached
            else:
                cand = []
                if module_name in ("wq", "wk", "wv"):
                    cand = [f"layers.{layer_idx}.attention.{x}.{local_param_name}" for x in ("wq","wk","wv")]
                elif module_name in ("w1", "w2", "w3"):
                    cand = [f"layers.{layer_idx}.feed_forward.{x}.{local_param_name}" for x in ("w1","w2","w3")]
                else:
                    cand = [full_name]
                _try_pick(cand)
                if chosen_tensor is None and cached is not None:
                    chosen_name, chosen_tensor = full_name, cached  # é€€è€Œæ±‚å…¶æ¬¡

            if chosen_tensor is not None:
                _h2d_stream = self._select_h2d_stream_for(module_name=module_name)
                with torch.cuda.stream(_h2d_stream):
                    _h2d_start = torch.cuda.Event(enable_timing=True)
                    _h2d_end   = torch.cuda.Event(enable_timing=True)
                    _h2d_start.record(_h2d_stream)
                    p_gpu = chosen_tensor.to(self.device, non_blocking=True)
                    _h2d_end.record(_h2d_stream)
                if G_PERF is not None:
                    G_PERF.record_h2d_pair(_h2d_start, _h2d_end, meta={"layer": int(layer_idx) if layer_idx is not None else None, "name": params_full_names.get(local_param_name, full_name)})
                params_to_replace[local_param_name] = torch.nn.Parameter(p_gpu, requires_grad=p.requires_grad)
                params_full_names[local_param_name] = chosen_name or full_name
                if getattr(self, "verbose", False):
                    print(f"[WSM DEBUG] âœ“ Loaded {'meta' if is_meta else 'stub'} param {params_full_names[local_param_name]} to GPU: {tuple(p_gpu.shape)}")
            else:
                if getattr(self, "verbose", False):
                    print(f"[WSM WARN] CPU cache miss for {full_name} (layer {layer_idx}); will rely on ensure_group_on_gpu() later")
            continue  # è¯¥å‚æ•°å¤„ç†å®Œæ¯•

        # å…¶å®ƒæƒ…å†µï¼šæ²¿ç”¨åŸæ¥çš„ CPUâ†’GPU é€»è¾‘
        self._ensure_param_on_gpu(p, layer_idx, full_name)

    # å®‰è£…æ›¿æ¢åçš„ Parameterï¼Œå¹¶ç»´æŠ¤ name æ˜ å°„
    for pname, new_param in params_to_replace.items():
        m._parameters[pname] = new_param
        full = params_full_names.get(pname)
        if full:
            try:
                pobj = getattr(m, pname)
            except Exception:
                pobj = new_param
            self.name_to_param[full] = pobj
            self.param_owner[full]   = (m, pname)

    # buffer ç»´æŒåŸæœ‰ç­–ç•¥ï¼šmetaâ†’materializeï¼ŒCPUâ†’ä¸Šå¡
    for b in m.buffers(recurse=True):
        if getattr(b, "is_meta", False):
            try:
                b = b.to_empty(device=self.device)
            except Exception:
                pass
        elif b.device.type == "cpu":
            _h2d_stream = self._select_h2d_stream_for(module_name=module_name)
            with torch.cuda.stream(_h2d_stream):
                _h2d_start = torch.cuda.Event(enable_timing=True)
                _h2d_end   = torch.cuda.Event(enable_timing=True)
                _h2d_start.record(_h2d_stream)
                b_gpu = b.detach().to(self.device, non_blocking=True)
                _h2d_end.record(_h2d_stream)
            if G_PERF is not None:
                G_PERF.record_h2d_pair(_h2d_start, _h2d_end, meta={"layer": int(layer_idx) if layer_idx is not None else None, "buffer": True})
            try:
                b.data = b_gpu
            except Exception:
                pass


# ---------- å±‚çº§ forward åŒ…è£¹ä»¥æµ‹ GPU è®¡ç®—æ—¶é—´ï¼Œå¹¶æ¨æ–­ prefill/decoder ----------
def _guess_phase_from_args_kwargs(args, kwargs):
    # ä¾æ®å¸¸è§ç­¾åï¼šè¾“å…¥å¼ é‡å½¢çŠ¶æ¨æ–­ seq_lenï¼Œstart_pos/cache_position æ¨æ–­é˜¶æ®µ
    seq_len = None
    # å°è¯•ä» args ä¸­æ‰¾å¼ é‡
    def _find_first_tensor(a):
        if isinstance(a, torch.Tensor):
            return a
        if isinstance(a, (tuple, list)) and a:
            for x in a:
                t = _find_first_tensor(x)
                if t is not None:
                    return t
        if isinstance(a, dict):
            for v in a.values():
                t = _find_first_tensor(v)
                if t is not None:
                    return t
        return None

    t = None
    for a in args:
        t = _find_first_tensor(a)
        if t is not None:
            break
    if t is None:
        for v in kwargs.values():
            t = _find_first_tensor(v)
            if t is not None:
                break

    if t is not None and hasattr(t, 'shape') and len(t.shape) >= 2:
        # å°è¯• [B, T, C] æˆ– [T, B, C] ä¸¤ç§
        B,T = None,None
        s = list(t.shape)
        # ç®€å•å¯å‘å¼ï¼šç»´åº¦ä¸­è¾ƒå°çš„ï¼ˆé€šå¸¸ < 8ï¼‰å¯èƒ½æ˜¯ batch
        if len(s) >= 3:
            candidates = [(s[0], s[1]), (s[1], s[0])]
            # é€‰æ‹© T è¾ƒå¤§è€…ä½œä¸º seq_len
            if candidates[0][0] <= 8:
                B,T = candidates[0]
            elif candidates[1][0] <= 8:
                B,T = candidates[1]
            else:
                # æ— æ˜æ˜¾ batchï¼Œå– max ä½œä¸º T
                T = max(s[0], s[1])
        else:
            # 2D å¼ é‡ï¼Œå–è¾ƒå¤§ç»´
            T = max(s[0], s[1])
        seq_len = T

    start_pos = kwargs.get("start_pos", None)
    cache_pos = kwargs.get("cache_position", None)
    position_ids = kwargs.get("position_ids", None)

    phase = None
    if seq_len is not None:
        if seq_len == 1:
            phase = "decode"
        elif seq_len > 1:
            phase = "prefill"

    # æ ¹æ® start/cache pos å¾®è°ƒ
    for pos in (start_pos, cache_pos):
        try:
            if pos is not None:
                pos_val = int(pos) if isinstance(pos, (int, float)) else int(pos[0] if isinstance(pos, (list,tuple)) else pos.item())
                if pos_val == 0 and seq_len and seq_len > 1:
                    phase = "prefill"
                elif pos_val > 0 and seq_len == 1:
                    phase = "decode"
        except Exception:
            pass

    return phase, seq_len

def wrap_model_layers_for_timing(llama):
    if not torch.cuda.is_available():
        print("[TIMER] CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡ GPU è®¡ç®—è®¡æ—¶åŒ…è£¹ã€‚")
        return

    model = getattr(llama, "model", llama)

    # å¯»æ‰¾å¯èƒ½çš„å±‚åˆ—è¡¨å±æ€§
    layers = getattr(model, "layers", None)
    if layers is None and hasattr(model, "model"):
        layers = getattr(model.model, "layers", None)
    if layers is None:
        # å…œåº•ï¼šå°è¯•æŒ‰å¸¸è§å‘½åç©ºé—´æœé›†å­æ¨¡å—
        layers = [m for m in model.modules() if hasattr(m, "forward")]
        print(f"[TIMER] æœªæ‰¾åˆ°æ ‡å‡† layers åˆ—è¡¨ï¼Œfallback åŒ…è£¹ {len(layers)} ä¸ªæ¨¡å—ï¼Œå¯èƒ½è¾ƒé‡ã€‚")
    else:
        print(f"[TIMER] åŒ…è£¹ {len(layers)} ä¸ª Transformer å±‚ç”¨äº GPU è®¡ç®—è®¡æ—¶ã€‚")

    def _wrap_one(layer, layer_idx):
        if hasattr(layer, "_orig_forward_for_timer"):
            return
        layer._orig_forward_for_timer = layer.forward
        def _timed_forward(*args, **kwargs):
            stream = torch.cuda.current_stream()
            start_evt = torch.cuda.Event(enable_timing=True)
            end_evt   = torch.cuda.Event(enable_timing=True)
            start_evt.record(stream)
            out = layer._orig_forward_for_timer(*args, **kwargs)
            end_evt.record(stream)
            if G_PERF is not None:
                phase, seq_len = _guess_phase_from_args_kwargs(args, kwargs)
                G_PERF.record_compute_pair(start_evt, end_evt, meta={"layer": layer_idx, "phase": phase, "seq_len": seq_len})
            return out
        layer.forward = _timed_forward

    # è‹¥æ˜¯åºåˆ—å®¹å™¨
    try:
        for i,layer in enumerate(layers):
            _wrap_one(layer, i)
    except Exception:
        # å°è¯•å¯¹æ‰€æœ‰å­æ¨¡å—åŒ…è£¹ï¼ˆä¿å®ˆï¼‰
        for i,layer in enumerate(list(model.modules())):
            try:
                _wrap_one(layer, i)
            except Exception:
                pass

# ===== è·¯å¾„ä¸å¸¸é‡ï¼ˆæŒ‰ä½ çš„ç¯å¢ƒï¼‰ =====
PROMPT_TXT = Path("/home/roger/llama3-inference/prompts/prompts_batch512_len2048.txt")
RAW_DEV    = "/dev/nvme0n1p4"
MANIFEST   = "/data1/70b-fixed.runtime_manifest.json"
CKPT_DIR   = "/home/roger/.llama/checkpoints/Llama3.1-70B"

# ---------- ç³»ç»Ÿ/GPU å†…å­˜å¿«ç…§ ----------
def _read_status():
    def _grep(path, keys):
        out = {}
        try:
            with open(path, "r") as f:
                for line in f:
                    for k in keys:
                        if line.startswith(k + ":"):
                            out[k] = line.split(":")[1].strip()
        except Exception:
            pass
        return out
    s = _grep("/proc/self/status", ["VmRSS","VmHWM","VmLck"])
    m = _grep("/proc/meminfo", ["MemAvailable","CommitLimit","Committed_AS","Cached","Buffers"])
    return s, m

def _gpu_mem():
    if not torch.cuda.is_available():
        return {}
    dev = torch.cuda.current_device()
    st  = torch.cuda.memory_stats(dev)
    return {
        "alloc_GB": st.get("allocated_bytes.all.current", 0)/(1<<30),
        "rsrv_GB":  st.get("reserved_bytes.all.current", 0)/(1<<30),
    }

def probe(stage: str):
    s, m = _read_status()
    g    = _gpu_mem()
    print(f"\n[MEM] {stage}")
    print(f"  VmRSS={s.get('VmRSS','?')}  VmLck(pinned)={s.get('VmLck','?')}  "
          f"CommitLimit={m.get('CommitLimit','?')}  Committed_AS={m.get('Committed_AS','?')}  "
          f"MemAvailable={m.get('MemAvailable','?')}")
    if g:
        print(f"  GPU: allocated={g['alloc_GB']:.2f} GiB  reserved={g['rsrv_GB']:.2f} GiB")
    print()

# ---------- æ‰«æå‚æ•°åœ¨ meta/cpu/cuda ä¸Šçš„å ç”¨ ----------
def dump_param_inventory(model, tag):
    buckets = {"cpu":0, "cuda":0, "meta":0, "other":0}
    big_cpu = []
    for n,p in model.named_parameters(recurse=True):
        b = p.numel() * p.element_size()
        if getattr(p, "is_meta", False):
            buckets["meta"] += b
        elif hasattr(p, "device"):
            t = p.device.type
            if t == "cpu":
                buckets["cpu"] += b
                if b >= (64<<20):
                    big_cpu.append((n,b))
            elif t == "cuda":
                buckets["cuda"] += b
            else:
                buckets["other"] += b
        else:
            buckets["other"] += b
    f = lambda x: f"{x/(1<<30):.2f} GiB"
    print(f"[PARAMS] {tag}: cpu={f(buckets['cpu'])}, cuda={f(buckets['cuda'])}, meta={f(buckets['meta'])}, other={f(buckets['other'])}")
    if big_cpu:
        big_cpu.sort(key=lambda x:-x[1])
        print("  [big-cpu] top:")
        for n,b in big_cpu[:10]:
            print(f"   - {n}  {b/(1<<20):.1f} MiB")

# ---------- è¿è¡Œæ—¶è¦†ç›–ï¼šæ”¶æ•› pinned/æ³¨å†Œæ±  ----------
def apply_runtime_overrides():
    """
    æŠŠæ³¨å†Œæ€»é‡é’³åœ¨ â‰¤256MiBï¼Œå¹¶æŠŠ EXTENT_BYTES é™åˆ° 1MiBï¼Œé™ä½é«˜é˜¶é¡µ order å‹åŠ›ã€‚
    """
    cfg = load_runtime_config({
        "pinned": {
            "WEIGHT_PINNED_BYTES":      8  << 30,
            "KV_PINNED_BYTES":          6  << 30,
            "EXTENT_BYTES":             1  << 20,   # 1MiB
            "PINNED_REGISTER_CHUNK":   16  << 20,   # 16MiB
            "PINNED_REGISTER_N":            8,      # 128MiB
        },
        "regpool": {
            "REG_POOL_N_BUFFERS":           8,
            "REG_POOL_BUF_BYTES":     16 << 20,     # ~128MiB ä¼ é€å¸¦
        },
        "io": {
            "RAW_IO_QD_WRITE":             24,      # å†™é˜Ÿåˆ—æ·±åº¦
            "IO_RAW_THROTTLE_MS":          30,      # å†™å¸¦å®½çª—å£
        }
    })
    D = runtime_config_to_dict(cfg)
    p = D["pinned"]
    need  = int(p["WEIGHT_PINNED_BYTES"])
    chunk = int(p["PINNED_REGISTER_CHUNK"])
    target_total = min(need // 2, 256 << 20)  # ç›®æ ‡ â‰¤ 256MiB
    newN = max(1, target_total // chunk)
    p["PINNED_REGISTER_N"] = newN
    cfg = load_runtime_config({"pinned": p, "io": D["io"]})
    print("[RuntimeConfig] pinned =", runtime_config_to_dict(cfg)["pinned"])
    print("[RuntimeConfig] io =", runtime_config_to_dict(cfg)["io"])
    return cfg

# ---------- KV æ± ï¼šæ‡’åˆ†é… + å•å— â‰¥ å•ä¸ª KV å— ----------
def configure_kv_pool():
    # DRAM é…ç½®
    KVCacheArgs.dram_limit_gb     = 24.0
    KVCacheArgs.dram_sizing_batch = 32
    KVCacheArgs.block_bytes       = 4 * 1024 * 1024
    KVCacheArgs.preallocate       = False
    KVCacheArgs.lazy_init         = True

    # å…³é—­ push å³æ—¶é•œåƒï¼Œé‡‡ç”¨åç§»/èšåˆå†™ï¼ˆé¿å…ä¸æƒé‡ H2D å†²çªï¼‰
    KVCacheArgs.mirror_on_push = False

    # I/O èŠ‚æµä¸å†™é€Ÿç‡é…ç½®ï¼ˆä¸æƒé‡ H2D ä»²è£ï¼‰
    KVCacheArgs.IO_RAW_THROTTLE_MS     = 30
    KVCacheArgs.NVME_WRITE_TARGET_MBPS = 1200

    if hasattr(KVCacheArgs, "prefer_bf16"):
        KVCacheArgs.prefer_bf16 = True

    print(f"[KVArgs] dram_limit={KVCacheArgs.dram_limit_gb} GiB, "
          f"block_bytes={KVCacheArgs.block_bytes//(1<<20)} MiB, prealloc={KVCacheArgs.preallocate}")
    print(f"[KVArgs] mirror_on_push={KVCacheArgs.mirror_on_push}, "
          f"IO_RAW_THROTTLE_MS={KVCacheArgs.IO_RAW_THROTTLE_MS}, "
          f"NVME_WRITE_TARGET_MBPS={KVCacheArgs.NVME_WRITE_TARGET_MBPS}")

# ---------- è¯†åˆ«â€œå®é™…è¿è¡Œçš„æ¨¡å¼â€ ----------
def classify_mode(llama) -> str:
    """
    è¿”å›ï¼š'ssd-streaming' / 'cpu-gpu-streaming' / 'full-gpu' / 'full-cpu' / 'meta-only'
    å¹¶æ‰“å°åˆ¤æ®ï¼Œæ–¹ä¾¿ç¡®è®¤ç°åœ¨åˆ°åº•è·‘çš„æ˜¯ä»€ä¹ˆã€‚
    """
    m = llama.model
    # 1) æ˜¯å¦è£…äº† WSMï¼ˆå¹¶ä¸”å¸¦ SSDï¼‰
    if hasattr(llama, "weight_streaming_manager"):
        wsm = llama.weight_streaming_manager
        ssd = bool(getattr(wsm, "ssd_enabled", False) or getattr(wsm, "ssd", None))
        cpu_warm = getattr(wsm, "disable_cpu_warm", None)
        mode = "ssd-streaming" if ssd else "cpu-gpu-streaming"
        print(f"[MODE] detected={mode}  (has WSM, ssd={ssd}, disable_cpu_warm={cpu_warm})")
        return mode
    # 2) æ—  WSMï¼šçœ‹å‚æ•°åˆ†å¸ƒ
    cpu, cuda, meta = 0,0,0
    for _,p in m.named_parameters():
        b = p.numel()*p.element_size()
        if getattr(p, "is_meta", False): meta += b
        elif p.device.type == "cpu":     cpu  += b
        elif p.device.type == "cuda":    cuda += b
    if cuda > 0 and cpu == 0 and meta == 0:
        print("[MODE] detected=full-gpu"); return "full-gpu"
    if cpu  > 0 and cuda == 0 and meta == 0:
        print("[MODE] detected=full-cpu"); return "full-cpu"
    if meta > 0 and cpu == 0 and cuda == 0:
        print("[MODE] detected=meta-only"); return "meta-only"
    print("[MODE] mixed/unrecognized (check PARAMS dump below)")
    return "unknown"

def main():
    # åŸºç¡€ç³»ç»Ÿå¼€é”€æ”¶æ•›
    os.environ.setdefault("OMP_NUM_THREADS",  "8")
    os.environ.setdefault("MALLOC_ARENA_MAX", "2")

    # ============================================================
    # â­ ç»„çº§ GPU é¢„å–ï¼ˆahead=4ï¼‰+ ç»„é¢„ç®— + ç­‰æ°´ä½è°ƒåº¦
    # ============================================================
    GPU_AHEAD_LAYERS = 4
    # GPU_MAX_GROUPS   = max(10, 2 + GPU_AHEAD_LAYERS * 2 + 1)  # â‰ˆ 11ï¼šå½“å‰(2) + é¢„å–(8) + ç¼“å†²(1)
    GPU_MAX_GROUPS = 10

    os.environ.setdefault("WSM_GPU_MAX_GROUPS", str(GPU_MAX_GROUPS))
    os.environ.setdefault("WSM_GROUP_PREFETCH_DEPTH", str(GPU_AHEAD_LAYERS))
    os.environ.setdefault("WSM_BALANCE_PREFETCH", "1")
    os.environ.setdefault("WSM_PAIR_AHEAD", "2")      # åŒå±‚â†’i+1â†’i+2
    os.environ.setdefault("WSM_KIND_AHEAD_CAP", "2")  # å•ä¸€ç±»å‹æœ€å¤§å‰ç»è·ç¦»
    os.environ.setdefault("WSM_H2D_GROUP_BACKLOG_MAX", "12")

    # è®¡ç®—ç»“æŸç«‹åˆ»é‡Šæ”¾ï¼ˆé¿å…ç»„å †ç§¯ï¼‰
    os.environ.setdefault("WSM_EVICT_FINISHED", "1")  # â† ä¿®æ­£ä¸º 1ï¼ˆä½ çš„è‰ç¨¿é‡Œè¯¯å†™æˆäº† 0ï¼‰
    os.environ.setdefault("WSM_GRP_RETAIN_MS", "3")   # æçŸ­ä¿ç•™çª—å£

    # è·³è¿‡é¢„åŠ è½½ç­‰å¾…ï¼šè¾¹è·‘è¾¹æ»šåŠ¨é¢„å–
    os.environ.setdefault("WSM_SKIP_PRELOAD_WAIT", "1")

    # ============================================================
    # â­ ç¯å½¢ CPU çª—å£ï¼ˆSSD -> pinned DRAMï¼Œ80 å±‚å–æ¨¡ï¼‰
    # ============================================================
    CPU_CAP_VALUE    = 40   # i+4..i+4+cap çš„ cap
    CPU_RING_OFFSET  = 4    # çª—å£ä» i+4 èµ·
    os.environ.setdefault("WSM_CPU_RING_MODE",     "1")
    os.environ.setdefault("WSM_CPU_RING_OFFSET",   str(CPU_RING_OFFSET))
    os.environ.setdefault("WSM_CPU_CACHE_CAP_LAYERS", str(CPU_CAP_VALUE))
    os.environ.setdefault("WSM_CPU_CACHE_HWM_LAYERS", str(CPU_CAP_VALUE + 3))
    os.environ.setdefault("WSM_CPU_CACHE_LWM_LAYERS", str(max(2, CPU_CAP_VALUE - 3)))
    os.environ.setdefault("WSM_CPU_BACK_MARGIN",   "4")

    # â€”â€” H2D/KV ä¼ è¾“ä»²è£ï¼ˆé˜²æ­¢ä¸¤è¾¹æŠ¢å¸¦å®½ï¼‰â€”â€”
    os.environ.setdefault("WSM_KV_THROTTLE_THRESHOLD", "2")
    os.environ.setdefault("WSM_KV_THROTTLE_MS",        "16")

    # é…ç½®æ€»ç»“
    print("=" * 80)
    print("ğŸ”§ ç»„çº§ GPU é¢„å–ï¼ˆahead=4ï¼‰+ ç¯å½¢ CPU çª—å£")
    print("=" * 80)
    print(f"GPU é¢„å–è·ç¦»: {GPU_AHEAD_LAYERS} å±‚")
    print(f"GPU ç»„é¢„ç®—:   {GPU_MAX_GROUPS} ç»„(attn/ffn)")
    print(f"CPU çª—å£å®¹é‡: {CPU_CAP_VALUE} å±‚ (ç¯å½¢ï¼Œå¯¹ 80 å±‚å–æ¨¡)")
    print(f"CPU ç¯å½¢åç§»: i+{CPU_RING_OFFSET}")
    print("=" * 80)

    device = "cuda:0" if torch.cuda.is_available() else "cpu"

    # æ€§èƒ½è®¡æ—¶å™¨åˆå§‹åŒ–
    global G_PERF
    G_PERF = _PerfRecorder(device)
    print(f"[PERF] G_PERF initialized: {G_PERF}, cuda={G_PERF.cuda}")

    # 1) è¦†ç›– pinned/æ³¨å†Œæ±  + KV æ± 
    apply_runtime_overrides()
    configure_kv_pool()
    probe("after runtime clamp")

    # 2) WSMï¼ˆSSD æµå¼ï¼‰æ„é€ å‚æ•°ï¼šå…³é—­æ•´å±‚é¢„å–ï¼Œæ”¹ç”¨ç»„çº§çª—å£
    mode_config = {
        "raw_device": RAW_DEV,
        "ssd_manifest_path": MANIFEST,
        "prefetch_distance": 0,                     # å…³é—­æ•´å±‚é¢„å–
        "group_prefetch_depth": GPU_AHEAD_LAYERS,   # ç»„çº§é¢„å–æ·±åº¦ï¼ˆ=4ï¼‰
        "max_cached_layers": 8,                     # ç»„çº§èµ·ä¸»å¯¼ï¼Œè¿™é‡Œä»…ä½œä¿é™©
        "cpu_cache_layers": CPU_CAP_VALUE,          # CPU ç¯å½¢å®¹é‡
        "warmup_layers": 1,                         # è‡³å°‘é¢„çƒ­ç¬¬ 0 å±‚åˆ° CPU
        "staging_mb": 64,
        "verbose": True,
    }

    # 3) æ„å»ºï¼ˆmeta + SSD æµå¼ï¼‰ï¼Œä¸ä¼šæŠŠ 70B æƒé‡å…¨è½½å…¥ CPU
    probe("before LLaMA.build")
    print("[CHECK] calling LLaMA.build(mode='mixed', load_model=False)")
    llama = LLaMA.build(
        checkpoints_dir=CKPT_DIR,
        load_model=False,           # å…³é”®ï¼šä¸æŠŠ checkpoint è½½å…¥ CPU
        device=device,
        max_seq_len=2048,
        max_batch_size=32,
        topk_blk=8,
        mode="mixed",
        mode_config=mode_config
    )
    probe("after LLaMA.build")

    # â­ å°† G_PERF æ³¨å…¥åˆ° llama3 æ¨¡å—ä¸­ï¼Œè®©å®ƒä»¬èƒ½å¤Ÿè®¿é—®åˆ°å…¨å±€ recorder
    import llama3.layers as layers_module
    import llama3.weight_streaming_manager as wsm_module
    layers_module.G_PERF = G_PERF
    wsm_module.G_PERF = G_PERF
    print(f"[PERF] Injected G_PERF into llama3 modules")

    # ç»‘å®š WSM è¡¥ä¸
    wsm = getattr(llama, "weight_streaming_manager", None)
    if wsm is not None:
        # â­ å…³é”®ï¼šå…ˆä¿å­˜åŸå§‹æ–¹æ³•ï¼Œé¿å…é€’å½’è°ƒç”¨ï¼
        wsm._original_wait_group_ready = wsm.wait_group_ready
        # ç„¶åç”¨ patch ç‰ˆæœ¬æ›¿æ¢
        wsm.wait_group_ready     = types.MethodType(_patched_wait_group_ready, wsm)
        wsm._ensure_module_on_gpu = types.MethodType(_patched_ensure_module_on_gpu, wsm)
        print("[WSM PATCH] strict group-ready + CPU stub loader enabled")

    # åŒ…è£¹æ¨¡å‹å±‚ï¼Œè®°å½• GPU è®¡ç®—äº‹ä»¶
    try:
        wrap_model_layers_for_timing(llama)
    except Exception as e:
        print(f"[TIMER] å±‚åŒ…è£¹å¤±è´¥ï¼š{e}")

    # è¯†åˆ«/æ‰“å°"å®é™…æ¨¡å¼" + å‚æ•°åˆ†å¸ƒ
    mode = classify_mode(llama)
    dump_param_inventory(llama.model, f"after build ({mode})")

    # 4) è¯»å– prompt å¹¶åšâ€œå®‰å…¨è£å‰ªâ€ï¼ˆmax_gen_len=32ï¼‰
    try:
        prompt_path = PROMPT_TXT
        prompt = prompt_path.read_text(encoding="utf-8").strip()
    except Exception as e:
        raise RuntimeError(f"æ— æ³•è¯»å– {prompt_path}: {e}")

    # â€”â€” å®‰å…¨è£å‰ªï¼šæŒ‰ tokenizer é™åˆ¶ prompt token æ•°
    max_gen_len = 32  
    max_prompt_tokens = llama.args.max_seq_len - max_gen_len
    tok = llama.tokenizer.encode(prompt, add_special_tokens=False)
    if len(tok) > max_prompt_tokens:
        tok = tok[-max_prompt_tokens:]
        prompt = llama.tokenizer.decode(tok)

    # 5) çœŸæ­£æ¨ç†ï¼ˆdecodeï¼‰
    probe("before inference (decode)")
    if G_PERF is not None:
        G_PERF.host_start()
        if torch.cuda.is_available():
            G_PERF.record_t0()
    out_tokens, out_texts = llama.text_completion(
        prompts=[prompt],
        temperature=0.0,
        max_gen_len=max_gen_len,
        batch_size=1,
    )
    probe("after inference (decode)")
    if G_PERF is not None:
        G_PERF.host_end()
        # æ±‡æ€»å¹¶æ‰“å°æŠ¥å‘Š
        extra = {}
        try:
            prompt_tok = len(tok)
        except Exception:
            prompt_tok = None
        try:
            gen_tok = len(out_tokens[0]) if isinstance(out_tokens, (list,tuple)) else None
        except Exception:
            gen_tok = None
        if prompt_tok is not None:
            extra['Prompt tokens'] = prompt_tok
        if gen_tok is not None:
            extra['Generated tokens'] = gen_tok
            if (G_PERF.host_infer_start is not None) and (G_PERF.host_infer_end is not None):
                ms = max(1e-9, (G_PERF.host_infer_end - G_PERF.host_infer_start)*1000.0)
                total_tok = (prompt_tok or 0) + (gen_tok or 0)
                extra['Throughput(est)'] = f"{total_tok / (ms/1000.0):.2f} tok/s"
        G_PERF.pretty_print(extra=extra)

    print(f"\n========== Generation (len={max_gen_len}) ==========")
    print(out_texts[0])
    print("=========================================")

if __name__ == "__main__":
    main()
