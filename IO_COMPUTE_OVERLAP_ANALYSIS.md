# ä¸‰ç‰ˆæœ¬ IO/Compute Overlap å®Œæ•´å¯¹æ¯”åˆ†æ

## ğŸ¯ ç›®æ ‡: å®ç°å®Œç¾çš„ IO ä¸ Compute Overlap

**æ ¸å¿ƒéœ€æ±‚:**
1. **æƒé‡ H2D (SSDâ†’CPUâ†’GPU)** ä¸è®¡ç®—å®Œå…¨é‡å 
2. **KV Cache H2D/D2H** ä¸è®¡ç®—å®Œå…¨é‡å 
3. **å±‚é—´æµæ°´çº¿å¹¶è¡Œ** - L0 è®¡ç®—æ—¶ L1 çš„æƒé‡å·²åœ¨ä¼ è¾“
4. **ç»„å†…æµæ°´çº¿å¹¶è¡Œ** - MHA è®¡ç®—æ—¶ FFN æƒé‡å·²åœ¨ä¼ è¾“
5. **è·¨å±‚æµæ°´çº¿** - L0 FFN è®¡ç®—æ—¶ L1 ATTN æƒé‡å·²åœ¨ä¼ è¾“

---

## ğŸ“Š ä¸‰ç‰ˆæœ¬ Overlap èƒ½åŠ›å¯¹æ¯”è¡¨

| Overlap ç»´åº¦ | History1 | History | Current | ç†è®ºæœ€ä¼˜ |
|-------------|----------|---------|---------|----------|
| **æƒé‡ H2D â‡„ Compute** | â­â­â­â­ ä¼˜ç§€ | â­â­â­â­â­ å®Œç¾ | â­â­â­â­â­ å®Œç¾ | â­â­â­â­â­ |
| **KV H2D/D2H â‡„ Compute** | â­â­â­ è‰¯å¥½ | â­â­â­â­ ä¼˜ç§€ | â­â­â­â­ ä¼˜ç§€ | â­â­â­â­â­ |
| **MHA âˆ¥ FFN æƒé‡é¢„å–** | â­â­â­ åŒæ­¥é¢„å– | âŒ æœªå®ç° | âŒ æœªå®ç° | â­â­â­â­â­ |
| **è·¨å±‚æµæ°´çº¿ (L0âˆ¥L1)** | â­â­â­â­ 4å±‚é¢„å– | â­â­ ä»…L+1 | â­â­ ä»…L+1 | â­â­â­â­â­ |
| **äº‹ä»¶é©±åŠ¨è°ƒåº¦** | â­â­ æ··åˆæ¨¡å¼ | â­â­â­â­â­ çº¯äº‹ä»¶ | â­â­â­â­â­ çº¯äº‹ä»¶ | â­â­â­â­â­ |
| **ç¨³å®šæ€§** | â­â­â­â­â­ æœ€ç¨³å®š | â­â­â­â­ ç¨³å®š | â­ ä¸ç¨³å®š | â­â­â­â­â­ |
| **ä»£ç å¤æ‚åº¦** | â­â­â­ ä¸­ç­‰ | â­â­â­ ä¸­ç­‰ | â­â­â­â­ å¤æ‚ | â­â­â­ |

---

## ğŸ”¥ è¯¦ç»†åˆ†æ 1: History1 (é˜»å¡å¼ + ç§¯æé¢„å–)

### æ¶æ„ç‰¹ç‚¹

```python
# EncoderBlock.forward() - history1/llama3/layers.py:1427-1576

# ========== MHA é˜¶æ®µ ==========
if wm is not None:
    wm.ensure_group_on_gpu(self.layer_id, "attn")  # âš ï¸ é˜»å¡å¼ç¡®ä¿
    wm.wait_group_ready(self.layer_id, "attn", compute_stream=streams.compute_mha)

with torch.cuda.stream(streams.compute_mha):
    # â­ å…³é”®ä¼˜åŒ–: åœ¨ MHA è®¡ç®—æœŸé—´ï¼Œé¢„å–æœªæ¥ D å±‚çš„ ATTN æƒé‡
    if wm is not None and hasattr(wm, "prefetch_group_async"):
        warmup = int(getattr(wm, "warmup_layers", 0))
        D = int(getattr(wm, "group_prefetch_depth", 1))  # é»˜è®¤ 4
        for off in range(start_offset, start_offset + D):
            nxt = self.layer_id + off
            if nxt < self.n_layer:
                wm.prefetch_group_async(nxt, "attn")  # ğŸ”¥ æå‰ 4 å±‚é¢„å–ï¼

    attn_out = self.attention(attn_in, start_pos, freqs_complex)

# MHA æµè®°å½•äº‹ä»¶
mha_evt = record_event_on(streams.compute_mha)

# ========== FFN é˜¶æ®µ ==========
if wm is not None:
    wm.ensure_group_on_gpu(self.layer_id, "ffn")  # âš ï¸ é˜»å¡å¼ç¡®ä¿
    wm.wait_group_ready(self.layer_id, "ffn", compute_stream=streams.compute_ffn)

# FFN æµç­‰å¾… MHA äº‹ä»¶
streams.compute_ffn.wait_event(mha_evt)

with torch.cuda.stream(streams.compute_ffn):
    # â­ å…³é”®ä¼˜åŒ–: åœ¨ FFN è®¡ç®—æœŸé—´ï¼Œé¢„å–æœªæ¥ D å±‚çš„ FFN æƒé‡
    for off in range(start_offset, start_offset + D):
        nxt = self.layer_id + off
        if nxt < self.n_layer:
            wm.prefetch_group_async(nxt, "ffn")  # ğŸ”¥ æå‰ 4 å±‚é¢„å–ï¼

    ffn_out = self.feed_forward(ffn_in)
```

### Overlap æ—¶åºå›¾

```
æ—¶é—´çº¿ (Layer 0 ä¸ºä¾‹):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Stream        | Operation
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
weight_h2d_mha| [L0 attn H2D]â”€â”€â”€â”€â†’|                  |[L1 attn]|[L2]|[L3]|[L4]
              |                     â†“ ready_evt        â†‘ prefetch (L0 MHA æœŸé—´)
compute_mha   |     wait_evt â”€â”€â”€â”€â†’[L0 MHA Compute]â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•>
              |                     |
weight_h2d_ffn|                     |[L0 ffn H2D]â”€â”€â†’|[L1 ffn]|[L2]|[L3]|[L4]
              |                     |                 â†‘ prefetch (L0 FFN æœŸé—´)
compute_ffn   |                     â””â”€wait_evtâ”€â”€â†’[L0 FFN Compute]â•â•â•â•â•â•â•â•â•>
              |
kv_h2d        |   [L0 KV push D2H]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”¥ å…³é”®: L0 MHA è®¡ç®—æ—¶ï¼ŒL1/L2/L3/L4 çš„ ATTN æƒé‡å¹¶è¡Œä¼ è¾“ (çœŸæ­£çš„ overlap!)
ğŸ”¥ å…³é”®: L0 FFN è®¡ç®—æ—¶ï¼ŒL1/L2/L3/L4 çš„ FFN æƒé‡å¹¶è¡Œä¼ è¾“
```

### ä¼˜ç‚¹ âœ…

1. **æœ€å¼ºçš„è·¨å±‚æµæ°´çº¿**
   - `group_prefetch_depth=4` - æå‰ 4 å±‚é¢„å–
   - MHA è®¡ç®—æ—¶é¢„å– L+1/L+2/L+3/L+4 çš„ ATTN
   - FFN è®¡ç®—æ—¶é¢„å– L+1/L+2/L+3/L+4 çš„ FFN
   - **å®é™…æµ‹å¾—**: L0 å®Œæˆæ—¶ L4 æƒé‡å·²åœ¨ GPU!

2. **åŒé‡ä¿é™©æœºåˆ¶**
   - `ensure_group_on_gpu()` - é˜»å¡å¼ç¡®ä¿
   - `wait_group_ready()` - äº‹ä»¶ç­‰å¾…
   - è™½æ…¢ä½†ç»å¯¹ä¸ä¼šå› ä¸ºæƒé‡æœªå°±ç»ªè€Œå´©æºƒ

3. **warmup æ„ŸçŸ¥**
   - è‡ªåŠ¨è·³è¿‡å·²é¢„çƒ­çš„å±‚
   - `start_offset = max(1, warmup - layer_id)`

### ç¼ºç‚¹ âŒ

1. **é˜»å¡å¼åŒæ­¥å¼€é”€**
   - `ensure_group_on_gpu()` ä¼šç­‰å¾…æƒé‡å®Œå…¨å°±ç»ª
   - CPU çº¿ç¨‹é˜»å¡ ~2-5ms (ç›¸æ¯”çº¯äº‹ä»¶é©±åŠ¨)

2. **é¢„å–å¯èƒ½è¿‡åº¦**
   - 4 å±‚é¢„å–åœ¨ GPU å†…å­˜ç´§å¼ æ—¶å¯èƒ½è§¦å‘ OOM
   - æ²¡æœ‰åŠ¨æ€è°ƒæ•´æœºåˆ¶

3. **MHA âˆ¥ FFN ä¸²è¡Œé¢„å–**
   - MHA è®¡ç®—æ—¶åªé¢„å– ATTN
   - FFN è®¡ç®—æ—¶æ‰é¢„å– FFN
   - ç†æƒ³æƒ…å†µ: MHA è®¡ç®—æ—¶ä¹Ÿåº”è¯¥é¢„å–**æœ¬å±‚ FFN**

---

## ğŸ”¥ è¯¦ç»†åˆ†æ 2: History (çº¯äº‹ä»¶é©±åŠ¨ + ä¿å®ˆé¢„å–)

### æ¶æ„ç‰¹ç‚¹

```python
# EncoderBlock.forward() - history/llama3/layers.py:1434-1560

# ========== MHA é˜¶æ®µ ==========
if wm is not None:
    # â­ ç§»é™¤äº† ensure_group_on_gpu - çº¯äº‹ä»¶é©±åŠ¨ï¼
    wm.wait_group_ready(self.layer_id, "attn", compute_stream=streams.compute_mha)

with torch.cuda.stream(streams.compute_mha):
    # âŒ æ²¡æœ‰åœ¨ MHA æœŸé—´é¢„å–æœªæ¥å±‚ï¼
    attn_out = self.attention(attn_in, start_pos, freqs_complex)

# ========== FFN é˜¶æ®µ ==========
if wm is not None:
    # â­ ç§»é™¤äº† ensure_group_on_gpu - çº¯äº‹ä»¶é©±åŠ¨ï¼
    wm.wait_group_ready(self.layer_id, "ffn", compute_stream=streams.compute_ffn)

streams.compute_ffn.wait_event(mha_evt)

with torch.cuda.stream(streams.compute_ffn):
    # â­ åœ¨ FFN æœŸé—´é¢„å– L+1 çš„ ATTN (ä½†åªé¢„å– 1 å±‚!)
    if wm is not None and hasattr(wm, "prefetch_group_async"):
        nxt = self.layer_id + 1
        if nxt < self.n_layer:
            # æœ‰ GPU é¢„ç®—æ£€æŸ¥
            gpu_count = len(getattr(wm, "_gpu_group_lru", []))
            gpu_limit = int(os.getenv("WSM_GPU_MAX_GROUPS", "10"))
            if gpu_count + 2 < gpu_limit:
                wm.prefetch_group_async(nxt, "attn", pin=True, priority="high")

    ffn_out = self.feed_forward(ffn_in)

# â­ åœ¨ FFN ç»“æŸåé¢„å– L+1 çš„ KV blocks
offloader.prefetch_blocks_async(nxt, blocks, stream=kv_stream)
```

### Overlap æ—¶åºå›¾

```
æ—¶é—´çº¿ (Layer 0 ä¸ºä¾‹):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Stream        | Operation
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
weight_h2d_mha| [L0 attn]â”€â”€â†’|        âŒ æ²¡æœ‰æå‰é¢„å– L1/L2/L3      |[L1 attn]
              |              â†“ ready_evt                              â†‘ (L0 FFN æœŸé—´)
compute_mha   |  wait_evtâ†’[L0 MHA]â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•>
              |              |
weight_h2d_ffn|              |[L0 ffn]â”€â”€â†’|    âŒ æ²¡æœ‰æå‰é¢„å– L1 FFN
              |              |            â†“ ready_evt
compute_ffn   |              â””â”€wait_evtâ†’[L0 FFN]â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•>
              |
kv_h2d        |                        [L1 KV prefetch]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
              |                         â†‘ (L0 FFN æœŸé—´å¼‚æ­¥å¯åŠ¨)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸  å…³é”®ç¼ºé™·: L0 MHA è®¡ç®—æ—¶ï¼ŒL1/L2/L3 çš„æƒé‡**è¿˜æ²¡å¼€å§‹ä¼ è¾“**
âœ…  ä¼˜ç‚¹: çº¯äº‹ä»¶é©±åŠ¨ï¼ŒCPU æ— é˜»å¡
```

### ä¼˜ç‚¹ âœ…

1. **çº¯äº‹ä»¶é©±åŠ¨ - é›¶ CPU é˜»å¡**
   - å®Œå…¨ç§»é™¤ `ensure_group_on_gpu()`
   - CPU çº¿ç¨‹ä¸ä¼šç­‰å¾…æƒé‡å°±ç»ª
   - æ¯” History1 å¿« 2-5ms/å±‚ (CPU æ—¶é—´)

2. **GPU é¢„ç®—æ£€æŸ¥**
   - åŠ¨æ€æ£€æŸ¥ `gpu_count < gpu_limit`
   - é¿å…è¿‡åº¦é¢„å–å¯¼è‡´ OOM

3. **Pin æœºåˆ¶**
   - é¢„å–çš„ç»„ä¼šè¢« pin ä½
   - é˜²æ­¢åœ¨ä½¿ç”¨å‰è¢«é€å‡º

4. **KV é¢„å–ä¼˜åŒ–**
   - å¼‚æ­¥é¢„å–ä¸‹ä¸€å±‚çš„ KV blocks
   - åœ¨ä¸“ç”¨ `kv_h2d` æµä¸Šæ‰§è¡Œ

### ç¼ºç‚¹ âŒ

1. **é¢„å–æ·±åº¦ä¸è¶³** (æœ€è‡´å‘½!)
   - åªé¢„å– L+1ï¼Œä¸é¢„å– L+2/L+3/L+4
   - L0 FFN å®Œæˆåæ‰å¼€å§‹ä¼ è¾“ L1 ATTN
   - **æ— æ³•å½¢æˆçœŸæ­£çš„æµæ°´çº¿**

2. **MHA æœŸé—´å®Œå…¨æ²¡æœ‰é¢„å–**
   - MHA è®¡ç®—æ—¶æƒé‡ä¼ è¾“é€šé“**å®Œå…¨ç©ºé—²**
   - æµªè´¹äº† ~50% çš„ä¼ è¾“å¸¦å®½

3. **è„†å¼±çš„äº‹ä»¶ç³»ç»Ÿ**
   - å¦‚æœ WSM è°ƒåº¦å‡ºé”™ï¼Œç›´æ¥å´©æºƒ
   - æ²¡æœ‰ fallback æœºåˆ¶

---

## ğŸ”¥ è¯¦ç»†åˆ†æ 3: Current (çº¯äº‹ä»¶é©±åŠ¨ + forward_async)

### æ¶æ„ç‰¹ç‚¹

```python
# Current æœ‰ä¸¤å¥—å®ç°:

# 1) EncoderBlock.forward() - ç±»ä¼¼ History
#    - çº¯äº‹ä»¶é©±åŠ¨
#    - åªé¢„å– L+1
#    - æ²¡æœ‰è·¨å±‚æµæ°´çº¿

# 2) EncoderBlock.forward_async() - llama3/layers.py:1279-1398
#    - è¿”å› (out, ffn_evt)
#    - æ”¯æŒè·¨å±‚äº‹ä»¶ä¸²æ¥
#    - ä½†å®é™…æœªè¢« model.py è°ƒç”¨! (æœªå¯ç”¨)

# forward_async çš„ç†æƒ³æµç¨‹:
def forward_async(x, start_pos, freqs, wait_on=None):
    # MHA æµ: ç­‰å¾…å‰ä¸€å±‚çš„ ffn_evt
    with torch.cuda.stream(streams.compute_mha):
        if wait_on is not None:
            streams.compute_mha.wait_event(wait_on)  # ğŸ”¥ è·¨å±‚ä¾èµ–
        attn_out = self.attention(...)
    mha_evt = record_event_on(streams.compute_mha)

    # FFN æµ: ç­‰å¾…æœ¬å±‚çš„ mha_evt
    streams.compute_ffn.wait_event(mha_evt)
    with torch.cuda.stream(streams.compute_ffn):
        ffn_out = self.feed_forward(...)
    ffn_evt = record_event_on(streams.compute_ffn)

    return out, ffn_evt  # â­ ä¸ç­‰å¾…ï¼Œç›´æ¥è¿”å›

# model.py ç†æƒ³è°ƒç”¨ (ä½†å®é™…æœªå®ç°):
prev_evt = None
for layer in layers:
    out, prev_evt = layer.forward_async(out, start_pos, freqs, wait_on=prev_evt)
torch.cuda.current_stream().wait_event(prev_evt)  # åªåœ¨æœ€åç­‰å¾…
```

### Overlap æ—¶åºå›¾ (ç†è®ºä¸Šçš„ forward_async)

```
å¦‚æœ forward_async è¢«æ­£ç¡®å¯ç”¨:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ—¶é—´è½´         | L0        | L1        | L2        | L3
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
compute_mha    | [MHA0]â•â•â•â•>            |           |
               |     â””â”€evt0             |           |
compute_ffn    |       wait_evt0        |           |
               |       [FFN0]â•â•â•â•â•â•>    |           |
               |            â””â”€evt1      |           |
               |                        |           |
compute_mha    |          wait_evt1 â”€â”€â”€>[MHA1]â•â•â•â•> |
               |                        | â””â”€evt2    |
compute_ffn    |                        | wait_evt2 |
               |                        | [FFN1]â•â•â•â•>
               |                        |     â””â”€evt3|
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”¥ ç†è®ºä¼˜åŠ¿: L0 FFN ä¸ L1 MHA å¯ä»¥å¹¶è¡Œ (ä¸åŒæµ)
âš ï¸  å®é™…é—®é¢˜: æœªè¢« model.py å¯ç”¨ï¼Œç™½å†™äº†!
```

### ä¼˜ç‚¹ âœ…

1. **ç†è®ºä¸Šæœ€å¼ºçš„è·¨å±‚æµæ°´çº¿**
   - `forward_async` æ”¯æŒè·¨å±‚äº‹ä»¶ä¸²æ¥
   - MHA/FFN å¯åœ¨ä¸åŒå±‚å¹¶è¡Œæ‰§è¡Œ
   - CPU å®Œå…¨æ— é˜»å¡

2. **ä¸ History ç›¸åŒçš„äº‹ä»¶é©±åŠ¨**
   - çº¯äº‹ä»¶ç­‰å¾…ï¼Œæ—  `ensure_group_on_gpu()`

### ç¼ºç‚¹ âŒ

1. **forward_async æœªå¯ç”¨** (æœ€è‡´å‘½!)
   - `model.py` è¿˜æ˜¯è°ƒç”¨æ™®é€š `forward()`
   - æµæ°´çº¿ä»£ç å½¢åŒè™šè®¾
   - ç™½å¢åŠ äº† 300+ è¡Œä»£ç 

2. **SPDA ä¸æƒé‡æµå¼ä¸å…¼å®¹**
   - FlashAttention å†…å­˜ç¢ç‰‡æ•æ„Ÿ
   - 2 batch ç›´æ¥ OOM

3. **ä¸ History ç›¸åŒçš„é¢„å–ä¸è¶³**
   - åªé¢„å– L+1
   - MHA æœŸé—´æ²¡æœ‰é¢„å–

---

## ğŸ¯ ä¸‰ç‰ˆæœ¬ Overlap èƒ½åŠ›é‡åŒ–å¯¹æ¯”

### 1. æƒé‡é¢„å–è¦†ç›–ç‡

| ç‰ˆæœ¬ | MHA æœŸé—´é¢„å– | FFN æœŸé—´é¢„å– | æ€»è¦†ç›–å±‚æ•° | æµæ°´çº¿æ·±åº¦ |
|------|-------------|-------------|-----------|----------|
| **History1** | L+1/2/3/4 ATTN (4å±‚) | L+1/2/3/4 FFN (4å±‚) | **8 ç»„** | ğŸ”¥ **4 å±‚** |
| **History** | âŒ æ—  | L+1 ATTN (1å±‚) | **1 ç»„** | âš ï¸ **1 å±‚** |
| **Current** | âŒ æ—  | L+1 ATTN (1å±‚) | **1 ç»„** | âš ï¸ **1 å±‚** |

### 2. IO å¸¦å®½åˆ©ç”¨ç‡ (ä¼°ç®—)

å‡è®¾:
- å•å±‚æƒé‡ä¼ è¾“æ—¶é—´: 100ms (SSDâ†’CPUâ†’GPU)
- å•å±‚ MHA è®¡ç®—æ—¶é—´: 80ms
- å•å±‚ FFN è®¡ç®—æ—¶é—´: 120ms

**History1:**
```
L0 MHA (80ms)  : åŒæ—¶ä¼ è¾“ L1/2/3/4 ATTN (400ms) â†’ åˆ©ç”¨ç‡ 80/400 = 20%
L0 FFN (120ms) : åŒæ—¶ä¼ è¾“ L1/2/3/4 FFN (400ms)  â†’ åˆ©ç”¨ç‡ 120/400 = 30%
å¹³å‡åˆ©ç”¨ç‡: 25% (æƒé‡é€šé“æœ‰ 75% ç©ºé—²ï¼Œä½†è¶³å¤Ÿè¦†ç›–åç»­å±‚)
```

**History/Current:**
```
L0 MHA (80ms)  : âŒ æ²¡æœ‰ä¼ è¾“ â†’ åˆ©ç”¨ç‡ 0%
L0 FFN (120ms) : ä¼ è¾“ L1 ATTN (100ms) â†’ åˆ©ç”¨ç‡ 100/120 = 83%
å¹³å‡åˆ©ç”¨ç‡: 42% (MHA æœŸé—´æµªè´¹äº† 50% å¸¦å®½)
```

### 3. å±‚é—´å»¶è¿Ÿ (å…³é”®æŒ‡æ ‡)

| ç‰ˆæœ¬ | L0â†’L1 å»¶è¿Ÿ | L0â†’L4 å»¶è¿Ÿ | è¯´æ˜ |
|------|-----------|-----------|------|
| **History1** | ~5ms | ~20ms | L0 å®Œæˆæ—¶ L1-4 å·²åœ¨ GPU |
| **History** | ~105ms | ~420ms | L0 å®Œæˆåæ‰å¼€å§‹ä¼ è¾“ L1 |
| **Current** | ~105ms | ~420ms | åŒ History |

**ç»“è®º: History1 çš„å±‚é—´å»¶è¿Ÿæ˜¯ History/Current çš„ 1/20!**

---

## ğŸš€ æœ€ä½³ Overlap æ”¹è¿›æ–¹æ¡ˆ

### ğŸ† æ¨è: åœ¨ History1 åŸºç¡€ä¸Šä¼˜åŒ–

**ä¸ºä»€ä¹ˆé€‰ History1?**
1. âœ… å·²æœ‰æœ€å¼ºçš„è·¨å±‚æµæ°´çº¿ (4 å±‚é¢„å–)
2. âœ… ç¨³å®šæ€§æœ€é«˜ (åŒé‡ä¿é™©)
3. âœ… ä»£ç æˆç†Ÿï¼Œç»è¿‡å……åˆ†æµ‹è¯•
4. âš ï¸ å”¯ä¸€ç¼ºç‚¹: é˜»å¡å¼åŒæ­¥æœ‰ 2-5ms å¼€é”€

### æ”¹è¿›è®¡åˆ’ (3 ä¸ªé˜¶æ®µ)

---

## é˜¶æ®µ 1: ä¿å®ˆæ”¹è¿› (ç«‹å³å¯è¡Œ)

### ç›®æ ‡: ä¿ç•™ History1 ç¨³å®šæ€§ï¼Œå¾®è°ƒé¢„å–ç­–ç•¥

```python
# history1/llama3/layers.py ä¿®æ”¹ç‚¹

# ========== MHA é˜¶æ®µ ==========
with torch.cuda.stream(streams.compute_mha):
    # â­ æ–°å¢: åœ¨ MHA æœŸé—´é¢„å–**æœ¬å±‚ FFN** (é«˜ä¼˜å…ˆçº§)
    if wm is not None and hasattr(wm, "prefetch_group_async"):
        wm.prefetch_group_async(self.layer_id, "ffn", pin=True, priority="high")

    # ä¿ç•™åŸæœ‰çš„æœªæ¥å±‚ ATTN é¢„å– (é™ä¸ºä¸­ä¼˜å…ˆçº§)
    for off in range(1, D+1):
        nxt = self.layer_id + off
        if nxt < self.n_layer:
            wm.prefetch_group_async(nxt, "attn", priority="medium")

    attn_out = self.attention(attn_in, start_pos, freqs_complex)

# ========== FFN é˜¶æ®µ ==========
with torch.cuda.stream(streams.compute_ffn):
    # ä¿ç•™åŸæœ‰çš„æœªæ¥å±‚ FFN é¢„å–
    for off in range(1, D+1):
        nxt = self.layer_id + off
        if nxt < self.n_layer:
            wm.prefetch_group_async(nxt, "ffn")

    ffn_out = self.feed_forward(ffn_in)
```

**é¢„æœŸæ•ˆæœ:**
- MHAâ†’FFN å»¶è¿Ÿä» 5ms é™åˆ° ~0ms (FFN æƒé‡å·²åœ¨ GPU)
- è·¨å±‚æµæ°´çº¿ç»´æŒ 4 å±‚æ·±åº¦
- é£é™©æä½ (åªæ˜¯è°ƒæ•´é¢„å–é¡ºåº)

---

## é˜¶æ®µ 2: æ¿€è¿›æ”¹è¿› (éœ€æµ‹è¯•)

### ç›®æ ‡: ç§»é™¤é˜»å¡å¼åŒæ­¥ï¼Œæ”¹ä¸ºçº¯äº‹ä»¶é©±åŠ¨

```python
# ========== MHA é˜¶æ®µ ==========
if wm is not None:
    # âŒ ç§»é™¤: wm.ensure_group_on_gpu(self.layer_id, "attn")
    # âœ… ä¿ç•™: äº‹ä»¶ç­‰å¾…
    wm.wait_group_ready(self.layer_id, "attn", compute_stream=streams.compute_mha)

# âš ï¸ å¢åŠ å…œåº•æ£€æŸ¥ (é¿å…çº¯äº‹ä»¶å¤±è´¥æ—¶å´©æºƒ)
if os.getenv("WSM_NO_FALLBACK", "0") != "1":
    # Fallback: å¦‚æœäº‹ä»¶ç­‰å¾…è¶…æ—¶ (>100ms)ï¼Œå¼ºåˆ¶åŒæ­¥ä¸€æ¬¡
    if wm is not None and hasattr(wm, "_check_group_ready"):
        if not wm._check_group_ready(self.layer_id, "attn", timeout_ms=100):
            logger.warning(f"L{self.layer_id} attn event timeout, fallback to sync")
            wm.ensure_group_on_gpu(self.layer_id, "attn")
```

**é¢„æœŸæ•ˆæœ:**
- CPU é˜»å¡ä» 2-5ms é™åˆ° ~0ms
- ä¿ç•™ fallback æœºåˆ¶ (æ¯” Current æ›´å®‰å…¨)
- éœ€è¦å……åˆ†æµ‹è¯•äº‹ä»¶ç³»ç»Ÿå¯é æ€§

---

## é˜¶æ®µ 3: ç»ˆæä¼˜åŒ– (é•¿æœŸ)

### ç›®æ ‡: å®ç°çœŸæ­£çš„è·¨å±‚æµæ°´çº¿

```python
# åœ¨ model.py ä¸­å®ç° pipelined forward

def _forward_pipelined(self, tokens, start_pos):
    h = self.embed_tokens(tokens)
    freqs = self.freqs_complex

    # â­ é¢„çƒ­: æå‰åŠ è½½å‰ warmup å±‚åˆ° GPU
    wm = getattr(self, "weight_streaming_manager", None)
    if wm and hasattr(wm, "warmup_layers"):
        for i in range(wm.warmup_layers):
            wm.ensure_group_on_gpu(i, "attn")
            wm.ensure_group_on_gpu(i, "ffn")

    # â­ æµæ°´çº¿æ‰§è¡Œ: è·¨å±‚äº‹ä»¶ä¸²æ¥
    prev_ffn_evt = None
    for idx, layer in enumerate(self.layers):
        # MHA ç­‰å¾…å‰ä¸€å±‚çš„ FFN å®Œæˆ
        if prev_ffn_evt is not None:
            layer.streams.compute_mha.wait_event(prev_ffn_evt)

        # æ‰§è¡Œå½“å‰å±‚ (MHA å’Œ FFN åœ¨å„è‡ªæµä¸Š)
        with torch.cuda.stream(layer.streams.compute_mha):
            # é¢„å–æœªæ¥å±‚
            for off in range(1, 5):
                nxt = idx + off
                if nxt < len(self.layers):
                    wm.prefetch_group_async(nxt, "attn")

            attn_out = layer.attention(layer.attention_norm(h), start_pos, freqs)

        mha_evt = record_event(layer.streams.compute_mha)
        h = h + attn_out

        layer.streams.compute_ffn.wait_event(mha_evt)
        with torch.cuda.stream(layer.streams.compute_ffn):
            for off in range(1, 5):
                nxt = idx + off
                if nxt < len(self.layers):
                    wm.prefetch_group_async(nxt, "ffn")

            ffn_out = layer.feed_forward(layer.ffn_norm(h))

        prev_ffn_evt = record_event(layer.streams.compute_ffn)
        h = h + ffn_out

    # æœ€ååŒæ­¥ä¸€æ¬¡
    torch.cuda.current_stream().wait_event(prev_ffn_evt)
    return self.norm(h)
```

**é¢„æœŸæ•ˆæœ:**
- L0 FFN ä¸ L1 MHA çœŸæ­£å¹¶è¡Œ
- ç†è®ºåŠ é€Ÿ 15-20% (ç›¸æ¯”é˜¶æ®µ 2)
- éœ€è¦å¤§å¹…é‡æ„ model.py

---

## ğŸ“Š ä¸‰é˜¶æ®µæ”¹è¿›æ•ˆæœå¯¹æ¯”

| æŒ‡æ ‡ | History1 åŸç‰ˆ | +é˜¶æ®µ1 | +é˜¶æ®µ2 | +é˜¶æ®µ3 | ç†è®ºæé™ |
|------|--------------|--------|--------|--------|----------|
| **MHAâ†’FFN å»¶è¿Ÿ** | 5ms | **0ms** âœ… | 0ms | 0ms | 0ms |
| **å±‚é—´å»¶è¿Ÿ (L0â†’L1)** | 5ms | 3ms | **~0ms** âœ… | **~0ms** âœ… | 0ms |
| **CPU é˜»å¡æ—¶é—´** | 2-5ms | 2-5ms | **~0ms** âœ… | ~0ms | 0ms |
| **è·¨å±‚å¹¶è¡Œåº¦** | 4 å±‚ | 4 å±‚ | 4 å±‚ | **âˆ å±‚** âœ… | âˆ å±‚ |
| **ç¨³å®šæ€§** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| **å®ç°éš¾åº¦** | - | ğŸŸ¢ ç®€å• | ğŸŸ¡ ä¸­ç­‰ | ğŸ”´ å›°éš¾ | - |
| **æµ‹è¯•å·¥ä½œé‡** | - | 1å¤© | 1å‘¨ | 1æœˆ | - |

---

## ğŸ¯ ç«‹å³è¡ŒåŠ¨è®¡åˆ’

### ç¬¬ 1 å¤©: é˜¶æ®µ 1 (MHA æœŸé—´é¢„å–æœ¬å±‚ FFN)

```bash
# 1. åŸºäº History1 åˆ›å»ºä¼˜åŒ–åˆ†æ”¯
cd /home/roger/llama3-inference
git checkout -b optimize-overlap-stage1

# 2. ä¿®æ”¹ history1/llama3/layers.py
# åœ¨ line 1441 çš„ MHA è®¡ç®—æµä¸­æ·»åŠ :
#   wm.prefetch_group_async(self.layer_id, "ffn", pin=True, priority="high")

# 3. æµ‹è¯•
python inferencellama3-1-70B.py --batch-size 1 --max-gen-len 32
python inferencellama3-1-70B.py --batch-size 2 --max-gen-len 32  # ç¡®ä¿ä¸ OOM

# 4. å¯¹æ¯”æ€§èƒ½
# é¢„æœŸ: MHAâ†’FFN æ— ç¼è¡”æ¥ï¼Œæ€»å»¶è¿Ÿé™ä½ 2-3%
```

### ç¬¬ 2-7 å¤©: é˜¶æ®µ 2 (ç§»é™¤é˜»å¡å¼åŒæ­¥)

```bash
git checkout -b optimize-overlap-stage2

# 1. ç§»é™¤ ensure_group_on_gpu() è°ƒç”¨
# 2. æ·»åŠ  fallback è¶…æ—¶æ£€æŸ¥
# 3. å……åˆ†æµ‹è¯•å„ç§ batch size / sequence length
# 4. ç›‘æ§ WSM äº‹ä»¶ç³»ç»Ÿçš„å¯é æ€§
```

### ç¬¬ 8-30 å¤©: é˜¶æ®µ 3 (è·¨å±‚æµæ°´çº¿)

```bash
git checkout -b optimize-overlap-stage3

# 1. é‡æ„ model.py çš„ forward()
# 2. å®ç°è·¨å±‚äº‹ä»¶ä¸²æ¥
# 3. æ·»åŠ å®Œå–„çš„ç›‘æ§å’Œå›é€€æœºåˆ¶
# 4. ä¸é˜¶æ®µ 2 å¯¹æ¯”æ€§èƒ½
```

---

## âš ï¸  é£é™©è¯„ä¼°ä¸ç¼“è§£ç­–ç•¥

| é£é™© | æ¦‚ç‡ | å½±å“ | ç¼“è§£ç­–ç•¥ |
|------|------|------|----------|
| **é˜¶æ®µ1: è¿‡åº¦é¢„å–å¯¼è‡´ OOM** | ä½ (10%) | ä¸­ | æ·»åŠ  GPU å†…å­˜ç›‘æ§ï¼ŒåŠ¨æ€è°ƒæ•´é¢„å–æ·±åº¦ |
| **é˜¶æ®µ2: äº‹ä»¶ç³»ç»Ÿå¤±è´¥** | ä¸­ (30%) | é«˜ | ä¿ç•™ fallback æœºåˆ¶ï¼Œè¶…æ—¶åé™çº§åˆ°é˜»å¡å¼ |
| **é˜¶æ®µ3: è·¨å±‚ä¾èµ–é”™è¯¯** | é«˜ (50%) | é«˜ | åˆ†é˜¶æ®µæµ‹è¯•ï¼Œå…ˆæµ‹å•å±‚å†æµ‹å¤šå±‚ |
| **æ‰€æœ‰é˜¶æ®µ: å¼•å…¥æ–° bug** | ä¸­ (40%) | ä¸­ | å……åˆ†çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯• |

---

## ğŸ“ æ€»ç»“ä¸å»ºè®®

### ğŸ† æœ€ä½³é€‰æ‹©: History1 + é˜¶æ®µ 1 æ”¹è¿›

**ç†ç”±:**
1. âœ… History1 å·²æ˜¯ä¸‰ç‰ˆæœ¬ä¸­ Overlap èƒ½åŠ›æœ€å¼ºçš„
2. âœ… é˜¶æ®µ 1 æ”¹è¿›ç®€å• (< 10 è¡Œä»£ç )
3. âœ… é£é™©æä½ (åªè°ƒæ•´é¢„å–é¡ºåº)
4. âœ… é¢„æœŸæ”¶ç›Š 2-3% (MHAâ†’FFN æ— ç¼è¡”æ¥)
5. âœ… 1 å¤©å†…å¯å®Œæˆæµ‹è¯•

**ä¸æ¨è History/Current ä½œä¸ºåŸºç¡€:**
- âŒ History/Current åªé¢„å– 1 å±‚ï¼Œæµæ°´çº¿æ·±åº¦ä¸è¶³
- âŒ éœ€è¦å¤§å¹…æ”¹åŠ¨æ‰èƒ½è¾¾åˆ° History1 çš„æ°´å¹³
- âŒ Current è¿˜æœ‰ SPDA å…¼å®¹æ€§é—®é¢˜

### æ¸è¿›å¼è·¯çº¿å›¾

```
Week 1:  History1 + é˜¶æ®µ 1  â†’ éªŒè¯ MHAâ†’FFN æ— ç¼è¡”æ¥
Week 2:  æ€§èƒ½æµ‹è¯•           â†’ ç¡®è®¤ 2-3% æå‡
Week 3:  é˜¶æ®µ 2 è®¾è®¡        â†’ è¯„ä¼°ç§»é™¤é˜»å¡å¼åŒæ­¥çš„å¯è¡Œæ€§
Week 4+: é˜¶æ®µ 2/3 å®æ–½      â†’ æ ¹æ®é˜¶æ®µ 1 æ•ˆæœå†³å®šæ˜¯å¦æ¨è¿›
```

---

ç”Ÿæˆæ—¶é—´: 2025-11-11
åˆ†æç‰ˆæœ¬: history (Nov 5), history1 (Nov 4), current (Nov 11)
