# Llama3 70B å¤šæµå¹¶è¡Œé‡å å®Œæ•´å®¡è®¡ä¸è§£å†³æ–¹æ¡ˆ

## æ‰§è¡Œæ¦‚è¦ (Executive Summary)

**ç¡¬ä»¶ç¯å¢ƒ**: 16GB GPU HBM + 128GB DRAM
**æ¨¡å‹**: Llama3-1-70B (80å±‚, ~140GBå‚æ•°)
**å½“å‰é—®é¢˜**: å°½ç®¡ç³»ç»Ÿå·²å®ç°6æ¡CUDAæµçš„å¤æ‚æ¶æ„,ä½†ä»æœªè¾¾åˆ°è®¡ç®—ä¸IOçš„å®Œç¾é‡å ,GPUç»å¸¸ç­‰å¾…æ•°æ®åŠ è½½

**æ ¹æœ¬åŸå› **: 7ä¸ªå…³é”®åŒæ­¥ç‚¹å¯¼è‡´å¹¶è¡Œæµæ°´çº¿è¢«é˜»å¡,åŒ…æ‹¬:
1. âŒ æ˜¾å¼çš„ `wait_stream()` è°ƒç”¨é˜»å¡ä¸»æµ
2. âŒ åŒæ­¥å¼çš„æƒé‡ç»„åŠ è½½ (`ensure_group_on_gpu`)
3. âŒ KV Cache H2Dåœ¨computeæµä¸Šçš„å¼ºåˆ¶ç­‰å¾…
4. âŒ å±‚é—´ä¸²è¡Œæ‰§è¡Œæ¨¡å¼ (æ¯å±‚MHAâ†’FFNâ†’ä¸‹ä¸€å±‚)
5. âŒ ç¼ºå°‘çœŸæ­£çš„åŒç¼“å†²/é¢„å–æµæ°´
6. âŒ äº‹ä»¶åŒæ­¥ç²’åº¦è¿‡ç²—
7. âŒ å…¨å±€åŒæ­¥ç‚¹ (`torch.cuda.synchronize()`)

---

## 1. å½“å‰æ¶æ„åˆ†æ

### 1.1 å¤šæµæ¶æ„ (6 Streams)

æ‚¨çš„ç³»ç»Ÿå·²å®ç°6æ¡CUDAæµ ([stream_mnt.py:107-220](stream_mnt.py#L107-L220)):

```python
class Streams:
    compute_mha: Optional[torch.cuda.Stream]       # ä¼˜å…ˆçº§=-1 (é«˜)
    compute_ffn: Optional[torch.cuda.Stream]       # ä¼˜å…ˆçº§=0 (æ™®é€š)
    weight_h2d_mha: Optional[torch.cuda.Stream]    # ä¼˜å…ˆçº§=-1 (é«˜)
    weight_h2d_ffn: Optional[torch.cuda.Stream]    # ä¼˜å…ˆçº§=0 (æ™®é€š)
    kv_h2d: Optional[torch.cuda.Stream]            # ä¼˜å…ˆçº§=-1 (é«˜)
    kv_d2h: Optional[torch.cuda.Stream]            # ä¼˜å…ˆçº§=0 (æ™®é€š)
```

**ä¼˜å…ˆçº§åˆ†é…åˆç†æ€§**: âœ… MHAè®¡ç®—å’Œæƒé‡/KVåŠ è½½ä½¿ç”¨é«˜ä¼˜å…ˆçº§,FFNä½¿ç”¨æ™®é€šä¼˜å…ˆçº§

### 1.2 æƒé‡æµå¼ç®¡ç† (WSM - Weight Streaming Manager)

**å…³é”®å‘ç°** ([weight_streaming_manager.py:92-500](weight_streaming_manager.py#L92-L500)):

1. **ç»„çº§è°ƒåº¦å™¨** (Group-level Scheduler):
   - å°†æ¯å±‚åˆ†è§£ä¸º `attn` å’Œ `ffn` ä¸¤ä¸ªç»„
   - ä½¿ç”¨ `gpu_max_groups=8` æ§åˆ¶GPUé©»ç•™ä¸Šé™
   - **é—®é¢˜**: å¹³è¡¡è°ƒåº¦å™¨ `rebalance_and_topoff()` æ˜¯åŒæ­¥çš„

2. **é¢„å–æœºåˆ¶** ([weight_streaming_manager.py:2551-2650](weight_streaming_manager.py#L2551-L2650)):
   ```python
   def prefetch_group_async(self, layer_idx, kind, pin=False):
       # âœ… å¼‚æ­¥é¢„å–åˆ°GPU
       # âŒ ä½†ç¼ºå°‘ä¸ä¸‹æ¸¸è®¡ç®—çš„çœŸæ­£æµæ°´
   ```

3. **ç­‰å¾…å±éšœ** ([weight_streaming_manager.py:2277-2390](weight_streaming_manager.py#L2277-L2390)):
   ```python
   def wait_group_ready(self, layer_idx, group, compute_stream):
       # âŒ å…³é”®é—®é¢˜: è¿™ä¼šè®©computeæµç­‰å¾…weight_h2dæµ!
       if evt is not None:
           s.wait_event(cuda_evt)  # é˜»å¡ç‚¹!
   ```

### 1.3 KV Cacheç®¡ç† (KVOffloader)

**å‘ç°** ([kv_offload.py:776-832](kv_offload.py#L776-L832)):

```python
def fetch(self, layer, blocks, ...):
    # åœ¨ kv_h2d æµä¸Šæ‰§è¡ŒDRAMâ†’GPU
    with torch.cuda.stream(stream):
        for b in uniq:
            k_parts.append(kc.to(self.device, non_blocking=True))
        k_full = torch.cat(k_parts, dim=2)  # âŒ åœ¨H2Dæµå†…æ‹¼æ¥!
    # âŒ æ²¡æœ‰è¿”å›å°±ç»ªäº‹ä»¶!
```

**é—®é¢˜**:
- KV fetchåæ²¡æœ‰è¿”å›äº‹ä»¶ç»™è°ƒç”¨æ–¹
- Attentionå±‚å¿…é¡»åœ¨computeæµä¸Šæ˜¾å¼ç­‰å¾… ([layers.py:767-771](layers.py#L767-L771)):
  ```python
  self.compute_stream.wait_stream(kv_h2d_stream)  # âŒ å¼ºåˆ¶åŒæ­¥!
  ```

### 1.4 å±‚çº§æ‰§è¡Œæµç¨‹ (Layer Execution Flow)

**SelfAttention.forward()** ([layers.py:497-1067](layers.py#L497-L1067)):

```mermaid
graph TD
    A[MHA forwardå…¥å£] --> B[wait_group_ready attnç»„]
    B --> C[âŒ é˜»å¡: ç­‰å¾…weight_h2d_mhaå®Œæˆ]
    C --> D[QKVæŠ•å½±]
    D --> E[KV fetch]
    E --> F[âŒ é˜»å¡: wait_stream kv_h2d]
    F --> G[Attentionè®¡ç®—]
    G --> H[è¾“å‡ºæŠ•å½±]
    H --> I[prefetch_group_async ffn+next_attn]
    I --> J[âŒ é—®é¢˜: é¢„å–å¤ªæ™š,MHAå·²å®Œæˆ]
```

**FeedForward.forward()** ([layers.py:1262-1450](layers.py#L1262-L1450)):

```mermaid
graph TD
    A[FFN forwardå…¥å£] --> B[wait_group_ready ffnç»„]
    B --> C[âŒ é˜»å¡: ç­‰å¾…weight_h2d_ffnå®Œæˆ]
    C --> D[SwiGLUè®¡ç®—]
    D --> E[å®Œæˆ]
```

**å…³é”®é—®é¢˜**:
1. æ¯ä¸ªç»„åœ¨ä½¿ç”¨å‰éƒ½å¿…é¡»ç­‰å¾…å…¶H2Då®Œæˆ â†’ é¢„å–æ— æ•ˆåŒ–
2. é¢„å–å‘ç”Ÿåœ¨**è®¡ç®—å¼€å§‹å**,è€Œé**å‰ä¸€å±‚è®¡ç®—æœŸé—´**
3. MHAå’ŒFFNä¹‹é—´æ²¡æœ‰é‡å  (ä¸²è¡Œæ‰§è¡Œ)

---

## 2. ç“¶é¢ˆæ ¹å› åˆ†æ (Root Cause Analysis)

### 2.1 âŒ åŒæ­¥ç‚¹#1: `wait_group_ready` é˜»å¡ä¸»æµ

**ä½ç½®**: [weight_streaming_manager.py:2277-2390](weight_streaming_manager.py#L2277-L2390)

```python
def wait_group_ready(self, layer_idx, group, compute_stream):
    evt = self._group_ready_events.get(k)
    if evt is not None:
        s = compute_stream or torch.cuda.current_stream()
        s.wait_event(evt)  # âŒ è®©è®¡ç®—æµç­‰å¾…H2Däº‹ä»¶!
```

**å½±å“**: å³ä½¿é¢„å–å·²å¯åŠ¨,è®¡ç®—æµä¹Ÿå¿…é¡»ç­‰å¾…æƒé‡H2Då®Œæˆæ‰èƒ½å¼€å§‹QKVæŠ•å½±

**è§£å†³æ–¹å‘**: éœ€è¦åœ¨**å‰ä¸€å±‚è®¡ç®—æ—¶**å°±å®ŒæˆH2D,è€Œéå½“å‰å±‚å…¥å£æ‰ç­‰å¾…

---

### 2.2 âŒ åŒæ­¥ç‚¹#2: KV Cacheå¼ºåˆ¶åŒæ­¥

**ä½ç½®**: [layers.py:767-771](layers.py#L767-L771)

```python
# ğŸ”‘ å…³é”®åŒæ­¥ç‚¹: è®©å½“å‰è®¡ç®—æµç­‰å¾… KV H2D æµ
if kv_h2d_stream is not None and self.compute_stream is not None:
    self.compute_stream.wait_stream(kv_h2d_stream)  # âŒ é˜»å¡!
```

**é—®é¢˜**:
- KV fetchåœ¨`kv_h2d`æµä¸Šæ‰§è¡Œ,ä½†æ²¡æœ‰è¿”å›å°±ç»ªäº‹ä»¶
- å¿…é¡»ç”¨`wait_stream`å¼ºåˆ¶åŒæ­¥æ•´ä¸ªæµ (åŒ…æ‹¬å…¶å®ƒå±‚çš„KVä¼ è¾“!)

**è§£å†³æ–¹å‘**: KV fetchåº”è¿”å›per-layeräº‹ä»¶,åªç­‰å¾…å½“å‰å±‚éœ€è¦çš„blocks

---

### 2.3 âŒ åŒæ­¥ç‚¹#3: é¢„å–æ—¶æœºé”™è¯¯

**å½“å‰**: é¢„å–å‘ç”Ÿåœ¨**å½“å‰å±‚MHA forwardå†…éƒ¨** ([layers.py:610-638](layers.py#L610-L638))

```python
def forward(self, x, start_pos, freqs):
    # ...MHAè®¡ç®—å¼€å§‹å...
    if wm and hasattr(wm, "prefetch_group_async"):
        wm.prefetch_group_async(self.layer_id, "ffn", pin=True)  # âŒ å¤ªæ™š!
        wm.rebalance_and_topoff(self.layer_id)
```

**é—®é¢˜**:
1. æ­¤æ—¶MHAå·²ç»åœ¨è®¡ç®—,æ— æ³•ä¸æƒé‡H2Dé‡å 
2. `rebalance_and_topoff()` æ˜¯åŒæ­¥çš„,ä¼šé˜»å¡å½“å‰forward
3. é¢„å–çš„FFNç»„éœ€è¦ç­‰åˆ°MHAå®Œæˆåæ‰ä½¿ç”¨ â†’ ç™½ç™½æµªè´¹äº†MHAè®¡ç®—æ—¶é—´

**ç†æƒ³æ—¶æœº**: åº”åœ¨**å‰ä¸€å±‚FFNè®¡ç®—æœŸé—´**é¢„å–å½“å‰å±‚MHA

---

### 2.4 âŒ åŒæ­¥ç‚¹#4: MHAä¸FFNä¸²è¡Œæ‰§è¡Œ

**å½“å‰æ¶æ„**: ([layers.py:1577-1660](layers.py#L1577-L1660))

```python
class EncoderBlock:
    def forward(self, x, start_pos, freqs):
        h = x + self.attention(self.attn_norm(x), start_pos, freqs)  # MHA
        out = h + self.ffn(self.ffn_norm(h))                        # FFN
        return out  # âŒ å®Œå…¨ä¸²è¡Œ!
```

**é—®é¢˜**:
- MHAå¿…é¡»å®Œæˆåæ‰èƒ½å¯åŠ¨FFN
- æ— æ³•å®ç°MHA(L)ä¸FFN(L-1)çš„å¹¶è¡Œé‡å 
- GPUåœ¨MHAæœŸé—´ç©ºé—²ç­‰å¾…æƒé‡,åœ¨FFNæœŸé—´ç©ºé—²ç­‰å¾…MHA

---

### 2.5 âŒ åŒæ­¥ç‚¹#5: ç¼ºå°‘çœŸæ­£çš„åŒç¼“å†²æµæ°´

**å½“å‰**: åªæœ‰é¢„å–,æ²¡æœ‰æµæ°´çº¿æ‰§è¡Œ

**ç¼ºå¤±çš„æµæ°´**:
```
ç†æƒ³:  L0_MHA | L0_FFN | L1_MHA | L1_FFN | ...
       -------   -------   -------   -------
ç°å®:  [L0_MHA_wait] â†’ L0_MHA â†’ [L0_FFN_wait] â†’ L0_FFN â†’ ...
```

**æ ¹å› **:
1. æ²¡æœ‰layer-levelå¹¶è¡Œ (åªèƒ½ä¸€å±‚ä¸€å±‚æ‰§è¡Œ)
2. æ²¡æœ‰operator-levelå¹¶è¡Œ (MHAå’ŒFFNä¸èƒ½åŒæ—¶æ‰§è¡Œä¸åŒå±‚)

---

## 3. å®Œæ•´è§£å†³æ–¹æ¡ˆ (Complete Solution)

### 3.1 æ¶æ„é‡æ„: å¼•å…¥æµæ°´çº¿è°ƒåº¦å™¨ (Pipeline Scheduler)

**æ ¸å¿ƒæ€æƒ³**: å°†å±‚çº§æ‰§è¡Œæ”¹ä¸º**ç®—å­çº§æµæ°´çº¿**

```python
class PipelineScheduler:
    """
    ç®¡ç†å¤šå±‚å¤šç®—å­çš„æµæ°´çº¿æ‰§è¡Œ:
    - MHA(L) å¯ä»¥ä¸ FFN(L-1) å¹¶è¡Œ
    - æƒé‡é¢„å–ä¸è®¡ç®—å®Œå…¨é‡å 
    - KV Cacheä¼ è¾“ä¸è®¡ç®—é‡å 
    """
    def __init__(self, model, streams, wsm, kv_offloader):
        self.layers = model.layers
        self.streams = streams
        self.wsm = wsm
        self.kv_off = kv_offloader

        # æµæ°´çº¿çŠ¶æ€
        self.pipeline_depth = 2  # æœ€å¤šåŒæ—¶æ‰§è¡Œ2ä¸ªstage
        self.stage_events = {}   # {stage_id: Event}

    def execute_token(self, x, start_pos, freqs):
        """
        æµæ°´çº¿æ‰§è¡Œå•ä¸ªtokençš„æ‰€æœ‰å±‚:

        æ—¶é—´è½´:
        t0: L0_MHA_weight_load | -
        t1: L0_MHA_compute      | L0_FFN_weight_load
        t2: L0_MHA_finish       | L0_FFN_compute | L1_MHA_weight_load
        t3: -                   | L0_FFN_finish  | L1_MHA_compute | L1_FFN_weight_load
        ...
        """
        h = x
        for L in range(len(self.layers)):
            # Stage 1: MHA
            h = self._execute_mha_stage(L, h, start_pos, freqs)

            # Stage 2: FFN (ä¸ä¸‹ä¸€å±‚MHAé¢„å–é‡å )
            h = self._execute_ffn_stage(L, h)

        return h
```

---

### 3.2 ä¿®æ”¹#1: å¼‚æ­¥éé˜»å¡çš„æƒé‡åŠ è½½

**é—®é¢˜æ–‡ä»¶**: [weight_streaming_manager.py:2277-2390](weight_streaming_manager.py#L2277-L2390)

**ä¿®æ”¹å‰**:
```python
def wait_group_ready(self, layer_idx, group, compute_stream):
    evt = self._group_ready_events.get(k)
    if evt is not None:
        s.wait_event(evt)  # âŒ é˜»å¡computeæµ!
```

**ä¿®æ”¹å**:
```python
def get_group_ready_event(self, layer_idx, group):
    """
    è¿”å›ç»„å°±ç»ªäº‹ä»¶,ä½†ä¸é˜»å¡ä»»ä½•æµã€‚
    ç”±è°ƒç”¨æ–¹å†³å®šä½•æ—¶/å¦‚ä½•ç­‰å¾…ã€‚
    """
    k = self._key(layer_idx, group)

    # å¦‚æœç»„å·²é©»ç•™,è¿”å›ä¸€ä¸ªå·²å®Œæˆçš„è™šæ‹Ÿäº‹ä»¶
    if self._group_is_resident(layer_idx, group):
        dummy_evt = torch.cuda.Event()
        dummy_evt.record()  # ç«‹å³å®Œæˆ
        return dummy_evt

    # è¿”å›H2Däº‹ä»¶(å¯èƒ½æœªå®Œæˆ)
    return self._group_ready_events.get(k)

def try_prefetch_if_not_ready(self, layer_idx, group):
    """
    éé˜»å¡åœ°æ£€æŸ¥ç»„æ˜¯å¦å°±ç»ª,å¦‚æœä¸å°±ç»ªåˆ™å¯åŠ¨é¢„å–ã€‚
    è¿”å›å°±ç»ªäº‹ä»¶ã€‚
    """
    evt = self.get_group_ready_event(layer_idx, group)
    if evt is None or not evt.query():
        # ä¸å°±ç»ª,å¯åŠ¨é¢„å–
        self.prefetch_group_async(layer_idx, group, pin=True)
        evt = self.get_group_ready_event(layer_idx, group)
    return evt
```

**ä½¿ç”¨æ–¹å¼** (åœ¨ SelfAttention.forward ä¸­):
```python
def forward(self, x, start_pos, freqs):
    wm = self.weight_manager

    # âœ… ä¸é˜»å¡åœ°è·å–attnç»„äº‹ä»¶
    attn_evt = wm.get_group_ready_event(self.layer_id, "attn")

    # âœ… é¢„å–ä¸‹ä¸€å±‚(ä¸å½“å‰è®¡ç®—é‡å )
    if self.layer_id + 1 < wm.n_layers:
        wm.try_prefetch_if_not_ready(self.layer_id + 1, "attn")

    # âœ… åªåœ¨çœŸæ­£ä½¿ç”¨æƒé‡å‰æ‰ç­‰å¾…
    # ä¸”åªç­‰å¾…attnç»„,ä¸ç­‰å¾…å…¶å®ƒç»„!
    with torch.cuda.stream(self.compute_stream):
        if attn_evt is not None:
            self.compute_stream.wait_event(attn_evt)

        # ç°åœ¨æƒé‡å·²å°±ç»ª,å¼€å§‹è®¡ç®—
        q = self.wq(x).view(...)
        ...
```

---

### 3.3 ä¿®æ”¹#2: KV Cacheè¿”å›per-layeräº‹ä»¶

**é—®é¢˜æ–‡ä»¶**: [kv_offload.py:776-832](kv_offload.py#L776-L832)

**ä¿®æ”¹å‰**:
```python
def fetch(self, layer, blocks, ...):
    with torch.cuda.stream(stream):
        for b in uniq:
            k_parts.append(kc.to(self.device, non_blocking=True))
        k_full = torch.cat(k_parts, dim=2)

    # âŒ æ²¡æœ‰è¿”å›äº‹ä»¶!
    return k_full, v_full
```

**ä¿®æ”¹å**:
```python
def fetch(self, layer, blocks, ..., return_event=False):
    """
    å¼‚æ­¥fetch KV blocksåˆ°GPUã€‚

    Args:
        return_event: å¦‚æœTrue,è¿”å› (k_full, v_full, event)
                     å…¶ä¸­eventåœ¨kv_h2dæµä¸Šè®°å½•

    Returns:
        å¦‚æœreturn_event=True: (k_full, v_full, cuda_event)
        å¦åˆ™: (k_full, v_full)
    """
    stream = self.h2d_stream or torch.cuda.current_stream()

    with torch.cuda.stream(stream):
        # ... åŸæœ‰çš„H2Dé€»è¾‘ ...
        k_full = torch.cat(k_parts, dim=2)
        v_full = torch.cat(v_parts, dim=2)

        if return_event:
            # âœ… åœ¨kv_h2dæµä¸Šè®°å½•å®Œæˆäº‹ä»¶
            evt = torch.cuda.Event()
            evt.record(stream)
            return k_full, v_full, evt

    return k_full, v_full
```

**ä½¿ç”¨æ–¹å¼** (åœ¨ SelfAttention.forward ä¸­):
```python
def forward(self, x, start_pos, freqs):
    # ... QKVæŠ•å½± ...

    # âœ… å¼‚æ­¥fetch KV,è·å–å°±ç»ªäº‹ä»¶
    k_full, v_full, kv_evt = self.offloader.fetch(
        self.layer_id, needed,
        return_event=True
    )

    # âœ… åªåœ¨çœŸæ­£ä½¿ç”¨KVå‰æ‰ç­‰å¾…
    with torch.cuda.stream(self.compute_stream):
        if kv_evt is not None:
            self.compute_stream.wait_event(kv_evt)

        # KVå·²å°±ç»ª,æ‰§è¡ŒAttention
        scores = torch.matmul(q, k_full.transpose(2, 3))
        ...
```

---

### 3.4 ä¿®æ”¹#3: æå‰é¢„å– (Early Prefetching)

**æ ¸å¿ƒç­–ç•¥**: åœ¨**å‰ä¸€å±‚FFNè®¡ç®—æœŸé—´**é¢„å–å½“å‰å±‚MHAæƒé‡

**æ–°å¢**: EncoderBlockçº§åˆ«çš„é¢„å–åè°ƒ

```python
class EncoderBlock(nn.Module):
    def forward(self, x, start_pos, freqs):
        # âœ… åœ¨MHAå¼€å§‹å‰,é¢„å–FFNç»„(ä¸MHAè®¡ç®—é‡å )
        if hasattr(self, 'weight_manager'):
            wm = self.weight_manager
            # é¢„å–æœ¬å±‚FFN + ä¸‹ä¸€å±‚MHA
            wm.try_prefetch_if_not_ready(self.layer_id, "ffn")
            if self.layer_id + 1 < wm.n_layers:
                wm.try_prefetch_if_not_ready(self.layer_id + 1, "attn")

        # MHAè®¡ç®— (ä¸ä¸Šè¿°é¢„å–é‡å )
        h = x + self.attention(
            self.attn_norm(x),
            start_pos,
            freqs
        )

        # âœ… åœ¨FFNå¼€å§‹å‰,é¢„å–ä¸‹ä¸€å±‚çš„attnç»„
        if hasattr(self, 'weight_manager'):
            wm = self.weight_manager
            if self.layer_id + 1 < wm.n_layers:
                wm.try_prefetch_if_not_ready(self.layer_id + 1, "attn")

        # FFNè®¡ç®— (ä¸é¢„å–L+1çš„attné‡å )
        out = h + self.ffn(
            self.ffn_norm(h)
        )

        return out
```

**æ—¶é—´çº¿æ”¹è¿›**:
```
ä¿®æ”¹å‰:
  L0_MHA (ç­‰æƒé‡) â†’ L0_MHA_è®¡ç®— â†’ L0_FFN (ç­‰æƒé‡) â†’ L0_FFN_è®¡ç®— â†’ L1...

ä¿®æ”¹å:
  [é¢„å–L0_attn] â†’ L0_MHA_è®¡ç®— (åŒæ—¶é¢„å–L0_ffn+L1_attn) â†’ L0_FFN_è®¡ç®— (L1_attnå·²å°±ç»ª!) â†’ L1_MHA_è®¡ç®— (æ— ç­‰å¾…) â†’ ...
```

---

### 3.5 ä¿®æ”¹#4: MHAä¸FFNæµçº§å¹¶è¡Œ (å¯é€‰é«˜çº§ä¼˜åŒ–)

**è­¦å‘Š**: æ­¤ä¿®æ”¹å¤æ‚åº¦é«˜,å»ºè®®å…ˆå®Œæˆä¿®æ”¹#1-#3

**æ ¸å¿ƒæ€æƒ³**: ä¸åŒå±‚çš„MHAå’ŒFFNå¯ä»¥å¹¶è¡Œæ‰§è¡Œ

```python
class PipelinedEncoderBlock(nn.Module):
    def __init__(self, args, layer_id):
        super().__init__()
        self.layer_id = layer_id
        self.attention = SelfAttention(args)
        self.ffn = FeedForward(args)
        self.attn_norm = RMSNorm(args.dim)
        self.ffn_norm = RMSNorm(args.dim)

        # âœ… ä¸ºæ¯ä¸ªstageåˆ†é…ä¸“ç”¨äº‹ä»¶
        self.mha_done_evt = None
        self.ffn_done_evt = None

    def forward_mha(self, x, start_pos, freqs):
        """
        åªæ‰§è¡ŒMHA stage,ä¸ç­‰å¾…ä¸Šä¸€å±‚FFNã€‚
        è¿”å› (output, event)
        """
        h_in = self.attn_norm(x)

        with torch.cuda.stream(self.streams.compute_mha):
            h_out = self.attention(h_in, start_pos, freqs)

            # âœ… è®°å½•MHAå®Œæˆäº‹ä»¶
            self.mha_done_evt = torch.cuda.Event()
            self.mha_done_evt.record(self.streams.compute_mha)

        return h_out, self.mha_done_evt

    def forward_ffn(self, x_plus_attn_out, prev_mha_evt):
        """
        æ‰§è¡ŒFFN stage,ç­‰å¾…MHAå®Œæˆã€‚

        Args:
            x_plus_attn_out: x + attn_output (residualå·²åŠ )
            prev_mha_evt: æœ¬å±‚MHAå®Œæˆäº‹ä»¶
        """
        h_in = self.ffn_norm(x_plus_attn_out)

        with torch.cuda.stream(self.streams.compute_ffn):
            # âœ… ç­‰å¾…æœ¬å±‚MHAå®Œæˆ
            if prev_mha_evt is not None:
                self.streams.compute_ffn.wait_event(prev_mha_evt)

            h_out = self.ffn(h_in)

            # âœ… è®°å½•FFNå®Œæˆäº‹ä»¶
            self.ffn_done_evt = torch.cuda.Event()
            self.ffn_done_evt.record(self.streams.compute_ffn)

        return h_out, self.ffn_done_evt
```

**é¡¶å±‚Transformerçš„æµæ°´çº¿å¾ªç¯**:
```python
class Transformer(nn.Module):
    def forward(self, tokens, start_pos):
        h = self.embed_tokens(tokens)

        # æµæ°´çº¿çŠ¶æ€
        pending_mha = []  # [(layer_idx, x, event), ...]

        for L in range(len(self.layers)):
            layer = self.layers[L]

            # Stage 1: å¯åŠ¨MHA (å¼‚æ­¥)
            attn_out, mha_evt = layer.forward_mha(h, start_pos, freqs)
            h_after_attn = h + attn_out  # residual

            # Stage 2: å¯åŠ¨FFN (å¯èƒ½ä¸ä¸‹ä¸€å±‚MHAé‡å )
            ffn_out, ffn_evt = layer.forward_ffn(h_after_attn, mha_evt)
            h = h_after_attn + ffn_out  # residual

            # âœ… å…³é”®: FFNåœ¨compute_ffnæµä¸Šæ‰§è¡Œ
            # è€Œä¸‹ä¸€å±‚MHAå°†åœ¨compute_mhaæµä¸Šæ‰§è¡Œ
            # ä¸¤è€…å¯ä»¥å¹¶è¡Œ!

        return h
```

**æ—¶é—´çº¿æ”¹è¿›** (æœ€ç»ˆå½¢æ€):
```
æµæ°´çº¿æ·±åº¦=2çš„ç†æƒ³æƒ…å†µ:

æ—¶åˆ»    compute_mhaæµ          compute_ffnæµ          weight_h2d_mhaæµ    weight_h2d_ffnæµ
-----   ------------------    ------------------    ---------------    ---------------
t0      [é¢„åŠ è½½L0_attn]       -                     L0_attn â†’ GPU      -
t1      L0_MHA                -                     L0_ffn â†’ GPU       -
t2      L1_MHA                L0_FFN                L1_attn â†’ GPU      L0_ffnä½¿ç”¨ä¸­
t3      L2_MHA                L1_FFN                L2_attn â†’ GPU      L1_ffn â†’ GPU
t4      L3_MHA                L2_FFN                L3_attn â†’ GPU      L2_ffn â†’ GPU
...
```

**è¾¾æˆæ•ˆæœ**:
- âœ… GPUå§‹ç»ˆåœ¨è®¡ç®— (MHAæˆ–FFN)
- âœ… æƒé‡åŠ è½½ä¸è®¡ç®—å®Œå…¨é‡å 
- âœ… KV Cacheä¼ è¾“ä¸è®¡ç®—é‡å 
- âœ… æ¶ˆé™¤æ‰€æœ‰ç­‰å¾…

---

## 4. å®æ–½è·¯çº¿å›¾ (Implementation Roadmap)

### Phase 1: ä½é£é™©ä¿®æ”¹ (1-2å¤©)

**ä¼˜å…ˆçº§æœ€é«˜,ç«‹å³å®æ–½**:

1. âœ… **ä¿®æ”¹#2**: KV Cacheè¿”å›äº‹ä»¶
   - æ–‡ä»¶: [kv_offload.py:776-832](kv_offload.py#L776-L832)
   - ä¿®æ”¹: `fetch()` è¿”å› `(k_full, v_full, event)`
   - éš¾åº¦: â­ (ç®€å•)
   - æ”¶ç›Š: â­â­â­ (æ¶ˆé™¤KVåŒæ­¥ç“¶é¢ˆ)

2. âœ… **ä¿®æ”¹#1**: å¼‚æ­¥æƒé‡åŠ è½½
   - æ–‡ä»¶: [weight_streaming_manager.py:2277-2390](weight_streaming_manager.py#L2277-L2390)
   - æ–°å¢: `get_group_ready_event()`, `try_prefetch_if_not_ready()`
   - ä¿®æ”¹: [layers.py:532-549](layers.py#L532-L549) ä½¿ç”¨æ–°API
   - éš¾åº¦: â­â­ (ä¸­ç­‰)
   - æ”¶ç›Š: â­â­â­â­ (æ¶ˆé™¤æƒé‡ç­‰å¾…)

3. âœ… **ä¿®æ”¹#3**: æå‰é¢„å–
   - æ–‡ä»¶: [layers.py:1577-1660](layers.py#L1577-L1660) EncoderBlock
   - ä¿®æ”¹: åœ¨MHAå¼€å§‹å‰é¢„å–FFN,åœ¨FFNå¼€å§‹å‰é¢„å–L+1çš„attn
   - éš¾åº¦: â­ (ç®€å•)
   - æ”¶ç›Š: â­â­â­â­â­ (æƒé‡åŠ è½½å®Œå…¨é‡å )

**é¢„æœŸæ”¹è¿›**: GPUåˆ©ç”¨ç‡ä» ~50% æå‡åˆ° ~75%

---

### Phase 2: ä¸­ç­‰é£é™©ä¿®æ”¹ (3-5å¤©)

**åœ¨Phase 1éªŒè¯åå®æ–½**:

4. âœ… **æµçº§å¹¶è¡Œ**: MHAä¸FFNåˆ†æµ
   - æ–‡ä»¶: [layers.py:1577-1660](layers.py#L1577-L1660)
   - ä¿®æ”¹: æ‹†åˆ† `forward()` ä¸º `forward_mha()` å’Œ `forward_ffn()`
   - è°ƒæ•´: [model.py:122-222](model.py#L122-L222) Transformerä¸»å¾ªç¯
   - éš¾åº¦: â­â­â­ (è¾ƒéš¾)
   - æ”¶ç›Š: â­â­â­â­â­ (å®Œç¾æµæ°´çº¿)

**é¢„æœŸæ”¹è¿›**: GPUåˆ©ç”¨ç‡ä» ~75% æå‡åˆ° ~90%+

---

### Phase 3: é«˜çº§ä¼˜åŒ– (å¯é€‰,1-2å‘¨)

5. **KV Cacheé¢„å–ä¼˜åŒ–**
   - åœ¨MHAè®¡ç®—æœŸé—´,å¼‚æ­¥é¢„å–L+1çš„KV blocks
   - ä½¿ç”¨ `prefetch_for_next_layer()` ([kv_offload.py:898-904](kv_offload.py#L898-L904))
   - ä½†éœ€ç¡®ä¿ä¸ä¸æƒé‡H2Dç«äº‰å¸¦å®½

6. **åŠ¨æ€æµè°ƒåº¦**
   - æ ¹æ®å±‚å¤§å°åŠ¨æ€è°ƒæ•´æµä¼˜å…ˆçº§
   - å°å±‚ä½¿ç”¨åˆå¹¶æµ,å¤§å±‚ä½¿ç”¨ç‹¬ç«‹æµ

7. **å¸¦å®½æ„ŸçŸ¥è°ƒåº¦**
   - ç›‘æ§PCIeåˆ©ç”¨ç‡ (å·²æœ‰ `_pcie_ema` æœºåˆ¶)
   - åŠ¨æ€è°ƒæ•´é¢„å–æ·±åº¦

---

## 5. ä»£ç ä¿®æ”¹æ¸…å• (Code Changes Checklist)

### 5.1 ä¿®æ”¹æ–‡ä»¶åˆ—è¡¨

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | è¡Œæ•°å˜åŒ– | é£é™© |
|------|---------|---------|------|
| [kv_offload.py](kv_offload.py) | fetch()è¿”å›äº‹ä»¶ | +15è¡Œ | ä½ |
| [weight_streaming_manager.py](weight_streaming_manager.py) | æ–°å¢å¼‚æ­¥API | +50è¡Œ | ä¸­ |
| [layers.py](layers.py) (SelfAttention) | ä½¿ç”¨æ–°äº‹ä»¶API | ~30è¡Œ | ä¸­ |
| [layers.py](layers.py) (FeedForward) | ä½¿ç”¨æ–°äº‹ä»¶API | ~20è¡Œ | ä½ |
| [layers.py](layers.py) (EncoderBlock) | æ·»åŠ æå‰é¢„å– | +20è¡Œ | ä½ |
| [model.py](model.py) (å¯é€‰Phase 2) | æµæ°´çº¿ä¸»å¾ªç¯ | ~50è¡Œ | é«˜ |

---

### 5.2 æµ‹è¯•éªŒè¯æ¸…å•

#### åŠŸèƒ½æµ‹è¯•:
- [ ] å•å±‚å‰å‘ä¼ æ’­ (æ— å›å½’)
- [ ] å¤šå±‚æ¨ç† (è¾“å‡ºä¸€è‡´æ€§)
- [ ] é•¿åºåˆ—ç”Ÿæˆ (ç¨³å®šæ€§)
- [ ] å¤šbatchæ¨ç†

#### æ€§èƒ½æµ‹è¯•:
- [ ] GPUåˆ©ç”¨ç‡ç›‘æ§ (ç›®æ ‡ >85%)
- [ ] PCIeå¸¦å®½åˆ©ç”¨ç‡
- [ ] å±‚é—´ç­‰å¾…æ—¶é—´ (ç›®æ ‡ <1ms)
- [ ] ç«¯åˆ°ç«¯ååé‡ (tokens/sec)

#### å‹åŠ›æµ‹è¯•:
- [ ] æœ€å¤§batch size (32)
- [ ] æœ€å¤§åºåˆ—é•¿åº¦ (2048)
- [ ] è¿ç»­è¿è¡Œç¨³å®šæ€§ (>1000 tokens)

---

## 6. æ€§èƒ½é¢„æµ‹ (Performance Projection)

### å½“å‰åŸºçº¿ (Current Baseline):
```
é…ç½®: Llama3-1-70B, batch=4, seq_len=2048
GPU: 16GB HBM, ~13 TFLOPS (bf16)
å®æµ‹: ~8 tokens/sec/batch

ç“¶é¢ˆåˆ†æ:
- GPUè®¡ç®—æ—¶é—´: ~40% (ç†è®ºå³°å€¼çš„50%)
- æƒé‡ç­‰å¾…: ~30%
- KVç­‰å¾…: ~20%
- å…¶å®ƒ: ~10%
```

### Phase 1åé¢„æµ‹:
```
æ”¹è¿›:
- æƒé‡ç­‰å¾…: 30% â†’ 5% (æå‰é¢„å–)
- KVç­‰å¾…: 20% â†’ 5% (äº‹ä»¶åŒ–)
- GPUåˆ©ç”¨ç‡: 50% â†’ 75%

é¢„æœŸåå: ~12 tokens/sec/batch (æå‡50%)
```

### Phase 2åé¢„æµ‹ (ç†æƒ³):
```
æ”¹è¿›:
- MHA/FFNæµæ°´çº¿: é¢å¤–15%è®¡ç®—é‡å 
- GPUåˆ©ç”¨ç‡: 75% â†’ 90%+

é¢„æœŸåå: ~15-16 tokens/sec/batch (æå‡100%)
```

---

## 7. é£é™©ä¸ç¼“è§£ (Risks & Mitigation)

### é£é™©#1: å†…å­˜æº¢å‡º
**åŸå› **: æµæ°´çº¿æ·±åº¦å¢åŠ  â†’ æ›´å¤šå±‚åŒæ—¶é©»ç•™GPU
**ç¼“è§£**:
- ä¸¥æ ¼æ‰§è¡Œ `gpu_max_groups=8` é™åˆ¶
- ç›‘æ§ `torch.cuda.max_memory_allocated()`
- å¿…è¦æ—¶é™ä½batch size

### é£é™©#2: äº‹ä»¶ç«æ€
**åŸå› **: å¤šæµå¹¶å‘ â†’ äº‹ä»¶é¡ºåºé”™ä¹±
**ç¼“è§£**:
- æ¯ä¸ªç»„ä½¿ç”¨ç‹¬ç«‹äº‹ä»¶
- ä½¿ç”¨ `stream.wait_event()` è€Œé `synchronize()`
- è¯¦ç»†æ—¥å¿—è®°å½•äº‹ä»¶ä¾èµ–é“¾

### é£é™©#3: æ•°å€¼ä¸ä¸€è‡´
**åŸå› **: æµæ°´çº¿æ”¹å˜è®¡ç®—é¡ºåº â†’ æµ®ç‚¹è¯¯å·®ç´¯ç§¯
**ç¼“è§£**:
- é€å±‚éªŒè¯è¾“å‡º (ä¸baselineå¯¹æ¯”)
- ä½¿ç”¨ç¡®å®šæ€§ç®—æ³• (`torch.use_deterministic_algorithms(True)`)
- å®¹å·®æ£€æŸ¥ (ç›¸å¯¹è¯¯å·® <1e-5)

---

## 8. ç›‘æ§ä¸è°ƒè¯• (Monitoring & Debugging)

### æ–°å¢ProfilingæŒ‡æ ‡:

```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            # æµæ°´çº¿æ•ˆç‡
            "weight_h2d_overlap_%": 0,    # æƒé‡åŠ è½½ä¸è®¡ç®—é‡å æ¯”ä¾‹
            "kv_h2d_overlap_%": 0,        # KVåŠ è½½ä¸è®¡ç®—é‡å æ¯”ä¾‹
            "mha_ffn_parallel_%": 0,      # MHAä¸FFNå¹¶è¡Œæ‰§è¡Œæ¯”ä¾‹

            # ç­‰å¾…æ—¶é—´
            "avg_weight_wait_us": 0,      # å¹³å‡æƒé‡ç­‰å¾…æ—¶é—´
            "avg_kv_wait_us": 0,          # å¹³å‡KVç­‰å¾…æ—¶é—´
            "avg_stream_sync_us": 0,      # å¹³å‡æµåŒæ­¥æ—¶é—´

            # èµ„æºåˆ©ç”¨ç‡
            "gpu_compute_%": 0,           # GPUè®¡ç®—æ—¶é—´å æ¯”
            "pcie_util_%": 0,             # PCIeå¸¦å®½åˆ©ç”¨ç‡
            "hbm_util_%": 0,              # HBMåˆ©ç”¨ç‡
        }

    def report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("=" * 70)
        print("Multi-Stream Overlap Performance Report")
        print("=" * 70)
        for k, v in self.metrics.items():
            print(f"  {k:.<50} {v:>10.2f}")
```

### NVTXæ ‡è®° (ç”¨äºNsight Systemsåˆ†æ):

```python
def forward(self, x, start_pos, freqs):
    nvtx.range_push(f"L{self.layer_id}_MHA")
    nvtx.range_push(f"  weight_wait")
    # ... wait for weights ...
    nvtx.range_pop()

    nvtx.range_push(f"  kv_fetch")
    # ... fetch KV ...
    nvtx.range_pop()

    nvtx.range_push(f"  compute")
    # ... attention ...
    nvtx.range_pop()
    nvtx.range_pop()  # MHA
```

---

## 9. ç»“è®ºä¸å»ºè®® (Conclusions & Recommendations)

### ç«‹å³è¡ŒåŠ¨é¡¹ (Immediate Actions):

1. **å®æ–½Phase 1ä¿®æ”¹** (é¢„è®¡2å¤©)
   - KV Cacheäº‹ä»¶åŒ–
   - æƒé‡åŠ è½½å¼‚æ­¥åŒ–
   - æå‰é¢„å–

2. **éªŒè¯æ”¹è¿›** (é¢„è®¡1å¤©)
   - è¿è¡Œbenchmark
   - å¯¹æ¯”baselineååé‡
   - åˆ†æNsight timeline

3. **è¿­ä»£ä¼˜åŒ–** (è§†ç»“æœè€Œå®š)
   - å¦‚æœPhase 1æ•ˆæœå¥½ (>30%æå‡) â†’ ç»§ç»­Phase 2
   - å¦‚æœæ•ˆæœä¸æ˜æ˜¾ â†’ åˆ†ææ–°ç“¶é¢ˆ

### é•¿æœŸå»ºè®®:

- **è€ƒè™‘FlashAttention-2**: å¦‚æœAttentionä»æ˜¯ç“¶é¢ˆ
- **æƒé‡é‡åŒ–**: INT8/INT4å¯é™ä½PCIeå‹åŠ›
- **ç¨€ç–æ³¨æ„åŠ›**: å‡å°‘KV Cacheä¼ è¾“
- **æ¨¡å‹å¹¶è¡Œ**: å¦‚æœå•GPUä»å—é™

### æœ€ç»ˆç›®æ ‡:

**è¾¾æˆæŒ‡æ ‡**:
- âœ… GPUåˆ©ç”¨ç‡ >85%
- âœ… æƒé‡ç­‰å¾…æ—¶é—´ <5%
- âœ… KVç­‰å¾…æ—¶é—´ <5%
- âœ… ååé‡æå‡ >80% (vs baseline)
- âœ… å®Œç¾çš„è®¡ç®—-IOæµæ°´çº¿é‡å 

---

## é™„å½•A: å…³é”®ä»£ç ç‰‡æ®µ (Key Code Snippets)

### A1: ä¿®æ”¹åçš„ KVOffloader.fetch()

```python
def fetch(self, layer: int, blocks: torch.Tensor,
          batch_idx: int = 0, bsz: int | None = None,
          return_event: bool = False):
    """
    å¼‚æ­¥fetch KV blocks,å¯é€‰è¿”å›å®Œæˆäº‹ä»¶ã€‚

    Returns:
        å¦‚æœreturn_event=False: (k_full, v_full)
        å¦‚æœreturn_event=True:  (k_full, v_full, cuda_event)
    """
    uniq = blocks.to(torch.long).unique(sorted=True).tolist()
    use_bsz = int(bsz) if bsz is not None else self.max_batch

    # æ£€æŸ¥é¢„å–ç¼“å­˜
    key = (int(layer), tuple(uniq), int(use_bsz))
    rec = None
    with self._prefetch_lock:
        rec = self._prefetch_map.pop(key, None)

    if rec is not None:
        # å‘½ä¸­é¢„å–ç¼“å­˜
        if return_event:
            return rec["k"], rec["v"], rec["evt"]
        else:
            torch.cuda.current_stream().wait_event(rec["evt"])
            return torch.cat(rec["k"], dim=2), torch.cat(rec["v"], dim=2)

    # æœªå‘½ä¸­,æ‰§è¡ŒåŒæ­¥fetch
    need_load = [b for b in uniq if self.on_ssd[layer][b]]
    for b in need_load:
        self._load_from_ssd(layer, b)

    stream = self.h2d_stream or torch.cuda.current_stream()
    k_parts, v_parts = [], []

    with torch.cuda.stream(stream):
        for b in uniq:
            kc = self.k_cpu[layer][b][:use_bsz]
            vc = self.v_cpu[layer][b][:use_bsz]
            k_parts.append(kc.to(self.device, non_blocking=True))
            v_parts.append(vc.to(self.device, non_blocking=True))

        k_full = torch.cat(k_parts, dim=2)
        v_full = torch.cat(v_parts, dim=2)

        if return_event:
            evt = torch.cuda.Event()
            evt.record(stream)
            return k_full, v_full, evt

    return k_full, v_full
```

### A2: ä¿®æ”¹åçš„ SelfAttention.forward()

```python
def forward(self, x: torch.Tensor, start_pos: int, freqs: torch.Tensor):
    wm = getattr(self, "weight_manager", None)

    # ========================================
    # 1. å¼‚æ­¥è·å–attnç»„å°±ç»ªäº‹ä»¶ (ä¸é˜»å¡)
    # ========================================
    attn_evt = None
    if wm and hasattr(wm, "get_group_ready_event"):
        attn_evt = wm.get_group_ready_event(self.layer_id, "attn")

    # ========================================
    # 2. ç«‹å³é¢„å–åç»­ç»„ (ä¸è®¡ç®—é‡å )
    # ========================================
    if wm and hasattr(wm, "try_prefetch_if_not_ready"):
        # é¢„å–æœ¬å±‚FFN (å°†åœ¨MHAåä½¿ç”¨)
        wm.try_prefetch_if_not_ready(self.layer_id, "ffn")

        # é¢„å–ä¸‹ä¸€å±‚attn (å°†åœ¨æœ¬å±‚FFNæœŸé—´å®Œæˆ)
        if self.layer_id + 1 < wm.n_layers:
            wm.try_prefetch_if_not_ready(self.layer_id + 1, "attn")

    # ========================================
    # 3. åœ¨computeæµä¸Šç­‰å¾…attnç»„äº‹ä»¶
    # ========================================
    with torch.cuda.stream(self.compute_stream):
        if attn_evt is not None:
            self.compute_stream.wait_event(attn_evt)

        # QKVæŠ•å½± (æƒé‡å·²å°±ç»ª)
        q = self.wq(x).view(bsz, seqlen, self.n_heads_q, self.head_dim)
        k = self.wk(x).view(bsz, seqlen, self.n_kv_heads, self.head_dim)
        v = self.wv(x).view(bsz, seqlen, self.n_kv_heads, self.head_dim)

        # RoPE
        q = apply_rotary_embeddings(q, freqs, start_pos)
        k = apply_rotary_embeddings(k, freqs, start_pos)

    # ========================================
    # 4. å¼‚æ­¥fetch KV,è·å–å°±ç»ªäº‹ä»¶
    # ========================================
    k_full, v_full, kv_evt = self.offloader.fetch(
        self.layer_id, needed_blocks,
        batch_idx=batch_idx,
        bsz=bsz,
        return_event=True  # âœ… å…³é”®å‚æ•°
    )

    # ========================================
    # 5. åœ¨computeæµä¸Šç­‰å¾…KVäº‹ä»¶,ç„¶åè®¡ç®—
    # ========================================
    with torch.cuda.stream(self.compute_stream):
        if kv_evt is not None:
            self.compute_stream.wait_event(kv_evt)

        # Attentionè®¡ç®— (KVå·²å°±ç»ª)
        scores = torch.matmul(q, k_full.transpose(2, 3)) / math.sqrt(self.head_dim)
        attn_weights = torch.softmax(scores, dim=-1)
        out = torch.matmul(attn_weights, v_full)

        # è¾“å‡ºæŠ•å½±
        result = self.wo(out.transpose(1, 2).reshape(bsz, seqlen, -1))

    return result
```

---

## é™„å½•B: æ€§èƒ½åˆ†æè„šæœ¬ (Performance Analysis Script)

```python
#!/usr/bin/env python3
"""
ä½¿ç”¨Nsight Systemsåˆ†æå¤šæµé‡å æ•ˆç‡ã€‚

ç”¨æ³•:
    nsys profile -o timeline.qdrep python analyze_overlap.py
    nsys stats --report cuda_gpu_trace timeline.qdrep
"""

import torch
import time
from llama3.generator import LLaMA

def analyze_stream_overlap(llama, prompt, max_gen_len=32):
    """
    åˆ†ææµé‡å æ•ˆç‡çš„è¯¦ç»†æŠ¥å‘Šã€‚
    """
    # å¯åŠ¨profiling
    torch.cuda.cudart().cudaProfilerStart()

    # ç”Ÿæˆtokens
    start = time.time()
    out_tokens, _ = llama.text_completion(
        prompts=[prompt],
        temperature=0.6,
        max_gen_len=max_gen_len,
        batch_size=4
    )
    end = time.time()

    # åœæ­¢profiling
    torch.cuda.cudart().cudaProfilerStop()

    # è®¡ç®—æŒ‡æ ‡
    elapsed_ms = (end - start) * 1000
    n_tokens = len(out_tokens[0])
    tokens_per_sec = n_tokens / (elapsed_ms / 1000)

    print("\n" + "=" * 70)
    print("Stream Overlap Analysis")
    print("=" * 70)
    print(f"Total time:      {elapsed_ms:.2f} ms")
    print(f"Tokens generated: {n_tokens}")
    print(f"Throughput:      {tokens_per_sec:.2f} tokens/sec")
    print("=" * 70)

    # è¯¦ç»†æŒ‡æ ‡ (éœ€è¦ä»global trackerè·å–)
    from llama3.global_state_tracker import get_global_tracker
    tracker = get_global_tracker()
    if tracker:
        summary = tracker.get_layer_timing_summary()
        if summary:
            print(f"\nPer-layer timing:")
            print(f"  Avg time/layer: {summary['avg_time_ms']:.4f} ms")
            print(f"  Min time:       {summary['min_time_ms']:.4f} ms")
            print(f"  Max time:       {summary['max_time_ms']:.4f} ms")

    return tokens_per_sec

if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    llama = LLaMA.build(
        checkpoints_dir="/path/to/Llama3.1-70B",
        load_model=False,
        device="cuda:0",
        max_seq_len=2048,
        max_batch_size=32,
        mode="mixed",
        mode_config={
            "ssd_manifest_path": "/path/to/manifest.json",
            "prefetch_distance": 6,
            "max_cached_layers": 4,
            "cpu_cache_layers": 40,
            "warmup_layers": 4,
        }
    )

    # åˆ†æ
    prompt = "Write a detailed analysis of..."
    tps = analyze_stream_overlap(llama, prompt, max_gen_len=32)

    print(f"\nâœ… Final throughput: {tps:.2f} tokens/sec")
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-01-XX
**ä½œè€…**: Claude (Anthropic)
**å®¡æ ¸çŠ¶æ€**: å¾…æŠ€æœ¯å®¡æ ¸
