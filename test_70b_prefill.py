#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
70B æ¨¡å‹ Prefill é˜¶æ®µæµ‹è¯•è„šæœ¬ (ä¼˜åŒ–for 16GB æ˜¾å­˜)

ä¸“é—¨æµ‹è¯• 70B æ¨¡å‹çš„ prefill é˜¶æ®µæ€§èƒ½å’Œæƒé‡æµå¼ä¼ è¾“æ•ˆæœã€‚
ä¸è¿›è¡Œ decode é˜¶æ®µï¼Œåªæµ‹è¯•ä¸€æ¬¡æ€§å¤„ç†é•¿åºåˆ—çš„èƒ½åŠ›ã€‚

é’ˆå¯¹ 16GB æ˜¾å­˜çš„é…ç½®ä¼˜åŒ–ï¼š
- æå°çš„ GPU ç¼“å­˜å±‚æ•° (1å±‚)
- è¾ƒå°çš„ batch size (1)
- ä¸­ç­‰çš„åºåˆ—é•¿åº¦ (256 tokens)

ä½¿ç”¨æ–¹å¼ï¼š
    python test_70b_prefill.py
"""
import sys
from pathlib import Path
import torch
import time

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ sys.path
sys.path.insert(0, str(Path(__file__).parent))

from llama3.generator import LLaMA
from llama3.config import KVCacheArgs


def test_70b_prefill():
    """
    æµ‹è¯• 70B æ¨¡å‹çš„ prefill é˜¶æ®µ (16GB æ˜¾å­˜ä¼˜åŒ–é…ç½®)
    """
    print("=" * 80)
    print("70B æ¨¡å‹ Prefill æµ‹è¯• - æƒé‡æµå¼ä¼ è¾“æ¨¡å¼ (16GB æ˜¾å­˜)")
    print("=" * 80)

    # ---- æ¨¡å‹è·¯å¾„é…ç½® ----
    checkpoint_dir = "/home/roger/.llama/checkpoints/Llama3.1-70B/"
    ckpt_path = Path(checkpoint_dir)

    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not ckpt_path.exists():
        print(f"\nâŒ é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {checkpoint_dir}")
        print("è¯·è®¾ç½®æ­£ç¡®çš„ 70B æ¨¡å‹è·¯å¾„")
        return False

    # æ£€æŸ¥ params.json
    params_file = ckpt_path / "params.json"
    if not params_file.exists():
        print(f"\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° params.json æ–‡ä»¶: {params_file}")
        return False

    # æ£€æŸ¥æƒé‡æ–‡ä»¶
    pth_files = list(ckpt_path.glob("*.pth"))
    if not pth_files:
        print(f"\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° .pth æƒé‡æ–‡ä»¶: {ckpt_path}")
        return False

    print("\nâœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    print(f"  - params.json: {params_file}")
    print(f"  - æƒé‡æ–‡ä»¶: {len(pth_files)} ä¸ª")

    # ---- è®¾å¤‡é…ç½® ----
    if not torch.cuda.is_available():
        print("\nâŒ é”™è¯¯ï¼šCUDA ä¸å¯ç”¨ï¼Œæ— æ³•æµ‹è¯• 70B æ¨¡å‹")
        return False

    device = "cuda:0"

    # æ‰“å° GPU ä¿¡æ¯
    print(f"\nğŸ“Š GPU ä¿¡æ¯:")
    try:
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"  - è®¾å¤‡: {gpu_name}")
        print(f"  - æ˜¾å­˜: {gpu_mem_gb:.2f} GB")

        if gpu_mem_gb < 16:
            print(f"\nâš ï¸  è­¦å‘Šï¼šGPU æ˜¾å­˜å°äº 16GBï¼Œæµ‹è¯•å¯èƒ½å¤±è´¥")
            return False
    except Exception as e:
        print(f"\nâŒ é”™è¯¯ï¼šæ— æ³•è·å– GPU ä¿¡æ¯: {e}")
        return False

    # ---- KV Cache å‚æ•°é…ç½® (16GB ä¼˜åŒ–) ----
    print("\nâš™ï¸  é…ç½® KV Cache å‚æ•° (16GB æ˜¾å­˜ä¼˜åŒ–)...")
    try:
        KVCacheArgs.dram_limit_gb = 16.0              # DRAM é™åˆ¶
        KVCacheArgs.dram_sizing_batch = 2             # å°æ‰¹é‡ä¼°ç®—
        KVCacheArgs.ssd_device_path = "/dev/nvme0n1p4"  # SSD è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        KVCacheArgs.max_concurrent_io = 4
        print(f"  - DRAM limit: {KVCacheArgs.dram_limit_gb} GB")
        print(f"  - DRAM sizing batch: {KVCacheArgs.dram_sizing_batch}")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯ï¼šKV Cache é…ç½®å¤±è´¥: {e}")
        return False

    # ---- Prefill æµ‹è¯•å‚æ•° (16GB ä¼˜åŒ–) ----
    prefill_seq_len = 256  # è¾ƒå°çš„åºåˆ—é•¿åº¦
    batch_size = 1         # å•batch

    print(f"\nğŸ“ Prefill æµ‹è¯•å‚æ•° (16GB ä¼˜åŒ–):")
    print(f"  - Sequence length: {prefill_seq_len}")
    print(f"  - Batch size: {batch_size}")

    # ---- æ„å»ºæ¨¡å‹ï¼ˆæƒé‡æµå¼ä¼ è¾“æ¨¡å¼ï¼‰----
    print("\nğŸ”¨ æ„å»º 70B æ¨¡å‹ï¼ˆæƒé‡æµå¼ä¼ è¾“æ¨¡å¼ï¼‰...")

    # 70B æ¨¡å‹æµå¼é…ç½® (16GB æ˜¾å­˜ä¼˜åŒ–)
    # æ ¹æ®ä½ çš„ GPU å†…å­˜æƒ…å†µè°ƒæ•´:
    # - å¦‚æœæœ‰ 20GB+: max_cached_layers=3-4, prefetch_distance=2-3
    # - å¦‚æœæ˜¯ 16GB: max_cached_layers=1-2, prefetch_distance=2
    # - å¦‚æœç»å¸¸ OOM: max_cached_layers=1, batch_size=1, seq_len=128

    streaming_config = {
        "prefetch_distance": 2,      # é¢„å–è·ç¦» (å¯ä»¥ç¨å¤§ï¼Œä¸å GPUå†…å­˜)
        "max_cached_layers": 1,      # GPUç¼“å­˜å±‚æ•° (70Bæ¯å±‚çº¦1.8GB)
        "warmup_layers": 0,          # ä¸é¢„çƒ­ï¼ˆèŠ‚çœåˆå§‹åŒ–æ—¶é—´ï¼‰
        "verbose": True,             # å¯ç”¨è¯¦ç»†æ—¥å¿—
    }

    print(f"\nğŸ“¦ æµå¼é…ç½® (16GB æ˜¾å­˜):")
    print(f"  - Prefetch distance: {streaming_config['prefetch_distance']} (å¼‚æ­¥é¢„å–)")
    print(f"  - Max cached layers: {streaming_config['max_cached_layers']} (æ¯å±‚çº¦1.8GB FP16)")
    print(f"  - Warmup layers: {streaming_config['warmup_layers']}")
    print(f"  - é¢„è®¡å³°å€¼æ˜¾å­˜: ~5GB (æ ¸å¿ƒ) + {streaming_config['max_cached_layers']*1.8:.1f}GB (å±‚ç¼“å­˜) + 2-3GB (KV+æ¿€æ´»)")
    print(f"  - æ€»è®¡é¢„è®¡: ~{5 + streaming_config['max_cached_layers']*1.8 + 2.5:.1f}GB")

    llama = None
    try:
        start_time = time.time()
        llama = LLaMA.build(
            checkpoints_dir=str(ckpt_path),
            load_model=True,
            device=device,
            mode="stream",
            mode_config=streaming_config,
            max_seq_len=prefill_seq_len,
            max_batch_size=batch_size,
            topk_blk=8,
        )
        build_time = time.time() - start_time
        print(f"\nâœ… æ¨¡å‹æ„å»ºæˆåŠŸ (è€—æ—¶: {build_time:.2f}s)")
    except torch.cuda.OutOfMemoryError as e:
        print(f"\nâŒ é”™è¯¯ï¼šæ„å»ºæ¨¡å‹æ—¶ GPU å†…å­˜ä¸è¶³")
        print(f"è¯¦ç»†ä¿¡æ¯: {e}")
        print("\nğŸ’¡ å»ºè®®:")
        print("  1) å…³é—­æ‰€æœ‰å…¶ä»–å ç”¨ GPU çš„ç¨‹åº")
        print("  2) å‡å°‘ prefill_seq_len åˆ° 128")
        print("  3) ç¡®ä¿ç³»ç»Ÿæœ‰è¶³å¤Ÿçš„ DRAM (å»ºè®® 32GB+)")
        torch.cuda.empty_cache()
        return False
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯ï¼šæƒé‡æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        return False
    except Exception as e:
        print(f"\nâŒ é”™è¯¯ï¼šæ¨¡å‹æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    # éªŒè¯æ¨¡å‹
    if llama is None or llama.model is None:
        print(f"\nâŒ é”™è¯¯ï¼šæ¨¡å‹å¯¹è±¡æœªæ­£ç¡®åˆå§‹åŒ–")
        return False

    # ---- å‡†å¤‡æµ‹è¯•æç¤ºè¯ ----
    prompts = [
        "Once upon a time, in a land far far away, there lived a wise old wizard who possessed great knowledge.",
    ][:batch_size]

    print(f"\nğŸ“ æµ‹è¯•æç¤ºè¯ ({len(prompts)} æ¡):")
    for i, p in enumerate(prompts, 1):
        preview = p[:80] + "..." if len(p) > 80 else p
        print(f"  {i}. '{preview}'")

    # ---- è¿è¡Œ Prefill æµ‹è¯• ----
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹ Prefill æµ‹è¯•")
    print("=" * 80)

    try:
        # Tokenize
        print("\n[Prefill] Tokenizing prompts...")
        try:
            prompts_tok = [llama.tokenizer.encode(p, add_special_tokens=False) for p in prompts]
        except Exception as e:
            print(f"\nâŒ é”™è¯¯ï¼šTokenization å¤±è´¥: {e}")
            return False

        prompt_lens = [len(tok) for tok in prompts_tok]
        print(f"[Prefill] Token lengths: {prompt_lens}")
        max_prompt_len = max(prompt_lens)
        print(f"[Prefill] Max prompt length: {max_prompt_len} tokens")

        # æ£€æŸ¥å¹¶æˆªæ–­
        if max_prompt_len > prefill_seq_len:
            print(f"\nâš ï¸  è­¦å‘Šï¼šæç¤ºè¯é•¿åº¦ ({max_prompt_len}) è¶…è¿‡é…ç½® ({prefill_seq_len})")
            print("å°†æˆªæ–­åˆ°é…ç½®é•¿åº¦")
            max_prompt_len = prefill_seq_len

        # Prepare input tensors
        print(f"\n[Prefill] Preparing input tensors...")
        try:
            bsz = len(prompts_tok)
            pad_id = llama.tokenizer.pad_token_id if llama.tokenizer.pad_token_id else llama.tokenizer.eos_token_id

            tokens = torch.full(
                (bsz, max_prompt_len),
                pad_id,
                dtype=torch.long,
                device=device,
            )

            for i, tok in enumerate(prompts_tok):
                tok_len = min(len(tok), max_prompt_len)
                tokens[i, :tok_len] = torch.tensor(tok[:tok_len], device=device)

            print(f"[Prefill] Input tensor shape: {tokens.shape}")
        except torch.cuda.OutOfMemoryError as e:
            print(f"\nâŒ é”™è¯¯ï¼šå‡†å¤‡è¾“å…¥å¼ é‡æ—¶ GPU å†…å­˜ä¸è¶³: {e}")
            torch.cuda.empty_cache()
            return False
        except Exception as e:
            print(f"\nâŒ é”™è¯¯ï¼šå‡†å¤‡è¾“å…¥å¼ é‡å¤±è´¥: {e}")
            return False

        # Run prefill forward pass
        print(f"\n[Prefill] Running forward pass...")
        print("=" * 80)
        print("âš ï¸  æ³¨æ„ï¼š16GB æ˜¾å­˜ä¸‹ï¼Œ70B æ¨¡å‹æ¯å±‚ä¼ è¾“éœ€è¦æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        print("=" * 80)

        start_time = time.time()
        logits = None

        try:
            with torch.no_grad():
                # åªåšä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œstart_pos=0 è¡¨ç¤ºè¿™æ˜¯ prefill
                logits = llama.model(tokens, start_pos=0)

            prefill_time = time.time() - start_time

            # éªŒè¯è¾“å‡º
            if logits is None:
                print(f"\nâŒ é”™è¯¯ï¼šæ¨¡å‹è¾“å‡ºä¸º None")
                return False

            print("=" * 80)
            print(f"\nâœ… Prefill å®Œæˆï¼")
            print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            print(f"  - Prefill time: {prefill_time:.3f}s")
            print(f"  - Tokens processed: {max_prompt_len * bsz}")
            print(f"  - Throughput: {(max_prompt_len * bsz) / prefill_time:.2f} tokens/s")
            print(f"  - Avg time per token: {(prefill_time / (max_prompt_len * bsz)) * 1000:.2f} ms")
            print(f"  - Logits shape: {logits.shape}")

        except torch.cuda.OutOfMemoryError as e:
            print(f"\nâŒ é”™è¯¯ï¼šForward pass æ—¶ GPU å†…å­˜ä¸è¶³: {e}")
            torch.cuda.empty_cache()
            return False
        except RuntimeError as e:
            print(f"\nâŒ é”™è¯¯ï¼šForward pass è¿è¡Œæ—¶é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False
        except Exception as e:
            print(f"\nâŒ é”™è¯¯ï¼šForward pass å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

        # GPU å†…å­˜ç»Ÿè®¡
        if torch.cuda.is_available():
            try:
                mem_allocated = torch.cuda.memory_allocated(device) / 1024**3
                mem_reserved = torch.cuda.memory_reserved(device) / 1024**3
                peak_mem = torch.cuda.max_memory_allocated(device) / 1024**3
                print(f"\nğŸ’¾ GPU å†…å­˜ä½¿ç”¨:")
                print(f"  - Current allocated: {mem_allocated:.2f} GB")
                print(f"  - Current reserved: {mem_reserved:.2f} GB")
                print(f"  - Peak allocated: {peak_mem:.2f} GB")
                print(f"  - Utilization: {(mem_allocated / gpu_mem_gb) * 100:.1f}%")
            except Exception as e:
                print(f"\nâš ï¸  è­¦å‘Šï¼šæ— æ³•è·å– GPU å†…å­˜ç»Ÿè®¡: {e}")

        # Weight Streaming Manager ç»Ÿè®¡
        if hasattr(llama, 'weight_streaming_manager'):
            try:
                wsm = llama.weight_streaming_manager
                print(f"\nğŸ“¦ Weight Streaming ç»Ÿè®¡:")
                print(f"  - GPU cache size: {len(wsm.gpu_cache)}/{wsm.max_cached_layers} layers")
                print(f"  - Total layers: {len(wsm.blocks)}")
                if wsm.ssd_enabled and hasattr(wsm, 'cpu_cache'):
                    print(f"  - CPU cache enabled: {len(wsm.cpu_cache)} layers cached")
            except Exception as e:
                print(f"\nâš ï¸  è­¦å‘Šï¼šæ— æ³•è·å– WSM ç»Ÿè®¡: {e}")

        return True

    except KeyboardInterrupt:
        print(f"\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return False
    except Exception as e:
        print(f"\nâŒ é”™è¯¯ï¼šPrefill æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        # æ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


if __name__ == "__main__":
    print("\nğŸ”¬ å¼€å§‹ 70B æ¨¡å‹ Prefill æµ‹è¯• (16GB æ˜¾å­˜)")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        try:
            print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"GPU è®¾å¤‡: {torch.cuda.get_device_name(0)}")
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"GPU å†…å­˜: {gpu_mem:.2f} GB")

            if gpu_mem > 16.5:
                print("âœ… æ˜¾å­˜å……è¶³ (>16GB)")
            elif gpu_mem >= 15.5:
                print("âš ï¸  æ˜¾å­˜åˆšå¥½è¾¾åˆ°è¦æ±‚ (~16GB)")
            else:
                print(f"âŒ æ˜¾å­˜ä¸è¶³ ({gpu_mem:.1f}GB < 16GB)")
                print("æµ‹è¯•å¯èƒ½ä¼šå¤±è´¥")
        except Exception as e:
            print(f"âš ï¸  è­¦å‘Šï¼šæ— æ³•è·å–å®Œæ•´ GPU ä¿¡æ¯: {e}")

    # è¿è¡ŒåŸºç¡€ prefill æµ‹è¯•
    success = False
    try:
        success = test_70b_prefill()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

    # æ€»ç»“
    print(f"\n{'=' * 80}")
    print("æµ‹è¯•æ€»ç»“")
    print(f"{'=' * 80}")
    print(f"Prefill æµ‹è¯•: {'âœ… é€šè¿‡' if success else 'âŒ å¤±è´¥'}")

    if success:
        print("\nğŸ‰ æ­å–œï¼70B æ¨¡å‹ Prefill æµ‹è¯•é€šè¿‡ (16GB æ˜¾å­˜)")
        print("\nå…³é”®éªŒè¯ç‚¹:")
        print("  âœ… æƒé‡æµå¼ä¼ è¾“å·¥ä½œæ­£å¸¸")
        print("  âœ… 16GB æ˜¾å­˜æˆåŠŸè¿è¡Œ 70B æ¨¡å‹")
        print("  âœ… GPU å†…å­˜ç®¡ç†æœ‰æ•ˆ")
        print("\næ€§èƒ½æç¤º:")
        print("  - 16GB æ˜¾å­˜ä¸‹ï¼Œæ¯å±‚éœ€è¦ä» CPU/SSD ä¼ è¾“")
        print("  - ååé‡ä¼šä½äºå¤§æ˜¾å­˜ GPU")
        print("  - é€‚åˆéªŒè¯åŠŸèƒ½ï¼Œä¸é€‚åˆç”Ÿäº§éƒ¨ç½²")
    else:
        print("\nâš ï¸  æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
        print("\nå¸¸è§é—®é¢˜:")
        print("  1) æ£€æŸ¥ 70B æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("  2) ç¡®ä¿æ²¡æœ‰å…¶ä»–ç¨‹åºå ç”¨ GPU")
        print("  3) å°è¯•å‡å° prefill_seq_len (å½“å‰ 256)")
        print("  4) ç¡®ä¿ç³»ç»Ÿæœ‰è¶³å¤Ÿ DRAM (å»ºè®® 32GB+)")
        print("  5) æ£€æŸ¥ CUDA å’Œ PyTorch å®‰è£…")
