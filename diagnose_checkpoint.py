#!/usr/bin/env python3
"""
Checkpoint è¯Šæ–­å·¥å…·
æ£€æŸ¥ checkpoint æ–‡ä»¶çš„å®Œæ•´æ€§å’Œå‚æ•°ä¸€è‡´æ€§
"""

import json
import torch
from pathlib import Path
from typing import Dict, List, Tuple

def check_checkpoint_integrity(ckpt_dir: str) -> Dict:
    """
    æ£€æŸ¥ checkpoint çš„å®Œæ•´æ€§

    Returns:
        dict: è¯Šæ–­ç»“æœ
    """
    ckpt_path = Path(ckpt_dir)
    result = {
        "checkpoint_dir": str(ckpt_path),
        "params_json_exists": False,
        "checkpoint_files_exist": False,
        "params_json_content": None,
        "checkpoint_vocab_size": None,
        "vocab_size_match": False,
        "issues": [],
        "recommendations": []
    }

    # 1. æ£€æŸ¥ params.json
    params_file = ckpt_path / "params.json"
    if params_file.exists():
        result["params_json_exists"] = True
        try:
            params = json.loads(params_file.read_text())
            result["params_json_content"] = params
            print(f"âœ… params.json å­˜åœ¨")
            print(f"   vocab_size: {params.get('vocab_size')}")
            print(f"   n_layers: {params.get('n_layers')}")
            print(f"   dim: {params.get('dim')}")
        except Exception as e:
            result["issues"].append(f"æ— æ³•è¯»å– params.json: {e}")
            print(f"âŒ æ— æ³•è¯»å– params.json: {e}")
    else:
        result["issues"].append("params.json ä¸å­˜åœ¨")
        print(f"âŒ params.json ä¸å­˜åœ¨")
        return result

    # 2. æ£€æŸ¥ checkpoint æ–‡ä»¶
    ckpt_files = list(ckpt_path.glob("consolidated.*.pth"))
    if ckpt_files:
        result["checkpoint_files_exist"] = True
        print(f"\nâœ… æ‰¾åˆ° {len(ckpt_files)} ä¸ª checkpoint æ–‡ä»¶")

        # åŠ è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶æ£€æŸ¥ vocab_size
        try:
            print(f"   åŠ è½½ {ckpt_files[0].name} æ£€æŸ¥å‚æ•°...")
            ckpt = torch.load(ckpt_files[0], map_location='cpu', weights_only=True)

            # æ£€æŸ¥ embedding å±‚
            embed_keys = [k for k in ckpt.keys() if 'embed' in k.lower()]
            output_keys = [k for k in ckpt.keys() if k == 'output.weight']

            if embed_keys:
                embed_key = embed_keys[0]
                embed_shape = ckpt[embed_key].shape
                checkpoint_vocab = embed_shape[0]
                result["checkpoint_vocab_size"] = checkpoint_vocab
                print(f"   {embed_key}: {embed_shape}")
                print(f"   Checkpoint vocab_size: {checkpoint_vocab}")

            if output_keys:
                output_shape = ckpt['output.weight'].shape
                print(f"   output.weight: {output_shape}")

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…
            params_vocab = result["params_json_content"].get("vocab_size")
            if result["checkpoint_vocab_size"] and params_vocab:
                if result["checkpoint_vocab_size"] == params_vocab:
                    result["vocab_size_match"] = True
                    print(f"\nâœ… vocab_size åŒ¹é…: {params_vocab}")
                else:
                    result["vocab_size_match"] = False
                    result["issues"].append(
                        f"vocab_size ä¸åŒ¹é…: params.json={params_vocab}, checkpoint={result['checkpoint_vocab_size']}"
                    )
                    print(f"\nâŒ vocab_size ä¸åŒ¹é…!")
                    print(f"   params.json: {params_vocab}")
                    print(f"   checkpoint: {result['checkpoint_vocab_size']}")

                    # åˆ†æé—®é¢˜
                    if result["checkpoint_vocab_size"] == 16032:
                        result["recommendations"].append(
                            "Checkpoint vocab_size=16032 å¯èƒ½æ˜¯:\n"
                            "  1. æ—§ç‰ˆæœ¬çš„ Llama æ¨¡å‹\n"
                            "  2. ä½¿ç”¨äº†ä¸åŒçš„ tokenizer\n"
                            "  3. Checkpoint æ–‡ä»¶è¢«æˆªæ–­æˆ–æŸå\n"
                            "  4. è¿™ä¸æ˜¯å®˜æ–¹çš„ Llama 3.1-70B checkpoint"
                        )

                    if params_vocab == 128256:
                        result["recommendations"].append(
                            "params.json vocab_size=128256 æ˜¯ Llama 3.1 çš„æ ‡å‡†é…ç½®"
                        )

        except Exception as e:
            result["issues"].append(f"æ— æ³•åŠ è½½ checkpoint æ–‡ä»¶: {e}")
            print(f"âŒ æ— æ³•åŠ è½½ checkpoint: {e}")
    else:
        result["issues"].append("æœªæ‰¾åˆ° checkpoint æ–‡ä»¶ (consolidated.*.pth)")
        print(f"âŒ æœªæ‰¾åˆ° checkpoint æ–‡ä»¶")

    # 3. æ£€æŸ¥ tokenizer
    print(f"\nğŸ“ æ£€æŸ¥ tokenizer...")
    try:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(str(ckpt_path))
        tokenizer_vocab = tokenizer.vocab_size
        print(f"   Tokenizer vocab_size: {tokenizer_vocab}")

        if result["checkpoint_vocab_size"] and tokenizer_vocab != result["checkpoint_vocab_size"]:
            result["issues"].append(
                f"Tokenizer vocab_size ({tokenizer_vocab}) ä¸ checkpoint ({result['checkpoint_vocab_size']}) ä¸åŒ¹é…"
            )
            print(f"   âš ï¸  ä¸ checkpoint ä¸åŒ¹é…!")
    except Exception as e:
        print(f"   âš ï¸  æ— æ³•åŠ è½½ tokenizer: {e}")

    return result


def generate_fix_script(result: Dict, output_path: str = "fix_vocab_mismatch.sh"):
    """
    ç”Ÿæˆä¿®å¤è„šæœ¬
    """
    if result["vocab_size_match"]:
        print(f"\nâœ… æ— éœ€ä¿®å¤ï¼Œvocab_size å·²åŒ¹é…")
        return

    checkpoint_vocab = result["checkpoint_vocab_size"]
    params_vocab = result["params_json_content"]["vocab_size"]

    script_content = f"""#!/bin/bash
# Checkpoint vocab_size ä¿®å¤è„šæœ¬
# è‡ªåŠ¨ç”Ÿæˆäº diagnose_checkpoint.py

set -e

CKPT_DIR="{result['checkpoint_dir']}"

echo "================================================"
echo "Llama3 Checkpoint vocab_size ä¿®å¤"
echo "================================================"
echo ""
echo "æ£€æµ‹åˆ°çš„é—®é¢˜:"
echo "  params.json vocab_size: {params_vocab}"
echo "  checkpoint vocab_size: {checkpoint_vocab}"
echo ""

# é€‰é¡¹1: ä½¿ç”¨ checkpoint çš„ vocab_size (æ¨èç”¨äºæµ‹è¯•)
fix_option_1() {{
    echo "é€‰é¡¹1: ä¿®æ”¹ params.json åŒ¹é… checkpoint ({checkpoint_vocab})"
    echo "  âš ï¸  æ³¨æ„: è¿™åªæ˜¯ä¸´æ—¶æ–¹æ¡ˆï¼Œç”¨äºæµ‹è¯•ç³»ç»Ÿæ˜¯å¦å·¥ä½œ"
    echo "  âš ï¸  æ­£ç¡®çš„åšæ³•æ˜¯è·å–å®Œæ•´çš„ checkpoint æ–‡ä»¶"
    echo ""

    # å¤‡ä»½åŸæ–‡ä»¶
    cp "$CKPT_DIR/params.json" "$CKPT_DIR/params.json.bak"
    echo "âœ… å·²å¤‡ä»½: params.json.bak"

    # ä¿®æ”¹ vocab_size
    python3 << 'PYTHON_SCRIPT'
import json
from pathlib import Path

params_path = Path("{result['checkpoint_dir']}/params.json")
params = json.loads(params_path.read_text())
params['vocab_size'] = {checkpoint_vocab}
params_path.write_text(json.dumps(params, indent=2))
print(f"âœ… å·²ä¿®æ”¹ params.json: vocab_size={checkpoint_vocab}")
PYTHON_SCRIPT

    echo ""
    echo "âœ… ä¿®å¤å®Œæˆï¼ç°åœ¨å¯ä»¥é‡æ–°ç”Ÿæˆ manifest:"
    echo ""
    echo "cd /home/roger/llama3-inference"
    echo "/home/roger/.pyenv/versions/inference_proj/bin/python generate_manifest.py from-meta /data1/llama3.1-70b.shapes_meta.json --out /data1/llama3.1-70b.runtime_manifest.json"
}}

# é€‰é¡¹2: æ¢å¤å¤‡ä»½
restore_backup() {{
    if [ -f "$CKPT_DIR/params.json.bak" ]; then
        cp "$CKPT_DIR/params.json.bak" "$CKPT_DIR/params.json"
        echo "âœ… å·²æ¢å¤å¤‡ä»½"
    else
        echo "âŒ æœªæ‰¾åˆ°å¤‡ä»½æ–‡ä»¶"
    fi
}}

# ä¸»èœå•
echo "è¯·é€‰æ‹©æ“ä½œ:"
echo "  1) ä¿®æ”¹ params.json åŒ¹é… checkpoint (ä¸´æ—¶æ–¹æ¡ˆ)"
echo "  2) æ¢å¤ params.json å¤‡ä»½"
echo "  3) é€€å‡º"
echo ""
read -p "è¯·è¾“å…¥é€‰é¡¹ (1-3): " choice

case $choice in
    1)
        fix_option_1
        ;;
    2)
        restore_backup
        ;;
    3)
        echo "é€€å‡º"
        exit 0
        ;;
    *)
        echo "âŒ æ— æ•ˆé€‰é¡¹"
        exit 1
        ;;
esac
"""

    output = Path(output_path)
    output.write_text(script_content)
    output.chmod(0o755)

    print(f"\nâœ… å·²ç”Ÿæˆä¿®å¤è„šæœ¬: {output_path}")
    print(f"\nè¿è¡Œæ–¹æ³•:")
    print(f"  ./{output_path}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="æ£€æŸ¥ checkpoint å®Œæ•´æ€§")
    parser.add_argument(
        "checkpoint_dir",
        type=str,
        default="/home/roger/.llama/checkpoints/Llama3.1-70B",
        nargs='?',
        help="Checkpoint ç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "--generate-fix",
        action="store_true",
        help="ç”Ÿæˆä¿®å¤è„šæœ¬"
    )

    args = parser.parse_args()

    print("=" * 60)
    print("Llama3 Checkpoint è¯Šæ–­å·¥å…·")
    print("=" * 60)
    print()

    result = check_checkpoint_integrity(args.checkpoint_dir)

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("è¯Šæ–­æ€»ç»“")
    print("=" * 60)

    if result["issues"]:
        print("\nâŒ å‘ç°çš„é—®é¢˜:")
        for i, issue in enumerate(result["issues"], 1):
            print(f"  {i}. {issue}")

    if result["recommendations"]:
        print("\nğŸ’¡ å»ºè®®:")
        for rec in result["recommendations"]:
            print(f"\n{rec}")

    if not result["issues"]:
        print("\nâœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡!")

    # ç”Ÿæˆä¿®å¤è„šæœ¬
    if args.generate_fix or not result["vocab_size_match"]:
        if not result["vocab_size_match"]:
            generate_fix_script(result)

    print("\n" + "=" * 60)


if __name__ == "__main__":
    main()
