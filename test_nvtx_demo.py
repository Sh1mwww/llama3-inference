#!/usr/bin/env python3
"""
NVTXæ¼”ç¤ºè„šæœ¬ - æ¨¡æ‹Ÿæƒé‡æµå¼ä¼ è¾“çš„æ€§èƒ½æ¨¡å¼
æ­¤è„šæœ¬ä¸éœ€è¦å®é™…çš„æ¨¡å‹ï¼Œåªæ˜¯æ¼”ç¤ºNVTXæ ‡è®°å’ŒCUDAæ“ä½œ
"""

import torch
import time
import random

# NVTX profiling support
try:
    import torch.cuda.nvtx as nvtx
    NVTX_AVAILABLE = True
    print("âœ… NVTXå¯ç”¨")
except ImportError:
    print("âŒ NVTXä¸å¯ç”¨")
    # Fallback no-op functions
    class nvtx:
        @staticmethod
        def range_push(name): pass
        @staticmethod
        def range_pop(): pass
    NVTX_AVAILABLE = False

def simulate_weight_loading(layer_id, modules):
    """æ¨¡æ‹Ÿæƒé‡åŠ è½½è¿‡ç¨‹"""
    nvtx.range_push(f"ensure_layer_{layer_id}")
    
    # æ¨¡æ‹Ÿç¼“å­˜æ£€æŸ¥
    cache_hit = random.random() > 0.3  # 70% å‘½ä¸­ç‡
    
    if cache_hit:
        nvtx.range_push(f"cache_hit_layer_{layer_id}")
        time.sleep(0.001)  # å¿«é€Ÿå‘½ä¸­
        nvtx.range_pop()  # cache_hit
    else:
        nvtx.range_push(f"cache_miss_layer_{layer_id}")
        
        # æ¨¡æ‹ŸLRUé€å‡º
        if random.random() > 0.5:
            old_layer = layer_id - 4
            nvtx.range_push(f"evict_layer_{old_layer}")
            time.sleep(0.002)
            nvtx.range_pop()  # evict
        
        # æ¨¡æ‹ŸH2Dä¼ è¾“
        nvtx.range_push(f"h2d_transfer_layer_{layer_id}")
        for module in modules:
            nvtx.range_push(f"h2d_{module}")
            # æ¨¡æ‹Ÿä¸åŒå¤§å°çš„æƒé‡ä¼ è¾“æ—¶é—´
            if module in ["w1", "w2", "w3"]:  # FFNæƒé‡æ›´å¤§
                time.sleep(0.01)
            else:  # attentionæƒé‡
                time.sleep(0.005)
            nvtx.range_pop()  # h2d_module
        nvtx.range_pop()  # h2d_transfer
        
        nvtx.range_pop()  # cache_miss
    
    nvtx.range_pop()  # ensure_layer

def simulate_prefetch(layer_ids):
    """æ¨¡æ‹Ÿæƒé‡é¢„å–"""
    if not layer_ids:
        return
        
    nvtx.range_push(f"prefetch_layers_{layer_ids}")
    
    for layer_id in layer_ids:
        nvtx.range_push(f"prefetch_layer_{layer_id}")
        
        # æ¨¡æ‹Ÿå¼‚æ­¥é¢„å–
        modules = ["wq", "wk", "wv", "wo", "w1", "w2", "w3"]
        nvtx.range_push(f"prefetch_h2d_layer_{layer_id}")
        for module in modules:
            nvtx.range_push(f"prefetch_h2d_{module}")
            time.sleep(0.003)  # é¢„å–ç¨æ…¢
            nvtx.range_pop()  # prefetch_h2d_module
        nvtx.range_pop()  # prefetch_h2d_layer
        
        nvtx.range_pop()  # prefetch_layer
    
    nvtx.range_pop()  # prefetch_layers

def simulate_cuda_operations():
    """æ¨¡æ‹ŸCUDAè®¡ç®—æ“ä½œ"""
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡æ‹Ÿ")
        return
    
    device = "cuda:0"
    
    # åˆ›å»ºä¸€äº›å¼ é‡è¿›è¡Œè®¡ç®—
    nvtx.range_push("tensor_creation")
    a = torch.randn(2048, 2048, device=device)
    b = torch.randn(2048, 2048, device=device)
    nvtx.range_pop()  # tensor_creation
    
    # æ¨¡æ‹ŸçŸ©é˜µä¹˜æ³•ï¼ˆç±»ä¼¼attentionè®¡ç®—ï¼‰
    nvtx.range_push("attention_simulation")
    c = torch.matmul(a, b)
    torch.cuda.synchronize()
    nvtx.range_pop()  # attention_simulation
    
    # æ¨¡æ‹ŸSoftmax
    nvtx.range_push("softmax_simulation")
    d = torch.softmax(c, dim=-1)
    torch.cuda.synchronize()
    nvtx.range_pop()  # softmax_simulation
    
    return c, d

def simulate_layer_computation(layer_id):
    """æ¨¡æ‹Ÿå•å±‚è®¡ç®—"""
    nvtx.range_push(f"layer_{layer_id}_forward")
    
    # æ³¨æ„åŠ›é˜¶æ®µ
    nvtx.range_push(f"layer_{layer_id}_attention")
    simulate_cuda_operations()
    nvtx.range_pop()  # attention
    
    # FFNé˜¶æ®µ  
    nvtx.range_push(f"layer_{layer_id}_ffn")
    simulate_cuda_operations()
    nvtx.range_pop()  # ffn
    
    nvtx.range_pop()  # layer_forward

def simulate_kv_operations(layer_id):
    """æ¨¡æ‹ŸKV cacheæ“ä½œ"""
    nvtx.range_push(f"layer_{layer_id}_kv_fetch")
    
    if torch.cuda.is_available():
        # æ¨¡æ‹Ÿä»CPUè·å–KV
        device = "cuda:0"
        k_cpu = torch.randn(256, 128, device="cpu")
        v_cpu = torch.randn(256, 128, device="cpu")
        
        nvtx.range_push("kv_h2d_transfer")
        k_gpu = k_cpu.to(device, non_blocking=True)
        v_gpu = v_cpu.to(device, non_blocking=True)
        torch.cuda.synchronize()
        nvtx.range_pop()  # kv_h2d_transfer
    else:
        time.sleep(0.005)  # æ¨¡æ‹ŸKVè·å–æ—¶é—´
    
    nvtx.range_pop()  # kv_fetch

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ¯ NVTXæƒé‡æµå¼ä¼ è¾“æ¼”ç¤º")
    print("=" * 50)
    
    if torch.cuda.is_available():
        print(f"ğŸ”§ ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
    else:
        print("ğŸ”§ ä½¿ç”¨CPUæ¨¡æ‹Ÿ")
    
    nvtx.range_push("weight_streaming_demo")
    
    # æ¨¡æ‹Ÿå¤šå±‚transformerçš„å¤„ç†
    num_layers = 8
    prefetch_distance = 2
    
    for layer_id in range(num_layers):
        print(f"ğŸ”„ å¤„ç†Layer {layer_id}")
        
        # 1. ç¡®ä¿å½“å‰å±‚æƒé‡åœ¨GPU
        modules = ["wq", "wk", "wv", "wo", "w1", "w2", "w3"]
        simulate_weight_loading(layer_id, modules)
        
        # 2. é¢„å–ä¸‹ä¸€å±‚æƒé‡ï¼ˆå¼‚æ­¥ï¼‰
        next_layers = [i for i in range(layer_id + 1, min(layer_id + 1 + prefetch_distance, num_layers))]
        if next_layers:
            simulate_prefetch(next_layers)
        
        # 3. KV cacheæ“ä½œ
        simulate_kv_operations(layer_id)
        
        # 4. å®é™…å±‚è®¡ç®—
        simulate_layer_computation(layer_id)
        
        # æ·»åŠ ä¸€äº›å˜åŒ–ä½¿æ—¶é—´çº¿æ›´æœ‰è¶£
        time.sleep(random.uniform(0.001, 0.005))
    
    nvtx.range_pop()  # weight_streaming_demo
    
    print("âœ… æ¼”ç¤ºå®Œæˆ!")
    print("\nğŸ“Š è¿è¡Œåˆ†æå‘½ä»¤:")
    print("nsys profile --trace=cuda,nvtx --output=demo python test_nvtx_demo.py")
    print("nsys stats --report nvtx_sum demo.nsys-rep")

if __name__ == "__main__":
    main()