#!/usr/bin/env python3
"""
æµ‹è¯•ç»„çº§é¢„å–åŠŸèƒ½çš„åŸºæœ¬é€»è¾‘
åŒ…å«GPU OOMä¿æŠ¤æœºåˆ¶
"""
import sys
import torch
import torch.nn as nn
import gc

# ç®€å•éªŒè¯å¯¼å…¥å’ŒåŸºæœ¬é€»è¾‘
def test_imports():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    try:
        from llama3.weight_streaming_manager import WeightStreamingManager
        print("âœ“ WeightStreamingManager å¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— WeightStreamingManager å¯¼å…¥å¤±è´¥: {e}")
        return False

def cleanup_gpu():
    """æ¸…ç†GPUæ˜¾å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
        torch.cuda.synchronize()

def check_gpu_memory():
    """æ£€æŸ¥GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        free, total = torch.cuda.mem_get_info()
        used = total - free
        print(f"  GPUæ˜¾å­˜: {used/(1024**3):.2f}GB / {total/(1024**3):.2f}GB used")
        return free, total
    return 0, 0

def test_grouped_mode_attributes():
    """æµ‹è¯•ç»„çº§æ¨¡å¼çš„å±æ€§"""
    try:
        from llama3.weight_streaming_manager import WeightStreamingManager
        from llama3.config import ModelArgs

        # åˆ›å»ºæœ€å°æ¨¡å‹é…ç½®
        args = ModelArgs(
            dim=512,
            n_layers=4,
            n_heads=8,
            n_kv_heads=8,  # å¿…éœ€å‚æ•°
            vocab_size=1000,
            multiple_of=256,  # å¿…éœ€å‚æ•°
            ffn_dim_multiplier=None,  # å¿…éœ€å‚æ•°
            norm_eps=1e-5,  # å¿…éœ€å‚æ•°
            rope_theta=10000.0,  # å¿…éœ€å‚æ•°
            use_scaled_rope=False,  # å¿…éœ€å‚æ•°
            max_batch_size=1,
            max_seq_len=128,
            device="cpu"  # ä½¿ç”¨CPUé¿å…CUDAä¾èµ–
        )

        # åˆ›å»ºç®€å•çš„æ¨¡å‹
        class DummyBlock(nn.Module):
            def __init__(self):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(512, 512))

        class DummyModel(nn.Module):
            def __init__(self, n_layers):
                super().__init__()
                self.layers = nn.ModuleList([DummyBlock() for _ in range(n_layers)])

        model = DummyModel(args.n_layers)

        # åˆ›å»ºWSMå®ä¾‹ï¼ˆä¸å¯ç”¨SSDï¼‰
        wsm = WeightStreamingManager(
            model,
            device="cpu",
            prefetch_distance=1,
            max_cached_layers=2,
            ssd_manifest_path=None,  # ä¸ä½¿ç”¨SSD
            verbose=True
        )

        # æ£€æŸ¥å…³é”®å±æ€§
        checks = {
            "grouped_mode": hasattr(wsm, "grouped_mode"),
            "n_layers": hasattr(wsm, "n_layers"),
            "name_to_param": hasattr(wsm, "name_to_param"),
            "_layer_prefetch_distance": hasattr(wsm, "_layer_prefetch_distance"),
            "_group_prefetch_distance": hasattr(wsm, "_group_prefetch_distance"),
            "gpu_max_groups": hasattr(wsm, "gpu_max_groups"),
            "_gpu_group_lru": hasattr(wsm, "_gpu_group_lru"),
        }

        print("\nå±æ€§æ£€æŸ¥:")
        all_passed = True
        for attr, exists in checks.items():
            status = "âœ“" if exists else "âœ—"
            print(f"  {status} {attr}: {exists}")
            if not exists:
                all_passed = False

        # æ£€æŸ¥å±æ€§å€¼
        if all_passed:
            print(f"\nå±æ€§å€¼:")
            print(f"  grouped_mode: {wsm.grouped_mode}")
            print(f"  n_layers: {wsm.n_layers}")
            print(f"  name_to_param entries: {len(wsm.name_to_param)}")
            print(f"  _layer_prefetch_distance: {wsm._layer_prefetch_distance}")
            print(f"  _group_prefetch_distance: {wsm._group_prefetch_distance}")
            print(f"  gpu_max_groups: {wsm.gpu_max_groups}")

        # æ¸…ç†
        del wsm, model
        cleanup_gpu()

        return all_passed

    except Exception as e:
        print(f"âœ— å±æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        cleanup_gpu()
        return False

def test_helper_methods():
    """æµ‹è¯•è¾…åŠ©æ–¹æ³•æ˜¯å¦å­˜åœ¨"""
    try:
        from llama3.weight_streaming_manager import WeightStreamingManager

        methods = [
            "_wait_cpu_ready",
            "_load_param_from_ssd",
            "_bg_submit",
            "_cpu_layer_ready",
            "_evict_cpu_layers_older_than",
            "ensure_group_on_gpu",
            "prefetch_group_async",
        ]

        print("\næ–¹æ³•æ£€æŸ¥:")
        all_exist = True
        for method_name in methods:
            exists = hasattr(WeightStreamingManager, method_name)
            status = "âœ“" if exists else "âœ—"
            print(f"  {status} {method_name}: {exists}")
            if not exists:
                all_exist = False

        return all_exist

    except Exception as e:
        print(f"âœ— æ–¹æ³•æ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_hook_integration():
    """æµ‹è¯•hooké›†æˆ"""
    try:
        from llama3.weight_streaming_manager import WeightStreamingManager
        from llama3.config import ModelArgs

        # åˆ›å»ºæœ€å°æ¨¡å‹
        class DummyBlock(nn.Module):
            def __init__(self):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(512, 512))

        class DummyModel(nn.Module):
            def __init__(self, n_layers):
                super().__init__()
                self.layers = nn.ModuleList([DummyBlock() for _ in range(n_layers)])

        args = ModelArgs(
            dim=512,
            n_layers=4,
            n_heads=8,
            n_kv_heads=8,
            vocab_size=1000,
            multiple_of=256,
            ffn_dim_multiplier=None,
            norm_eps=1e-5,
            rope_theta=10000.0,
            use_scaled_rope=False,
            max_batch_size=1,
            max_seq_len=128,
            device="cpu"
        )

        model = DummyModel(args.n_layers)
        wsm = WeightStreamingManager(
            model,
            device="cpu",
            prefetch_distance=1,
            max_cached_layers=2,
            ssd_manifest_path=None,
            verbose=False
        )

        # æ£€æŸ¥hookæ˜¯å¦è¢«æ­£ç¡®å®‰è£…
        hooks_installed = True
        for i, block in enumerate(wsm.blocks):
            if not hasattr(block, "_forward_pre_hooks") or len(block._forward_pre_hooks) == 0:
                print(f"  âœ— Layer {i}: No pre-hooks installed")
                hooks_installed = False
            else:
                print(f"  âœ“ Layer {i}: {len(block._forward_pre_hooks)} pre-hook(s) installed")

        # æ¸…ç†
        del wsm, model
        cleanup_gpu()

        return hooks_installed

    except Exception as e:
        print(f"âœ— Hooké›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        cleanup_gpu()
        return False

def test_gpu_oom_protection():
    """æµ‹è¯•GPU OOMä¿æŠ¤æœºåˆ¶"""
    if not torch.cuda.is_available():
        print("  âŠ˜ GPUä¸å¯ç”¨ï¼Œè·³è¿‡OOMä¿æŠ¤æµ‹è¯•")
        return True

    try:
        from llama3.weight_streaming_manager import WeightStreamingManager
        from llama3.config import ModelArgs

        print("\nåˆå§‹GPUçŠ¶æ€:")
        check_gpu_memory()

        # åˆ›å»ºä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹æµ‹è¯•OOMä¿æŠ¤
        class DummyBlock(nn.Module):
            def __init__(self, size=256):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(size, size))

        class DummyModel(nn.Module):
            def __init__(self, n_layers, size=256):
                super().__init__()
                self.layers = nn.ModuleList([DummyBlock(size) for _ in range(n_layers)])

        args = ModelArgs(
            dim=256,
            n_layers=8,
            n_heads=4,
            n_kv_heads=4,
            vocab_size=1000,
            multiple_of=256,
            ffn_dim_multiplier=None,
            norm_eps=1e-5,
            rope_theta=10000.0,
            use_scaled_rope=False,
            max_batch_size=1,
            max_seq_len=128,
            device="cuda:0"
        )

        try:
            # æµ‹è¯•_ensure_gpu_roomæ–¹æ³•
            model = DummyModel(args.n_layers, size=256)
            wsm = WeightStreamingManager(
                model,
                device="cuda:0",
                prefetch_distance=1,
                max_cached_layers=3,
                ssd_manifest_path=None,
                verbose=True
            )

            print("\næµ‹è¯•GPUç©ºé—´æ£€æŸ¥:")
            if hasattr(wsm, '_ensure_gpu_room'):
                # æµ‹è¯•å°é‡åˆ†é…
                need_bytes = 1024 * 1024  # 1MB
                wsm._ensure_gpu_room(need_bytes)
                print(f"  âœ“ å°é‡åˆ†é…æµ‹è¯•é€šè¿‡ ({need_bytes/(1024**2):.1f}MB)")

                # æ£€æŸ¥æ˜¾å­˜çŠ¶æ€
                check_gpu_memory()
            else:
                print("  âš  _ensure_gpu_room æ–¹æ³•ä¸å­˜åœ¨")

            # æµ‹è¯•LRUæ·˜æ±°
            print("\næµ‹è¯•LRUæ·˜æ±°æœºåˆ¶:")
            if hasattr(wsm, '_evict_one_group_from_gpu'):
                # æ¨¡æ‹Ÿæ·»åŠ ä¸€äº›ç»„åˆ°LRU
                wsm._gpu_group_lru = [(0, 'attn'), (1, 'attn'), (2, 'attn')]
                print(f"  æ·»åŠ æµ‹è¯•æ•°æ®åˆ°LRU: {wsm._gpu_group_lru}")

                # å°è¯•æ·˜æ±°
                result = wsm._evict_one_group_from_gpu(exclude=set())
                if result:
                    print(f"  âœ“ æˆåŠŸæ·˜æ±°ä¸€ä¸ªç»„")
                    print(f"  LRUçŠ¶æ€: {wsm._gpu_group_lru}")
                else:
                    print(f"  âš  æ·˜æ±°å¤±è´¥")
            else:
                print("  âš  _evict_one_group_from_gpu æ–¹æ³•ä¸å­˜åœ¨")

            # æ¸…ç†
            del wsm, model
            cleanup_gpu()

            print("\næ¸…ç†åGPUçŠ¶æ€:")
            check_gpu_memory()

            return True

        except torch.cuda.OutOfMemoryError as e:
            print(f"  âš  GPU OOMå¼‚å¸¸è¢«æ­£ç¡®æ•è·: {e}")
            cleanup_gpu()
            return True  # OOMè¢«æ•è·ä¹Ÿç®—é€šè¿‡

    except Exception as e:
        print(f"âœ— GPU OOMä¿æŠ¤æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        cleanup_gpu()
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 60)
    print("ç»„çº§é¢„å–åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)

    # åˆå§‹æ¸…ç†
    cleanup_gpu()

    tests = [
        ("æ¨¡å—å¯¼å…¥", test_imports),
        ("ç»„çº§æ¨¡å¼å±æ€§", test_grouped_mode_attributes),
        ("è¾…åŠ©æ–¹æ³•", test_helper_methods),
        ("Hooké›†æˆ", test_hook_integration),
        ("GPU OOMä¿æŠ¤", test_gpu_oom_protection),
    ]

    results = []
    for test_name, test_func in tests:
        print(f"\næµ‹è¯•: {test_name}")
        print("-" * 60)
        try:
            passed = test_func()
            results.append((test_name, passed))
        except Exception as e:
            print(f"âœ— æµ‹è¯•å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
        finally:
            # æ¯ä¸ªæµ‹è¯•åæ¸…ç†
            cleanup_gpu()

    # æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)

    total = len(results)
    passed = sum(1 for _, p in results if p)

    for test_name, passed_flag in results:
        status = "âœ“ é€šè¿‡" if passed_flag else "âœ— å¤±è´¥"
        print(f"  {status}: {test_name}")

    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")

    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç»„çº§é¢„å–åŠŸèƒ½å·²æ­£ç¡®é›†æˆã€‚")
        return 0
    else:
        print(f"\nâš ï¸  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤ã€‚")
        return 1

if __name__ == "__main__":
    sys.exit(main())
