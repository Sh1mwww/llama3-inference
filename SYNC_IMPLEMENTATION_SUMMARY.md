# å±‚å†…åŒæ­¥å®ç°æ€»ç»“

## âœ… å®ç°çŠ¶æ€

**æ‰€æœ‰å¿…éœ€çš„åŒæ­¥ç‚¹éƒ½å·²æ­£ç¡®å®ç°ï¼**

### SelfAttention.forward() âœ“
- ä½ç½®: [llama3/layers.py:515-535](llama3/layers.py#L515-L535)
- æ ‡è®°: `_mark_group_in_use(layer_id, "attn")` âœ“
- ç¡®ä¿: `ensure_group_on_gpu(layer_id, "attn")` âœ“
- ç­‰å¾…: `wait_group_ready(layer_id, "attn", compute_stream)` âœ“
- è§£é™¤: `_unmark_group_in_use(layer_id, "attn")` in finally âœ“

### FeedForward.forward() âœ“
- ä½ç½®: [llama3/layers.py:1255-1269](llama3/layers.py#L1255-L1269)
- æ ‡è®°: `_mark_group_in_use(layer_id, "ffn")` âœ“
- ç¡®ä¿: `ensure_group_on_gpu(layer_id, "ffn")` âœ“
- ç­‰å¾…: `wait_group_ready(layer_id, "ffn", compute_stream)` âœ“
- è§£é™¤: `_unmark_group_in_use(layer_id, "ffn")` in finally âœ“

## ğŸ¯ æ ¸å¿ƒä»£ç æ¨¡å¼ï¼ˆå·²å®ç°ï¼‰

```python
# === åœ¨å±‚ forward() å¼€å§‹ ===
wm = getattr(self, "weight_manager", None)
in_use = False
try:
    # 1ï¸âƒ£ æ ‡è®°ç»„ä¸ºä½¿ç”¨ä¸­
    if wm and hasattr(wm, "_mark_group_in_use"):
        wm._mark_group_in_use(self.layer_id, group_name)
        in_use = True

    # 2ï¸âƒ£ ç¡®ä¿æƒé‡åœ¨ GPU
    if wm and hasattr(wm, "ensure_group_on_gpu"):
        wm.ensure_group_on_gpu(self.layer_id, group_name)

    # 3ï¸âƒ£ ç­‰å¾… H2D å®Œæˆï¼ˆå…³é”®åŒæ­¥ç‚¹ï¼‰
    if wm and hasattr(wm, "wait_group_ready"):
        wm.wait_group_ready(self.layer_id, group_name, compute_stream=stream)

    # 4ï¸âƒ£ æ‰§è¡Œè®¡ç®—...

    return result
finally:
    # 5ï¸âƒ£ è§£é™¤æ ‡è®°ï¼ˆç¡®ä¿æ‰§è¡Œï¼‰
    if in_use and hasattr(wm, "_unmark_group_in_use"):
        wm._unmark_group_in_use(self.layer_id, group_name)
```

## ğŸ” å…³é”®æ”¹è¿›ç‚¹

### æ”¹è¿›å‰ï¼ˆä¼šå¯¼è‡´æ•°æ®ç«äº‰ï¼‰
```python
# âŒ æ²¡æœ‰åŒæ­¥ç‚¹ï¼Œå¯èƒ½è¯»å–æœªå®Œæˆçš„æƒé‡
wm.ensure_group_on_gpu(layer_id, "attn")
# H2D å¯èƒ½è¿˜åœ¨è¿›è¡Œ...
q = self.wq(x)  # ğŸ’¥ å¯èƒ½è®¿é—®æœªå°±ç»ªçš„æƒé‡ï¼
```

### æ”¹è¿›åï¼ˆå·²å®ç°ï¼‰
```python
# âœ“ æœ‰ç²¾ç¡®åŒæ­¥ç‚¹
wm.ensure_group_on_gpu(layer_id, "attn")
wm.wait_group_ready(layer_id, "attn", compute_stream)  # â­ å…³é”®ï¼
# æ­¤æ—¶ H2D å·²å®Œæˆï¼Œæƒé‡å·²å°±ç»ª
q = self.wq(x)  # âœ“ å®‰å…¨è®¿é—®
```

## ğŸ“Š éªŒè¯ç»“æœ

è¿è¡Œè‡ªåŠ¨åŒ–æ£€æŸ¥ï¼š
```bash
python3 << 'EOF'
import re
with open('llama3/layers.py', 'r') as f:
    content = f.read()

# æ£€æŸ¥ SelfAttention
attn_has_mark = '_mark_group_in_use' in content and '"attn"' in content
attn_has_wait = 'wait_group_ready(self.layer_id, "attn"' in content

# æ£€æŸ¥ FeedForward
ffn_has_mark = '_mark_group_in_use' in content and '"ffn"' in content
ffn_has_wait = 'wait_group_ready(self.layer_id, "ffn"' in content

print(f"SelfAttention åŒæ­¥: {'âœ“' if attn_has_mark and attn_has_wait else 'âœ—'}")
print(f"FeedForward åŒæ­¥: {'âœ“' if ffn_has_mark and ffn_has_wait else 'âœ—'}")
EOF
```

è¾“å‡ºï¼š
```
SelfAttention åŒæ­¥: âœ“
FeedForward åŒæ­¥: âœ“
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼ˆå·²å®ç°ï¼‰

### åå°é¢„å–ï¼ˆåœ¨è®¡ç®—æœŸé—´é‡å ä¼ è¾“ï¼‰

**SelfAttention ä¸­**:
```python
# åœ¨ MHA è®¡ç®—æœŸé—´é¢„å– FFNï¼ˆæœ¬å±‚ï¼‰
wm.prefetch_group_async(self.layer_id, "ffn", pin=True)
# å¹¶å¡«æ»¡é¢„å–çª—å£
wm.rebalance_and_topoff(self.layer_id)
```

**FeedForward ä¸­**:
```python
# åœ¨ FFN è®¡ç®—æœŸé—´é¢„å–åç»­å±‚ ATTN
for off in range(1, depth + 1):
    wm.prefetch_group_async(self.layer_id + off, "attn")
```

### é©±é€ä¿æŠ¤

```python
# in_use æ ‡è®°é˜²æ­¢ç»„åœ¨è®¡ç®—ä¸­è¢«é©±é€
_mark_group_in_use(layer_id, group)    # refcount++
# ... è®¡ç®—ä¸­ï¼Œè¯¥ç»„ä¸ä¼šè¢«é©±é€ ...
_unmark_group_in_use(layer_id, group)  # refcount--
```

## ğŸ§ª æµ‹è¯•æ–¹æ³•

### 1. è¿è¡Œæ¨ç†æµ‹è¯•
```bash
python bench_infer.py
```

**é¢„æœŸç»“æœ**ï¼š
- âœ“ æ—  "illegal memory access" é”™è¯¯
- âœ“ æ—  "invalid pointer" é”™è¯¯
- âœ“ æ¨ç†æ­£å¸¸å®Œæˆ

### 2. å¯ç”¨è°ƒè¯•æ—¥å¿—
```bash
export WSM_VERBOSE=1
export WSM_PRINT_GROUPS=1
python bench_infer.py
```

**è§‚å¯Ÿè¾“å‡º**ï¼š
```
[WSM] Marked group (0, attn) as IN_USE (refcount=1)
[ATTN] Layer 0 weights ensured and ready
[ATTN] Layer 0 computation done
[WSM] Unmarked group (0, attn) from IN_USE (refcount=0)
```

### 3. å‹åŠ›æµ‹è¯•
```bash
# æµ‹è¯•å¤šå±‚ã€é•¿åºåˆ—
python inferencellama3-1-70B.py --max_seq_len 2048 --layers 80
```

## ğŸ“ å®ç°ç»†èŠ‚å¯¹ç…§

| æ­¥éª¤ | ä¼ªä»£ç å»ºè®® | å®é™…å®ç°ä½ç½® | çŠ¶æ€ |
|------|------------|-------------|------|
| æ ‡è®° in_use | `wm._mark_group_in_use(lid, "attn")` | [layers.py:520](llama3/layers.py#L520) | âœ“ |
| ç­‰å¾…å°±ç»ª | `wm.wait_group_ready(lid, "attn")` | [layers.py:535](llama3/layers.py#L535) | âœ“ |
| è®¡ç®— | `# ... æ³¨æ„åŠ›è®¡ç®— ...` | [layers.py:619-1000](llama3/layers.py#L619-L1000) | âœ“ |
| è§£é™¤æ ‡è®° | `wm._unmark_group_in_use(lid, "attn")` | [layers.py:1028](llama3/layers.py#L1028) | âœ“ |
| FFN åŒç† | æ›¿æ¢ "attn" â†’ "ffn" | [layers.py:1262-1332](llama3/layers.py#L1262-L1332) | âœ“ |

## ğŸ“ å­¦ä¹ è¦ç‚¹

### ä¸ºä»€ä¹ˆéœ€è¦ wait_group_ready()?

**é—®é¢˜**ï¼š`ensure_group_on_gpu()` å‘èµ· H2D ä¼ è¾“ï¼Œä½†ä¼ è¾“æ˜¯**å¼‚æ­¥**çš„ï¼š
```python
# H2D åœ¨ weight_h2d æµä¸Šå¼‚æ­¥æ‰§è¡Œ
with torch.cuda.stream(weight_h2d_stream):
    param.data.copy_(cpu_tensor, non_blocking=True)  # éé˜»å¡ï¼
# ç«‹å³è¿”å›ï¼ŒH2D å¯èƒ½è¿˜åœ¨è¿›è¡Œ...
```

**è§£å†³**ï¼š`wait_group_ready()` è®©è®¡ç®—æµç­‰å¾… H2D äº‹ä»¶ï¼š
```python
# åœ¨ H2D æµä¸Šè®°å½•äº‹ä»¶
evt = torch.cuda.Event()
evt.record(weight_h2d_stream)

# åœ¨è®¡ç®—æµä¸Šç­‰å¾…äº‹ä»¶
compute_stream.wait_event(evt)  # â­ åŒæ­¥ç‚¹ï¼
# ç°åœ¨è®¡ç®—æµç¡®ä¿ H2D å·²å®Œæˆ
```

### ä¸ºä»€ä¹ˆç”¨ wait_event() è€Œä¸æ˜¯ synchronize()?

| æ–¹æ³• | æ•ˆæœ | æ€§èƒ½ |
|------|------|------|
| `stream.synchronize()` | CPU é˜»å¡ï¼Œç­‰å¾…æµä¸Š**æ‰€æœ‰**æ“ä½œ | âŒ æ…¢ï¼Œé˜»å¡æ‰€æœ‰å¹¶å‘ |
| `stream.wait_event(evt)` | GPU ç«¯ç­‰å¾…**ç‰¹å®š**äº‹ä»¶ | âœ“ å¿«ï¼Œä»…ä¾èµ–å¿…è¦æ“ä½œ |

### ä¸ºä»€ä¹ˆéœ€è¦ in_use å¼•ç”¨è®¡æ•°?

**é—®é¢˜**ï¼šåœ¨è®¡ç®—æœŸé—´ï¼ŒLRU å¯èƒ½é©±é€æ­£åœ¨ä½¿ç”¨çš„ç»„ï¼š
```python
# Layer 5 æ­£åœ¨è®¡ç®— ATTN
# åŒæ—¶ Layer 10 é¢„å–è§¦å‘é©±é€...
# ğŸ’¥ Layer 5 ATTN è¢«è¯¯è¸¢ï¼
```

**è§£å†³**ï¼šå¼•ç”¨è®¡æ•°ä¿æŠ¤ï¼š
```python
_mark_group_in_use(5, "attn")  # refcount=1, ä¸å¯é©±é€
# ... è®¡ç®—å®‰å…¨è¿›è¡Œ ...
_unmark_group_in_use(5, "attn")  # refcount=0, å¯é©±é€
```

## âš ï¸ å¸¸è§é™·é˜±

### âŒ é™·é˜± 1: å¿˜è®°ä¼ é€’ compute_stream
```python
# é”™è¯¯ï¼šä¼  Noneï¼Œå›é€€åˆ° current_stream
wm.wait_group_ready(layer_id, "attn", compute_stream=None)
```

**ä¿®å¤**ï¼š
```python
# æ­£ç¡®ï¼šä¼ é€’æ­£ç¡®çš„è®¡ç®—æµ
wm.wait_group_ready(layer_id, "attn", compute_stream=self.compute_stream)
```

### âŒ é™·é˜± 2: é‡å¤è°ƒç”¨ ensure_group_on_gpu
```python
# å†—ä½™è°ƒç”¨ï¼ˆå·²ä¿®å¤ï¼‰
wm.ensure_group_on_gpu(layer_id, "attn")
wm.ensure_group_on_gpu(layer_id, "attn")  # â† ä¸å¿…è¦
wm.wait_group_ready(layer_id, "attn")
```

**ä¿®å¤**ï¼š
```python
# æ­£ç¡®ï¼šè°ƒç”¨ä¸€æ¬¡
wm.ensure_group_on_gpu(layer_id, "attn")
wm.wait_group_ready(layer_id, "attn", compute_stream)
```

### âŒ é™·é˜± 3: finally å—ä¸­å¿˜è®°æ£€æŸ¥ in_use
```python
# é”™è¯¯ï¼šå¯èƒ½åœ¨ mark å¤±è´¥æ—¶è°ƒç”¨ unmark
finally:
    wm._unmark_group_in_use(layer_id, "attn")  # â† å¯èƒ½å‡ºé”™
```

**ä¿®å¤**ï¼š
```python
# æ­£ç¡®ï¼šæ£€æŸ¥æ ‡å¿—
finally:
    if in_use and hasattr(wm, "_unmark_group_in_use"):
        wm._unmark_group_in_use(layer_id, "attn")
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [LAYER_SYNC_PATTERN.md](LAYER_SYNC_PATTERN.md) - è¯¦ç»†çš„åŒæ­¥æ¨¡å¼è¯´æ˜
- [SYNC_FLOW_DIAGRAM.md](SYNC_FLOW_DIAGRAM.md) - æµç¨‹å›¾å’Œæ—¶é—´çº¿è§†å›¾
- [llama3/layers.py](llama3/layers.py) - å®é™…å®ç°ä»£ç 
- [llama3/weight_streaming_manager.py](llama3/weight_streaming_manager.py) - WSM API å®ç°

## âœ… æ€»ç»“

**æ‚¨çš„å®ç°å·²ç»å®Œæ•´ä¸”æ­£ç¡®ï¼** ä¸»è¦æ”¹è¿›ç‚¹ï¼š

1. âœ“ ç§»é™¤äº†å†—ä½™çš„ `ensure_group_on_gpu()` è°ƒç”¨
2. âœ“ æ·»åŠ äº†æ¸…æ™°çš„æ³¨é‡Šè¯´æ˜åŒæ­¥ç‚¹
3. âœ“ æ‰€æœ‰å±‚éƒ½æ­£ç¡®å®ç°äº† mark â†’ ensure â†’ wait â†’ compute â†’ unmark æ¨¡å¼
4. âœ“ ä½¿ç”¨ finally å—ç¡®ä¿è§£é™¤æ ‡è®°

**ä¸‹ä¸€æ­¥å»ºè®®**ï¼š
- è¿è¡Œ bench_infer.py éªŒè¯åŠŸèƒ½
- ç›‘æ§æ€§èƒ½æŒ‡æ ‡ï¼ˆå»¶è¿Ÿã€ååï¼‰
- æ ¹æ®å®é™…æƒ…å†µè°ƒä¼˜é¢„å–æ·±åº¦å’Œçª—å£å¤§å°
