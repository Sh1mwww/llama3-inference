#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Llama3.1-70B æ¨ç† + è½»é‡çº§ Profilerï¼ˆJSON/CSV è‡ªåŠ¨å†™å…¥å›ºå®šç›®å½•ï¼‰
- è®°å½•ä¼šå½±å“ inference çš„å…³é”®è·¯å¾„ç”¨æ—¶ï¼ˆprefill / decode per-token / e2e / FTL è¿‘ä¼¼ / ååï¼‰
- åŒæ—¶è®°å½•ä¸ä¼šå½±å“ inference çš„å‡†å¤‡/æ¢é’ˆ/æ—¥å¿—æ—¶é—´ï¼ˆnon_inference ç±»åˆ«ï¼‰
- å¯¹ WSM ä¸¤ä¸ªå…³é”®å‡½æ•°ï¼ˆwait_group_ready / _ensure_module_on_gpuï¼‰åšåŸ‹ç‚¹ç»Ÿè®¡
- é‡‡ç”¨ CUDA Events é€ token è®¡æ—¶ï¼Œç»Ÿä¸€åŒæ­¥ï¼Œå°½é‡ä½æ‰°åŠ¨
- ç”Ÿæˆç»“æœå›ºå®šå†™å…¥ LOG_DIRï¼Œè‡ªåŠ¨è¾“å‡º JSON + CSV ä¸¤ç§æ ¼å¼
"""

import os
from pathlib import Path
import types
from typing import Any, Dict, List, Optional
import json, csv, uuid, platform, math, time, re
from datetime import datetime, timezone
from contextlib import contextmanager, nullcontext

# ğŸ”¥ CUDA å†…å­˜åˆ†é…å™¨é…ç½®ï¼ˆå¿…é¡»åœ¨ import torch ä¹‹å‰ï¼‰
# expandable_segments ä¸å¼‚æ­¥æµæ“ä½œå¯èƒ½æœ‰å†²çªï¼Œæš‚æ—¶ç¦ç”¨
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"  # é™åˆ¶åˆ†å—å¤§å°ï¼Œå‡å°‘ç¢ç‰‡

# ğŸ”¥ WSM æ— å…œåº•ç­–ç•¥ï¼šé”å®šäº‹ä»¶é©±åŠ¨è°ƒåº¦ï¼Œç¦ç”¨åŒæ­¥å…œåº• (no-fallback)
os.environ["WSM_NO_FALLBACK"] = "1"

# ğŸ”¥ å¯ç”¨å±‚çº§æ€§èƒ½ profilingï¼ˆCUDA timer ç»Ÿè®¡ attn/ffn/kv_fetch ç­‰è¯¦ç»†æ—¶é—´ï¼‰
os.environ["LLM_PROFILE"] = "1"
CHUNK_SIZE = int(os.environ.setdefault("PREFILL_T_CHUNK", "256"))
MIRCO_BATCH_SIZE  = os.environ.setdefault("MIRCO_BATCH_SIZE", "4")
ATTN_MICRO_B = os.environ.setdefault("ATTN_MICRO_B", "4")
import torch

# Optional NVTX for GPU decode-step ranges
try:
    import torch.cuda.nvtx as nvtx
    NVTX_AVAILABLE = True
except Exception:
    nvtx = None
    NVTX_AVAILABLE = False

# ===== ä½ å¯ä»¥æ”¹è¿™é‡Œï¼šæ—¥å¿—è¾“å‡ºç›®å½• & è¿è¡Œæ ‡ç­¾ï¼ˆå¯ç•™ç©ºï¼‰ =====
LOG_DIR = Path("/home/roger/logs")   # è‡ªåŠ¨åˆ›å»º
RUN_TAG = ""                         # ä¾‹å¦‚ "ablation-a1"ï¼›ç•™ç©ºåˆ™è‡ªåŠ¨ä»…ç”¨ run_id

# ===== é¡¹ç›®å†…æ¨¡å— =====
from llama3.generator import LLaMA
from llama3.config import KVCacheArgs, load_runtime_config, runtime_config_to_dict
from llama3 import generator as _gen, stream_mnt
try:
    from llama3.layers import PERF_TRACKER  # æ–°å¢ï¼šä» layers.py æ‹¿åˆ°å…¨å±€çš„æ€§èƒ½ç»Ÿè®¡å™¨
except Exception:
    PERF_TRACKER = None

# ========== build() åŒ…è£…ï¼šåªåŠ æ—¥å¿—ï¼Œä¸æ”¹åº“ ==========
_orig_build = _gen.LLaMA.build

def _debug_build(*args, **kw):
    mode       = kw.get("mode", None)
    load_model = kw.get("load_model", None)
    mode_cfg   = (kw.get("mode_config", {}) or {})
    raw_dev    = mode_cfg.get("raw_device")
    manifest   = mode_cfg.get("manifest_path") or mode_cfg.get("ssd_manifest_path")

    print(f"[MODE-DECISION] LLaMA.build(mode={mode}, load_model={load_model})")
    use_raw_ssd = (mode in {"ssd", "mixed"}) or (mode_cfg.get("weight_source") == "raw-ssd")
    print(f"[MODE-DECISION] use_raw_ssd={use_raw_ssd} raw_device={raw_dev} manifest={manifest}")

    llama = _orig_build(*args, **kw)

    has_wsm = hasattr(llama, "weight_streaming_manager")
    if has_wsm:
        wsm = llama.weight_streaming_manager
        ssd = bool(getattr(wsm, "ssd_enabled", False) or getattr(wsm, "ssd", None))
        print(f"[MODE-DECISION] built: WSM present, ssd_enabled={ssd}")
    else:
        print("[MODE-DECISION] built: NO WSM (å¯èƒ½æ˜¯ full-cpu/full-gpu/æ—§ streaming)")
    return llama

_gen.LLaMA.build = staticmethod(_debug_build)

# ======= è½»é‡çº§ Profilerï¼ˆä½æ‰°åŠ¨ï¼›CUDA Eventsï¼›ä¿å­˜ JSON/CSVï¼‰ =======
PROFILER = None  # å…¨å±€å¥æŸ„

def _now_utc():
    return datetime.now(timezone.utc).isoformat()

def _flatten_extras(extras: dict):
    out = {}
    for k,v in (extras or {}).items():
        out[k] = v if (isinstance(v,(int,float,str,bool)) or v is None) else str(v)
    return out

class InferenceProfiler:
    def __init__(self, run_name: str | None = None):
        self.run_id   = run_name or f"run-{uuid.uuid4().hex[:8]}"
        self.t0_ns    = time.perf_counter_ns()
        self.timeline = []   # å¢™é’Ÿé˜¶æ®µ
        self.active   = False
        self.cuda     = torch.cuda.is_available()
        self.forward_events = []      # GPUï¼š[(kind,batch,seqlen,start_ev,end_ev)]
        self.forward_events_cpu = []  # CPU å›é€€ï¼š[(kind,batch,seqlen,dt_ms)]
        self.bookkeep  = {}
        
        # æ–°å¢ï¼šdecode step è®¡æ•° + æ˜¯å¦å¯ç”¨ NVTX
        self.decode_step_idx = 0
        self.use_nvtx = bool(self.cuda and NVTX_AVAILABLE)

        # ä¾› finalize åˆå¹¶çš„ WSM è¿è¡ŒæœŸç»Ÿè®¡ï¼ˆç”± main() å†™å…¥ï¼‰
        self.wsm_runtime = None
        
        self.meta      = {
            "started_at_utc": _now_utc(),
            "python": platform.python_version(),
            "torch": getattr(torch, "__version__", "unknown"),
            "device": ("cuda" if self.cuda else "cpu"),
        }
        if self.cuda:
            try:
                self.meta["cuda_device_name"] = torch.cuda.get_device_name(0)
                self.meta["cuda_cc"] = ".".join(map(str, torch.cuda.get_device_capability(0)))
            except Exception:
                pass

    @contextmanager
    def span(self, name: str, category: str, **extras):
        s = time.perf_counter_ns()
        try:
            yield
        finally:
            e = time.perf_counter_ns()
            rec = {
                "name": name, "cat": category,
                "t_start_ms": (s - self.t0_ns) / 1e6,
                "t_end_ms":   (e - self.t0_ns) / 1e6,
                "dur_ms":     (e - s) / 1e6,
            }
            rec.update(_flatten_extras(extras))
            self.timeline.append(rec)
            if name == "inference_e2e":
                self.bookkeep["inference_s_ns"] = s
                self.bookkeep["inference_e_ns"] = e

    @contextmanager
    def inference_scope(self):
        self.active = True
        with self.span("inference_e2e", "inference"):
            yield
        self.active = False
    
    def now_ms(self) -> float:
        """å½“å‰ç›¸å¯¹ t0 çš„å¢™é’Ÿæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œä¾›å¤–éƒ¨è¡¥ä¸ä½¿ç”¨ã€‚"""
        return (time.perf_counter_ns() - self.t0_ns) / 1e6

    def wrap_model_forward(self, model):
        orig = model.forward

        def _classify_args(args, kwargs):
            """
            è§£æå‡º tokens / B / T / start_pos
            è¿”å›: (kind, B, T, start_pos) å…¶ä¸­ kind: "prefill" or "decode"
            """
            # 1) æ‰¾åˆ° tokens tensor
            cand = None
            for k in ("tokens", "input_ids"):
                t = kwargs.get(k, None)
                if torch.is_tensor(t) and t.dim() == 2:
                    cand = t; break
            if cand is None:
                for a in args:
                    if torch.is_tensor(a) and a.dtype in (torch.long, torch.int32, torch.int64) and a.dim() == 2:
                        cand = a; break
            if cand is None:
                return "unknown", None, None, None

            B, T = int(cand.size(0)), int(cand.size(1))

            # 2) å°è¯•è§£æ start_pos
            start_pos = kwargs.get("start_pos", None)
            if start_pos is None and len(args) >= 2:
                # å¯èƒ½æ˜¯ä½ç½®å‚æ•°ï¼šforward(tokens, start_pos, ...)
                if isinstance(args[1], int):
                    start_pos = args[1]

            # 3) åˆ¤æ–­ kind
            kind = None
            if T is not None and T > 1:
                # ä¸ç®¡ start_pos æ˜¯ 0 è¿˜æ˜¯ >0ï¼Œéƒ½å½“ prefillï¼ˆåŒ…å« chunk prefillï¼‰
                kind = "prefill"
            elif T == 1:
                # å• tokenï¼Œä¸€å¾‹è®¤ä¸ºæ˜¯ decode
                kind = "decode"
            elif start_pos is not None and start_pos > 0:
                # T å–ä¸åˆ°ã€ä½† start_pos>0 æ—¶å†å…œåº•å½“ decode
                kind = "decode"
            else:
                kind = "unknown"

            return kind, B, T, start_pos

        def wrapped(*args, **kwargs):
            if not self.active:
                return orig(*args, **kwargs)

            kind, B, T, start_pos = _classify_args(args, kwargs)

            # ---- ç»Ÿä¸€ç»´æŠ¤ decode_step_idxï¼ˆä¸ä¾èµ– NVTX æ˜¯å¦å¼€å¯ï¼‰----
            step_idx = None
            if kind == "decode":
                step_idx = self.decode_step_idx
                self.decode_step_idx += 1

            # åœ¨è¿›å…¥ forward å‰ï¼Œå‘Šè¯‰ PERF_TRACKER å½“å‰ phase + step
            if PERF_TRACKER is not None:
                # prefill ä¸éœ€è¦è®°å½• stepï¼Œå°±ç»™ Noneï¼›decode ç»™å…·ä½“ step_idx
                PERF_TRACKER.set_step_context(kind, step_idx if kind == "decode" else None)

            if self.cuda:
                s_ev = torch.cuda.Event(enable_timing=True)
                e_ev = torch.cuda.Event(enable_timing=True)

                label = None
                if self.use_nvtx and NVTX_AVAILABLE and kind == "decode":
                    # è¿™é‡Œç›´æ¥ç”¨ä¸Šé¢ç®—å¥½çš„ step_idx
                    label = f"decode_step_{step_idx}_B{B}_T{T}"
                    nvtx.range_push(label)

                s_ev.record()
                try:
                    out = orig(*args, **kwargs)
                finally:
                    e_ev.record()
                    if NVTX_AVAILABLE and label is not None:
                        nvtx.range_pop()
                    # é€€å‡º forward åï¼Œæ¸…æ‰ step context
                    if PERF_TRACKER is not None:
                        PERF_TRACKER.set_step_context(None, None)

                self.forward_events.append((kind, B, T, s_ev, e_ev))
                return out
            else:
                t0 = time.time()
                try:
                    out = orig(*args, **kwargs)
                finally:
                    t1 = time.time()
                    # CPU ç‰ˆæœ¬ä¹Ÿç»“æŸæ—¶æ¸… context
                    if PERF_TRACKER is not None:
                        PERF_TRACKER.set_step_context(None, None)

                self.forward_events_cpu.append((kind, B, T, (t1 - t0) * 1000.0))
                return out

        model.forward = wrapped


    # ä¾› WSM è¡¥ä¸ä½¿ç”¨
    def span_if_active(self, name, category, **extras):
        return self.span(name, category, **extras) if self is not None else nullcontext()

    def _compute_decode_stats(self, arr, batch_size: int = 1):
        """
        é€šç”¨çš„å»¶è¿Ÿç»Ÿè®¡å·¥å…·ã€‚

        å‚æ•°:
            arr: ä¸€ä¸ªåŒ…å«è‹¥å¹²è€—æ—¶ï¼ˆå•ä½: msï¼‰çš„åˆ—è¡¨ã€‚
            batch_size: æ¯æ¬¡è°ƒç”¨ forward å¤„ç†çš„æ ·æœ¬æ•°ï¼ˆç”¨äºæ¢ç®— token/sï¼‰ã€‚

        è¿”å›:
            ç»Ÿè®¡å­—å…¸ï¼Œå­—æ®µåŒ…æ‹¬:
              - count: æ ·æœ¬æ•°
              - sum_ms: æ€»è€—æ—¶ (ms)
              - mean_ms: å¹³å‡å€¼ (ms)
              - p50_ms / p90_ms / p99_ms / p999_ms: åˆ†ä½æ•° (ms)
              - max_ms: æœ€å¤§å€¼ (ms)
              - decode_toks_per_s: å¦‚æœ batch_size>0ï¼Œåˆ™è®¤ä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆ batch_size ä¸ª tokenï¼Œ
                                   ç»™å‡ºå¯¹åº”çš„ token/sï¼ˆå¦åˆ™ä¸º Noneï¼‰
        """
        if not arr:
            return {
                "count": 0,
                "sum_ms": 0.0,
                "mean_ms": None,
                "p50_ms": None,
                "p90_ms": None,
                "p99_ms": None,
                "p999_ms": None,
                "max_ms": None,
                "decode_toks_per_s": None,
            }

        # è¿‡æ»¤éæ³•å€¼å¹¶è½¬æˆ float
        arr = [float(x) for x in arr if isinstance(x, (int, float))]
        if not arr:
            return {
                "count": 0,
                "sum_ms": 0.0,
                "mean_ms": None,
                "p50_ms": None,
                "p90_ms": None,
                "p99_ms": None,
                "p999_ms": None,
                "max_ms": None,
                "decode_toks_per_s": None,
            }

        s = sorted(arr)
        n = len(s)

        def q(p: float) -> float:
            if n == 1:
                return s[0]
            # ç®€å•æŒ‰ index å–å€¼å³å¯ï¼Œè¶³å¤Ÿç”¨äºç»Ÿè®¡
            idx = int((n - 1) * p)
            idx = max(0, min(n - 1, idx))
            return s[idx]

        total_ms = float(sum(s))
        mean_ms = total_ms / n
        max_ms = s[-1]

        total_s = total_ms / 1000.0
        total_tokens = n * max(int(batch_size), 1)
        toks_per_s = total_tokens / total_s if total_s > 0 else None

        return {
            "count": n,
            "sum_ms": total_ms,
            "mean_ms": mean_ms,
            "p50_ms": q(0.50),
            "p90_ms": q(0.90),
            "p99_ms": q(0.99),
            "p999_ms": q(0.999),
            "max_ms": max_ms,
            "decode_toks_per_s": toks_per_s,
        }


    def finalize(
        self,
        tokens_in: int,
        tokens_out: int,
        extra_meta: Optional[Dict[str, Any]] = None,
        kv_stats: Optional[Dict[str, Any]] = None,
        wsm_runtime: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        æŠŠ timeline + forward_events æ±‡æ€»æˆç»“æ„åŒ–ç»“æœï¼Œå¹¶è¡¥å……ä¸€ç»„æ›´ã€Œè®ºæ–‡å‹å¥½ã€çš„æŒ‡æ ‡:

        - TE2E: ç«¯åˆ°ç«¯å»¶è¿Ÿ (Time End-to-End)
        - TTFT: Time To First Token
        - TBT: Time Between Tokensï¼ˆç¨³å®šé˜¶æ®µï¼Œå¿½ç•¥ç¬¬ä¸€ä¸ª tokenï¼‰
        - decode æœŸé—´ IO / Compute çš„è¿‘ä¼¼ overlap ratio
        - æ¯å±‚ decoder çš„ compute / IO æ±‡æ€»
        """
        # -------- meta / runtime è¡¥å…… --------
        if extra_meta:
            self.meta.update(extra_meta)

        if wsm_runtime is not None:
            self.wsm_runtime = wsm_runtime
        elif self.wsm_runtime is None:
            self.wsm_runtime = {}

        # ä» meta ä¸­è·å– batch_sizeï¼ˆç”¨äºæ‰€æœ‰ token/s ç›¸å…³ç»Ÿè®¡ï¼‰
        bsz = int(self.meta.get("batch_size") or 1)

        # -------- 1. ä» CUDA events è§£æ prefill / decode æ¯ step è€—æ—¶ --------
        decode_ms: List[float] = []
        prefill_ms: List[float] = []
        if torch.cuda.is_available():
            for ev in self.forward_events:
                # forward_events: (kind, B, T, start_evt, end_evt)
                kind, B, T, start_evt, end_evt = ev
                if start_evt is None or end_evt is None:
                    continue
                try:
                    end_evt.synchronize()
                    dt_ms = float(start_evt.elapsed_time(end_evt))
                except Exception:
                    continue

                if kind == "decode":
                    decode_ms.append(dt_ms)
                elif kind == "prefill":
                    prefill_ms.append(dt_ms)

        # -------- 2. E2E / prefill / decode å¢™é’Ÿæ—¶é—´ --------
        # inference_scope() ç”¨ "inference_e2e" åŒ…è£¹æ•´ä¸ªæ¨ç†
        infer_spans = [ev for ev in self.timeline if ev.get("name") == "inference_e2e"]
        if infer_spans:
            t0 = min(float(ev["t_start_ms"]) for ev in infer_spans)
            t1 = max(float(ev["t_end_ms"]) for ev in infer_spans)
            e2e_ms = t1 - t0
        else:
            e2e_ms = None

        prefill_total_ms = float(sum(prefill_ms)) if prefill_ms else 0.0
        decode_total_ms = float(sum(decode_ms)) if decode_ms else 0.0

        # é¢„çƒ­ GPU decoder windowï¼ˆwarmup layerï¼‰æ—¶é—´
        warmup_spans = [ev for ev in self.timeline if ev.get("name") == "gpu_window_warmup"]
        warmup_total_ms = sum(float(ev.get("dur_ms", 0.0)) for ev in warmup_spans)
        warmup_calls = len(warmup_spans)

        # -------- 3. æŒ‰ category ç»Ÿè®¡å„é˜¶æ®µå¢™é’Ÿæ—¶é—´ --------
        by_cat: Dict[str, float] = {}
        for ev in self.timeline:
            cat = ev.get("cat")
            dur = float(ev.get("dur_ms", 0.0))
            if not cat or dur <= 0:
                continue
            by_cat[cat] = by_cat.get(cat, 0.0) + dur

        sum_cat = {
            "total_ms": sum(by_cat.values()) if by_cat else 0.0,
            "by_cat_ms": by_cat,
        }

        # -------- 4. decode per-step ç»Ÿè®¡ï¼ˆç”¨äº TBTã€decode throughputï¼‰--------
        decode_stats = self._compute_decode_stats(decode_ms, batch_size=bsz)

        # TBT é‡‡ç”¨ã€Œç¨³å®šé˜¶æ®µã€çš„ token é—´éš”ï¼Œæ‰€ä»¥è·³è¿‡ç¬¬ä¸€ä¸ª token
        tbt_stats = None
        if len(decode_ms) > 1:
            tbt_stats = self._compute_decode_stats(decode_ms[1:], batch_size=bsz)

        # TTFT: prefill æ€»æ—¶é—´ + ç¬¬ä¸€ä¸ª decode step
        # è¿™å’Œå¸¸è§æ–‡çŒ®/æ–‡æ¡£ä¸­â€œä»è¯·æ±‚åˆ°ç¬¬ä¸€ä¸ª token çš„å»¶è¿Ÿâ€ä¸€è‡´
        ttft_ms = None
        if prefill_total_ms and decode_ms:
            ttft_ms = prefill_total_ms + decode_ms[0]
        elif prefill_total_ms:
            ttft_ms = prefill_total_ms

        # -------- 5. slack_ms ç»Ÿè®¡ï¼ˆgroup ready åˆ°çœŸæ­£ä½¿ç”¨ä¹‹é—´çš„é—´éš”ï¼‰--------
        slack_times: List[float] = []
        for ev in self.timeline:
            if ev.get("name") == "wsm.wait_group_ready":
                s = ev.get("slack_ms")
                if isinstance(s, (int, float)) and s >= 0:
                    slack_times.append(float(s))

        slack_stats = (
            self._compute_decode_stats(slack_times)
            if slack_times
            else {
                "count": 0,
                "sum_ms": 0.0,
                "mean_ms": None,
                "p50_ms": None,
                "p90_ms": None,
                "p99_ms": None,
                "p999_ms": None,
                "max_ms": None,
                "decode_toks_per_s": None,
            }
        )

        # -------- 6. WSM ç»´åº¦çš„æ€» IO æ—¶é—´ / ç­‰å¾…æ—¶é—´ --------
        def _sum_by_name(name: str) -> float:
            total = 0.0
            for ev in self.timeline:
                if ev.get("name") != name:
                    continue
                total += float(ev.get("dur_ms", 0.0))
            return total

        def _sum_io(name: str, group_key: Optional[str] = None) -> Dict[str, Any]:
            total = 0.0
            by_group: Dict[str, float] = {}
            for ev in self.timeline:
                if ev.get("name") != name:
                    continue
                dur = float(ev.get("dur_ms", 0.0))
                total += dur
                if group_key is not None:
                    g = str(ev.get(group_key, "unknown"))
                    by_group[g] = by_group.get(g, 0.0) + dur
            out: Dict[str, Any] = {"total_ms": total}
            if group_key is not None:
                out["by_group"] = by_group
            return out

        # -------- 5. WSM é«˜å±‚ç»Ÿè®¡ï¼ˆå’Œä½ ä¹‹å‰ä¸€æ ·ï¼‰--------
        wait_total_ms = _sum_by_name("wsm.wait_group_ready")
        ensure_total_ms = _sum_by_name("wsm.ensure_module_on_gpu")

        # IO æŒ‰ç±»å‹ç»Ÿè®¡ï¼ˆSSD->CPU / CPU->GPUï¼‰ï¼Œå¸¦ phase æ–¹ä¾¿çœ‹ prefill / decode
        ssd_io = _sum_io("wsm.ssd_to_cpu_layer", group_key="phase")
        h2d_io = _sum_io("wsm.h2d_param", group_key="phase")

        w_io = {
            "ssd_to_cpu_ms": ssd_io,
            "h2d_param_ms": h2d_io,
        }

        wsm_stats: Dict[str, Any] = {
            "wait_group_ready": {"total_ms": wait_total_ms},
            "ensure_module_on_gpu": {"total_ms": ensure_total_ms},
            "io": w_io,
        }
        if self.wsm_runtime:
            wsm_stats["runtime"] = self.wsm_runtime

        # -------- 6. æ¯ä¸ª decoder layer çš„ compute / IO / å¸¦å®½ç»Ÿè®¡ --------
        decoder_layers_global: Dict[str, float] = {}
        decoder_layers_per_layer: Dict[str, Any] = {}

        # 6.1 GPU è®¡ç®—æ—¶é—´ï¼ˆlayers.py é‡Œçš„ PERF_TRACKERï¼‰
        perf_per_layer: Dict[int, Dict[str, float]] = {}
        perf_stats: Dict[str, Any] = {}
        if PERF_TRACKER is not None:
            try:
                perf_stats = PERF_TRACKER.get_stats()  # {"global": {...}, "per_layer": {...}}
                decoder_layers_global = dict(perf_stats.get("global", {}))
                perf_per_layer = perf_stats.get("per_layer", {}) or {}
            except Exception:
                perf_stats = {}
                decoder_layers_global = {}
                perf_per_layer = {}

        # 6.2 ä» WSM timeline æŒ‰ layer_idx èšåˆ IO æ—¶é—´ + å­—èŠ‚æ•°
        layer_io: Dict[int, Dict[str, float]] = {}
        for ev in self.timeline:
            lid = ev.get("layer_idx")
            if lid is None:
                continue
            name = ev.get("name", "")
            if not (
                name.startswith("wsm.ssd_to_cpu_layer")
                or name.startswith("wsm.h2d_param")
                or name.startswith("wsm.wait_group_ready")
            ):
                continue

            lid = int(lid)
            entry = layer_io.setdefault(
                lid,
                {
                    "ssd_to_cpu_ms": 0.0,
                    "ssd_to_cpu_bytes": 0.0,
                    "h2d_param_ms": 0.0,
                    "h2d_param_bytes": 0.0,
                    "wait_group_ready_ms": 0.0,
                },
            )
            dur = float(ev.get("dur_ms", 0.0))
            b = float(ev.get("bytes", 0.0))

            if name.startswith("wsm.ssd_to_cpu_layer"):
                entry["ssd_to_cpu_ms"] += dur
                entry["ssd_to_cpu_bytes"] += b
            elif name.startswith("wsm.h2d_param"):
                entry["h2d_param_ms"] += dur
                entry["h2d_param_bytes"] += b
            elif name.startswith("wsm.wait_group_ready"):
                entry["wait_group_ready_ms"] += dur

        # 6.3 åˆå¹¶ per-layer compute + IO + å¸¦å®½
        all_layer_ids = sorted(set(list(perf_per_layer.keys()) + list(layer_io.keys())))
        for lid in all_layer_ids:
            lp = perf_per_layer.get(lid, {})  # æ¥è‡ª cuda_timer çš„å„ç±» us ç»Ÿè®¡
            li = layer_io.get(lid, {})

            attn_us = float(lp.get("attn_us", 0.0))
            ffn_us = float(lp.get("ffn_us", 0.0))
            kv_us = float(lp.get("kv_fetch_us", 0.0))
            total_forward_us = float(lp.get("total_forward_us", 0.0))
            if total_forward_us == 0.0 and (attn_us or ffn_us or kv_us):
                total_forward_us = attn_us + ffn_us + kv_us

            mem_us = float(lp.get("memory_alloc_us", 0.0))
            weights_hbm_us = float(lp.get("weights_hbm_us", 0.0))

            ssd_ms = float(li.get("ssd_to_cpu_ms", 0.0))
            ssd_bytes = float(li.get("ssd_to_cpu_bytes", 0.0))
            h2d_ms = float(li.get("h2d_param_ms", 0.0))
            h2d_bytes = float(li.get("h2d_param_bytes", 0.0))
            wait_ms = float(li.get("wait_group_ready_ms", 0.0))

            io_total_ms = ssd_ms + h2d_ms + wait_ms

            # å¹³å‡å¸¦å®½ (GB/s, 1GB=1e9 bytes)ï¼Œæ³¨æ„ ms -> s
            ssd_gbps = (
                ssd_bytes / ssd_ms / 1e6 if (ssd_ms > 0.0 and ssd_bytes > 0.0) else None
            )
            h2d_gbps = (
                h2d_bytes / h2d_ms / 1e6 if (h2d_ms > 0.0 and h2d_bytes > 0.0) else None
            )

            decoder_layers_per_layer[str(lid)] = {
                # MHA / FFN / KV çš„ç´¯è®¡è®¡ç®—æ—¶é—´ï¼ˆusï¼‰
                "compute_us": {
                    "attn_us": attn_us,
                    "ffn_us": ffn_us,
                    "kv_fetch_us": kv_us,
                    "total_forward_us": total_forward_us,
                    "memory_alloc_us": mem_us,
                    "weights_hbm_us": weights_hbm_us,
                },
                # å…¼å®¹è€å­—æ®µï¼šåªæ”¾æ—¶é—´ï¼ˆmsï¼‰
                "io_ms": {
                    "ssd_to_cpu_ms": ssd_ms,
                    "h2d_param_ms": h2d_ms,
                    "wait_group_ready_ms": wait_ms,
                },
                # æ–°å¢ï¼šIO å­—èŠ‚æ•°
                "io_bytes": {
                    "ssd_to_cpu_bytes": ssd_bytes if ssd_bytes > 0 else None,
                    "h2d_param_bytes": h2d_bytes if h2d_bytes > 0 else None,
                },
                # æ–°å¢ï¼šå¹³å‡å¸¦å®½ï¼ˆGB/sï¼‰
                "io_bandwidth_gbps": {
                    "ssd_to_cpu_gbps": ssd_gbps,
                    "h2d_param_gbps": h2d_gbps,
                },
                "summary": {
                    # å•å±‚ç´¯è®¡å‰å‘æ—¶é—´ï¼ˆprefill+decodeï¼Œè¿‘ä¼¼ï¼‰
                    "compute_ms_pure": total_forward_us / 1000.0
                    if total_forward_us
                    else None,
                    "io_ms_total": io_total_ms,
                },
            }

        # 6.4 å…¨å±€ decode é˜¶æ®µ IO-Compute overlap ä¼°è®¡
        decoder_compute_ms_total = sum(
            v["summary"]["compute_ms_pure"] or 0.0
            for v in decoder_layers_per_layer.values()
        )
        decoder_io_ms_total = sum(
            v["summary"]["io_ms_total"] for v in decoder_layers_per_layer.values()
        )
        decoder_wall_ms = float(decode_stats.get("sum_ms") or decode_total_ms or 0.0)

        overlap_ratio = None
        overlap_ms = None
        uncovered_io_ms = None
        if decoder_wall_ms > 0.0 and decoder_io_ms_total > 0.0:
            # ç†è®ºï¼šwall_time â‰ˆ max(compute, io) + å…¶å®ƒå¼€é”€
            # è¿‘ä¼¼ï¼šcompute + io - wall â‰ˆ è¢«è¦†ç›–æ‰çš„ IO æ—¶é—´
            raw_overlap = decoder_compute_ms_total + decoder_io_ms_total - decoder_wall_ms
            overlap_ms = max(0.0, raw_overlap)
            overlap_ratio = max(0.0, min(1.0, overlap_ms / decoder_io_ms_total))
            uncovered_io_ms = max(0.0, decoder_io_ms_total - overlap_ms)

        decoder_layers_summary = {
            "compute_ms_total": decoder_compute_ms_total,
            "io_ms_total": decoder_io_ms_total,
            "decode_wall_ms": decoder_wall_ms,
            "overlap_ms_approx": overlap_ms,
            "uncovered_io_ms_approx": uncovered_io_ms,
            "overlap_ratio": overlap_ratio,
        }

        # ä¹ŸæŠŠ overlap ä¿¡æ¯å¡è¿› decode è¿™å—ï¼Œæ–¹ä¾¿ timings.decode.* ç›´æ¥è®¿é—®
        if overlap_ratio is not None:
            decode_stats["overlap_ratio"] = overlap_ratio
            decode_stats["approx_compute_ms_total"] = decoder_compute_ms_total
            decode_stats["approx_io_ms_total"] = decoder_io_ms_total
            decode_stats["approx_overlap_ms"] = overlap_ms
            decode_stats["approx_uncovered_io_ms"] = uncovered_io_ms

        decoder_layers = {
            "global": decoder_layers_global,
            "per_layer": decoder_layers_per_layer,
            "summary": decoder_layers_summary,
        }


        # ä¹ŸæŠŠ overlap_ratio æ”¾è¿› decode ç»Ÿè®¡é‡Œï¼Œæ–¹ä¾¿ç”¨ timings.decode.overlap_ratio è®¿é—®
        if overlap_ratio is not None:
            decode_stats["overlap_ratio"] = overlap_ratio
            decode_stats["approx_compute_ms_total"] = decoder_compute_ms_total
            decode_stats["approx_io_ms_total"] = decoder_io_ms_total

        decoder_layers = {
            "global": decoder_layers_global,
            "per_layer": decoder_layers_per_layer,
            "summary": decoder_layers_summary,
        }

        # -------- 8. throughput ç»Ÿè®¡ --------
        # decode token æ•°ï¼ˆæŒ‰æ¯ä¸ª requestï¼‰
        decode_tokens_per_seq = (
            max(tokens_out - tokens_in, 0)
            if (tokens_out is not None and tokens_in is not None)
            else len(decode_ms)
        )
        total_decode_tokens = decode_tokens_per_seq * bsz
        total_decode_s = decode_total_ms / 1000.0 if decode_total_ms > 0 else None

        decode_toks_per_s = (
            (total_decode_tokens / total_decode_s) if (total_decode_s and total_decode_tokens > 0) else None
        )

        total_prefill_tokens = (tokens_in or 0) * bsz
        total_prefill_s = prefill_total_ms / 1000.0 if prefill_total_ms > 0 else None
        prefill_toks_per_s = (
            (total_prefill_tokens / total_prefill_s)
            if (total_prefill_s and total_prefill_tokens > 0)
            else None
        )

        throughput = {
            "prefill_toks_per_s": prefill_toks_per_s,
            "decode_toks_per_s": decode_toks_per_s,
        }

        # -------- 9. TE2E / TTFT / TBT ç­‰æ±‡æ€»åˆ° timings --------
        timings: Dict[str, Any] = {
            "e2e_ms": e2e_ms,
            "te2e_ms": e2e_ms,  # aliasï¼Œæ–¹ä¾¿ç›´æ¥åœ¨è®ºæ–‡é‡Œå¼•ç”¨
            "prefill_total_ms": prefill_total_ms,
            "ttft_ms": ttft_ms,
            "first_token_latency_ms": ttft_ms,  # ä¿æŒå‘åå…¼å®¹
            "decode": decode_stats,
            "tbt_steady_ms": tbt_stats,
            "warmup_total_ms": warmup_total_ms,
            "warmup_calls": warmup_calls,
            "by_category_ms": sum_cat,
        }

        # -------- 10. GPU å†…å­˜å³°å€¼ --------
        memory_stats: Dict[str, Any] = {}
        if torch.cuda.is_available():
            try:
                alloc_gb = torch.cuda.max_memory_allocated() / (1024 ** 3)
                reserved_gb = torch.cuda.max_memory_reserved() / (1024 ** 3)
                memory_stats = {
                    "gpu_peak_allocated_gb": float(alloc_gb),
                    "gpu_peak_reserved_gb": float(reserved_gb),
                }
            except Exception:
                memory_stats = {}

        # -------- 11. æ±‡æ€»æˆ result --------
        self.result = {
            "run": self.meta
            | {
                "run_id": self.run_id,
                "finished_at_utc": _now_utc(),
            },
            "counts": {
                "tokens_in": tokens_in,
                "tokens_out": tokens_out,
                "tokens_in_total": tokens_in * bsz if tokens_in is not None else None,
                "tokens_out_total": tokens_out * bsz if tokens_out is not None else None,
            },
            "timings": timings,
            "throughput": throughput,
            "wsm": wsm_stats,
            "decoder_layers": decoder_layers,
            "decode_step_ms": decode_ms,
            "timeline": self.timeline,
            "memory": memory_stats,
        }

        if kv_stats is not None:
            self.result["kv_cache"] = kv_stats

        return self.result




    def save(self, path: str):
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        if path.lower().endswith(".json"):
            with open(path, "w", encoding="utf-8") as f:
                json.dump(self.result, f, ensure_ascii=False, indent=2)
        elif path.lower().endswith(".csv"):
            rows = []
            for ev in self.timeline:
                r = {"kind":"span","name":ev["name"],"cat":ev["cat"],
                     "t_start_ms":ev["t_start_ms"],"t_end_ms":ev["t_end_ms"],"dur_ms":ev["dur_ms"]}
                rows.append(r)
            for i,dt in enumerate(self.result.get("decode_step_ms", [])):
                rows.append({"kind":"decode_step","name":f"decode_{i:04d}","cat":"inference","t_start_ms":"", "t_end_ms":"", "dur_ms":dt})
            with open(path, "w", newline="", encoding="utf-8") as f:
                w = csv.DictWriter(f, fieldnames=["kind","name","cat","t_start_ms","t_end_ms","dur_ms"])
                w.writeheader(); w.writerows(rows)
        else:
            with open(path + ".json", "w", encoding="utf-8") as f:
                json.dump(self.result, f, ensure_ascii=False, indent=2)

# ===== è·¯å¾„ä¸å¸¸é‡ï¼ˆæŒ‰ä½ çš„ç¯å¢ƒï¼‰ =====
PROMPT_TXT = Path(os.getenv(
    "PROMPT_TXT",
    "/home/roger/llama3-inference/prompts/prompts_batch512_len2048.txt",
))
RAW_DEV  = os.getenv("RAW_DEV",  "/dev/nvme0n1p4")
MANIFEST = os.getenv("MANIFEST", "/data1/70b-fixed.runtime_manifest.json")
CKPT_DIR = os.getenv("CKPT_DIR", "/home/roger/.llama/checkpoints/Llama3.1-70B")

# ===== Workload é…ç½®ï¼šç»Ÿä¸€æ”¹ batch / ç”Ÿæˆ token =====
# å¯ä»¥ç›´æ¥æ”¹è¿™é‡Œçš„é»˜è®¤å€¼ï¼Œä¹Ÿå¯ä»¥ç”¨ç¯å¢ƒå˜é‡è¦†ç›–



# ---------- ç³»ç»Ÿ/GPU å†…å­˜å¿«ç…§ ----------
def _read_status():
    def _grep(path, keys):
        out = {}
        try:
            with open(path, "r") as f:
                for line in f:
                    for k in keys:
                        if line.startswith(k + ":"):
                            out[k] = line.split(":")[1].strip()
        except Exception:
            pass
        return out
    s = _grep("/proc/self/status", ["VmRSS","VmHWM","VmLck"])
    m = _grep("/proc/meminfo", ["MemAvailable","CommitLimit","Committed_AS","Cached","Buffers"])
    return s, m

def _gpu_mem():
    if not torch.cuda.is_available():
        return {}
    dev = torch.cuda.current_device()
    st  = torch.cuda.memory_stats(dev)
    return {
        "alloc_GB": st.get("allocated_bytes.all.current", 0)/(1<<30),
        "rsrv_GB":  st.get("reserved_bytes.all.current", 0)/(1<<30),
    }

def probe(stage: str):
    s, m = _read_status()
    g    = _gpu_mem()
    print(f"\n[MEM] {stage}")
    print(f"  VmRSS={s.get('VmRSS','?')}  VmLck(pinned)={s.get('VmLck','?')}  "
          f"CommitLimit={m.get('CommitLimit','?')}  Committed_AS={m.get('Committed_AS','?')}  "
          f"MemAvailable={m.get('MemAvailable','?')}")
    if g:
        print(f"  GPU: allocated={g['alloc_GB']:.2f} GiB  reserved={g['rsrv_GB']:.2f} GiB")
    print()

# ---------- æ‰«æå‚æ•°åœ¨ meta/cpu/cuda ä¸Šçš„å ç”¨ ----------
def dump_param_inventory(model, tag):
    buckets = {"cpu":0, "cuda":0, "meta":0, "other":0}
    big_cpu = []
    for n,p in model.named_parameters(recurse=True):
        b = p.numel() * p.element_size()
        if getattr(p, "is_meta", False):
            buckets["meta"] += b
        elif hasattr(p, "device"):
            t = p.device.type
            if t == "cpu":
                buckets["cpu"] += b
                if b >= (64<<20):
                    big_cpu.append((n,b))
            elif t == "cuda":
                buckets["cuda"] += b
            else:
                buckets["other"] += b
        else:
            buckets["other"] += b
    f = lambda x: f"{x/(1<<30):.2f} GiB"
    print(f"[PARAMS] {tag}: cpu={f(buckets['cpu'])}, cuda={f(buckets['cuda'])}, meta={f(buckets['meta'])}, other={f(buckets['other'])}")
    if big_cpu:
        big_cpu.sort(key=lambda x:-x[1])
        print("  [big-cpu] top:")
        for n,b in big_cpu[:10]:
            print(f"   - {n}  {b/(1<<20):.1f} MiB")

# ---------- è¿è¡Œæ—¶è¦†ç›–ï¼šæ”¶æ•› pinned/æ³¨å†Œæ±  ----------
def apply_runtime_overrides():
    """
    æŠŠæ³¨å†Œæ€»é‡é’³åœ¨ â‰¤256MiBï¼Œå¹¶æŠŠ EXTENT_BYTES é™åˆ° 1MiBï¼Œé™ä½é«˜é˜¶é¡µ order å‹åŠ›ã€‚
    """
    cfg = load_runtime_config({
        "pinned": {
            "WEIGHT_PINNED_BYTES":      8  << 30,
            "KV_PINNED_BYTES":          8  << 30,
            "EXTENT_BYTES":             1  << 20,   # 1MiB
            "PINNED_REGISTER_CHUNK":   16  << 20,   # 16MiB
            "PINNED_REGISTER_N":            8,      # 128MiB
        },
        "regpool": {
            "REG_POOL_N_BUFFERS":           8,
            "REG_POOL_BUF_BYTES":     16 << 20,     # ~128MiB ä¼ é€å¸¦
        },
        "io": {
            "RAW_IO_QD_WRITE":             24,      # å†™é˜Ÿåˆ—æ·±åº¦
            "IO_RAW_THROTTLE_MS":          30,      # å†™å¸¦å®½çª—å£
        }
    })
    D = runtime_config_to_dict(cfg)
    p = D["pinned"]
    need  = int(p["WEIGHT_PINNED_BYTES"])
    chunk = int(p["PINNED_REGISTER_CHUNK"])
    target_total = min(need // 2, 256 << 20)  # ç›®æ ‡ â‰¤ 256MiB
    newN = max(1, target_total // chunk)
    p["PINNED_REGISTER_N"] = newN
    cfg = load_runtime_config({"pinned": p, "io": D["io"]})
    print("[RuntimeConfig] pinned =", runtime_config_to_dict(cfg)["pinned"])
    print("[RuntimeConfig] io =", runtime_config_to_dict(cfg)["io"])
    return cfg

# ---------- KV æ± ï¼šæ‡’åˆ†é… + å•å— â‰¥ å•ä¸ª KV å— ----------
def configure_kv_pool():
    # DRAM é…ç½®
    KVCacheArgs.dram_limit_gb     = 32.0
    KVCacheArgs.dram_sizing_batch = 32
    KVCacheArgs.block_bytes       = 1 * 1024 * 1024
    KVCacheArgs.preallocate       = False
    KVCacheArgs.lazy_init         = True

    # å…³é—­ push å³æ—¶é•œåƒï¼Œé‡‡ç”¨åç§»/èšåˆå†™ï¼ˆé¿å…ä¸æƒé‡ H2D å†²çªï¼‰
    KVCacheArgs.mirror_on_push = False

    # I/O èŠ‚æµä¸å†™é€Ÿç‡é…ç½®ï¼ˆä¸æƒé‡ H2D ä»²è£ï¼‰
    KVCacheArgs.IO_RAW_THROTTLE_MS     = 25
    KVCacheArgs.NVME_WRITE_TARGET_MBPS = 1500

    if hasattr(KVCacheArgs, "prefer_bf16"):
        KVCacheArgs.prefer_bf16 = True

    print(f"[KVArgs] dram_limit={KVCacheArgs.dram_limit_gb} GiB, "
          f"block_bytes={KVCacheArgs.block_bytes//(1<<20)} MiB, prealloc={KVCacheArgs.preallocate}")
    print(f"[KVArgs] mirror_on_push={KVCacheArgs.mirror_on_push}, "
          f"IO_RAW_THROTTLE_MS={KVCacheArgs.IO_RAW_THROTTLE_MS}, "
          f"NVME_WRITE_TARGET_MBPS={KVCacheArgs.NVME_WRITE_TARGET_MBPS}")

# ---------- è¯†åˆ«â€œå®é™…è¿è¡Œçš„æ¨¡å¼â€ ----------
def classify_mode(llama) -> str:
    """
    è¿”å›ï¼š'ssd-streaming' / 'cpu-gpu-streaming' / 'full-gpu' / 'full-cpu' / 'meta-only'
    """
    m = llama.model
    if hasattr(llama, "weight_streaming_manager"):
        wsm = llama.weight_streaming_manager
        ssd = bool(getattr(wsm, "ssd_enabled", False) or getattr(wsm, "ssd", None))
        cpu_warm = getattr(wsm, "disable_cpu_warm", None)
        mode = "ssd-streaming" if ssd else "cpu-gpu-streaming"
        print(f"[MODE] detected={mode}  (has WSM, ssd={ssd}, disable_cpu_warm={cpu_warm})")
        return mode
    cpu, cuda, meta = 0,0,0
    for _,p in m.named_parameters():
        b = p.numel()*p.element_size()
        if getattr(p, "is_meta", False): meta += b
        elif p.device.type == "cpu":     cpu  += b
        elif p.device.type == "cuda":    cuda += b
    if cuda > 0 and cpu == 0 and meta == 0:
        print("[MODE] detected=full-gpu"); return "full-gpu"
    if cpu  > 0 and cuda == 0 and meta == 0:
        print("[MODE] detected=full-cpu"); return "full-cpu"
    if meta > 0 and cpu == 0 and cuda == 0:
        print("[MODE] detected=meta-only"); return "meta-only"
    print("[MODE] mixed/unrecognized (check PARAMS dump below)")
    return "unknown"


# ===== WSM wait_group_ready åŒ…è£…ï¼ˆä»…æ·»åŠ  Profiler è®¡æ—¶ï¼Œä¸æ”¹é€»è¾‘ï¼‰ =====
def _wrap_wait_group_ready(original_method):
    """
    åŒ…è£… WSM.wait_group_readyï¼Œæ·»åŠ ï¼š
      - profiler è®¡æ—¶åŸ‹ç‚¹ï¼›
      - ä» group ready åˆ°é¦–æ¬¡ compute ä½¿ç”¨çš„ slack_ms ä¼°è®¡ï¼›
      - pipeline æ°´ä½ï¼ˆring / inflight / in_use çš„æœ€å¤§å€¼ï¼‰ã€‚
    """
    def wrapped(self, layer_idx: int, group: str, compute_stream=None):
        prof = globals().get("PROFILER")
        extras = {
            "layer_idx": int(layer_idx),
            "group": str(group),
        }

        # 1) slack_ms = ç°åœ¨æ—¶é—´ - è¯¥ group ready äº‹ä»¶è®°å½•æ—¶é—´
        if prof is not None and hasattr(self, "_group_ready_wallclock"):
            try:
                key = (int(layer_idx), str(group))
                ready_ms = self._group_ready_wallclock.get(key)
                if isinstance(ready_ms, (int, float)):
                    now_ms = prof.now_ms()
                    extras["slack_ms"] = max(0.0, now_ms - float(ready_ms))
            except Exception:
                pass

        # 2) pipeline æ°´ä½ç»Ÿè®¡ï¼ˆæœ€å¤§ ring/inflight/in_useï¼‰
        if hasattr(self, "_pipeline_watermark"):
            try:
                lock = getattr(self, "_group_lock", None)
                if lock is not None:
                    with lock:
                        ring_len     = len(getattr(self, "_gpu_group_ring", []))
                        inflight_len = len(getattr(self, "_gpu_group_inflight", set()))
                        in_use_len   = len(getattr(self, "_gpu_group_in_use", {}))
                else:
                    ring_len     = len(getattr(self, "_gpu_group_ring", []))
                    inflight_len = len(getattr(self, "_gpu_group_inflight", set()))
                    in_use_len   = len(getattr(self, "_gpu_group_in_use", {}))

                wm = self._pipeline_watermark
                wm["max_gpu_ring"]  = max(wm.get("max_gpu_ring", 0), ring_len)
                wm["max_inflight"]  = max(wm.get("max_inflight", 0), inflight_len)
                wm["max_in_use"]    = max(wm.get("max_in_use", 0), in_use_len)
            except Exception:
                pass

        ctx = prof.span("wsm.wait_group_ready", "wsm", **extras) if prof is not None else nullcontext()
        with ctx:
            return original_method(layer_idx, group, compute_stream)

    return wrapped



def _patched_ensure_module_on_gpu(self, m: torch.nn.Module, layer_idx: int | None = None, module_name: str | None = None):
    """
    æ‰©å±•ï¼šæŠŠ **0-size CPU stub** å½“ä½œ meta ä¸€æ ·å¤„ç†ï¼Œä¼˜å…ˆä» CPU cache å–å›å¹¶ä¸Šå¡ã€‚
    å…¶å®ƒæƒ…å†µä»å¤ç”¨åŸå…ˆçš„ _ensure_param_on_gpu() è·¯å¾„ã€‚
    """
    with (PROFILER.span("wsm.ensure_module_on_gpu", "wsm", layer_idx=(None if layer_idx is None else int(layer_idx)), module=str(module_name))
          if (globals().get("PROFILER") is not None) else nullcontext()):
        params_to_replace = {}
        params_full_names = {}

        def _full_name(layer_idx: int, module_name: str, local_param_name: str) -> str:
            if module_name in ("wq", "wk", "wv", "wo"):
                parent = "attention"
            elif module_name in ("w1", "w2", "w3"):
                parent = "feed_forward"
            else:
                parent = module_name or ""
            return f"layers.{layer_idx}.{parent}.{module_name}.{local_param_name}" if parent else f"layers.{layer_idx}.{module_name}.{local_param_name}"

        def _fetch_from_cpu_cache(name: str):
            if (layer_idx is not None) and (layer_idx in self.cpu_cache):
                return self.cpu_cache[layer_idx].get(name)
            return None

        for local_param_name, p in m.named_parameters(recurse=False):
            full_name = None
            if (layer_idx is not None) and (module_name is not None):
                full_name = _full_name(layer_idx, module_name, local_param_name)

            is_meta     = (p.device.type == "meta") or getattr(p, "is_meta", False)
            is_cpu_stub = (p.device.type == "cpu")  and (p.numel() == 0)

            if (is_meta or is_cpu_stub) and self.ssd_enabled and full_name:
                # ç¡®ä¿æœ¬å±‚å·²æœ‰ CPU cacheï¼ˆæ²¡æœ‰å°±ç«‹å³åŠ è½½ï¼‰
                if (layer_idx not in self.cpu_cache):
                    try:
                        self._load_layer_to_cpu(int(layer_idx))
                    except Exception:
                        pass

                cached = _fetch_from_cpu_cache(full_name)
                expected = tuple(getattr(getattr(m, local_param_name), "shape", ()))
                chosen_name, chosen_tensor = None, None

                def _try_pick(names: list[str]):
                    nonlocal chosen_name, chosen_tensor
                    for nm in names:
                        t = _fetch_from_cpu_cache(nm)
                        if t is not None and (not expected or tuple(t.shape) == expected):
                            chosen_name, chosen_tensor = nm, t
                            break

                if cached is not None and (not expected or tuple(cached.shape) == expected):
                    chosen_name, chosen_tensor = full_name, cached
                else:
                    cand = []
                    if module_name in ("wq", "wk", "wv"):
                        cand = [f"layers.{layer_idx}.attention.{x}.{local_param_name}" for x in ("wq","wk","wv")]
                    elif module_name in ("w1", "w2", "w3"):
                        cand = [f"layers.{layer_idx}.feed_forward.{x}.{local_param_name}" for x in ("w1","w2","w3")]
                    else:
                        cand = [full_name]
                    _try_pick(cand)
                    if chosen_tensor is None and cached is not None:
                        chosen_name, chosen_tensor = full_name, cached  # é€€è€Œæ±‚å…¶æ¬¡

                if chosen_tensor is not None:
                    with torch.cuda.stream(self._select_h2d_stream_for(module_name=module_name)):
                        p_gpu = chosen_tensor.to(self.device, non_blocking=True)
                    params_to_replace[local_param_name] = torch.nn.Parameter(p_gpu, requires_grad=p.requires_grad)
                    params_full_names[local_param_name] = chosen_name or full_name
                    if getattr(self, "verbose", False):
                        print(f"[WSM DEBUG] âœ“ Loaded {'meta' if is_meta else 'stub'} param {params_full_names[local_param_name]} to GPU: {tuple(p_gpu.shape)}")
                else:
                    if getattr(self, "verbose", False):
                        print(f"[WSM WARN] CPU cache miss for {full_name} (layer {layer_idx}); will rely on ensure_group_on_gpu() later")
                continue  # è¯¥å‚æ•°å¤„ç†å®Œæ¯•

            # å…¶å®ƒæƒ…å†µï¼šæ²¿ç”¨åŸæ¥çš„ CPUâ†’GPU é€»è¾‘
            self._ensure_param_on_gpu(p, layer_idx, full_name)

        # å®‰è£…æ›¿æ¢åçš„ Parameterï¼Œå¹¶ç»´æŠ¤ name æ˜ å°„
        for pname, new_param in params_to_replace.items():
            m._parameters[pname] = new_param
            full = params_full_names.get(pname)
            if full:
                try:
                    pobj = getattr(m, pname)
                except Exception:
                    pobj = new_param
                self.name_to_param[full] = pobj
                self.param_owner[full]   = (m, pname)

        # buffer ç»´æŒåŸæœ‰ç­–ç•¥ï¼šmetaâ†’materializeï¼ŒCPUâ†’ä¸Šå¡
        for b in m.buffers(recurse=True):
            if getattr(b, "is_meta", False):
                try:
                    b = b.to_empty(device=self.device)
                except Exception:
                    pass
            elif b.device.type == "cpu":
                with torch.cuda.stream(self._select_h2d_stream_for(module_name=module_name)):
                    b_gpu = b.detach().to(self.device, non_blocking=True)
                try:
                    b.data = b_gpu
                except Exception:
                    pass
                
def _patch_wsm_for_profiling(wsm):
    """
    ç»™ WeightStreamingManager æ‰“è¡¥ä¸ï¼Œè¡¥é½ï¼š
      - SSD -> pinned CPU è¯»å±‚æ—¶é—´ï¼ˆå«å­—èŠ‚æ•°ï¼‰
      - pinned CPU -> GPU ç»„çº§ H2D æ—¶é—´ï¼ˆå«å­—èŠ‚æ•°ã€attn/ffnï¼‰
      - wait_group_ready äº‹ä»¶ï¼ˆä¿æŒåŸæ¥ç»Ÿè®¡ï¼‰
    æ³¨æ„ï¼šåªä¾èµ– run æ—¶çš„ wsm å®ä¾‹ï¼Œä¸æ”¹ wsm æºç ã€‚
    """
    global PROFILER
    prof = PROFILER
    if prof is None:
        # æ²¡å¼€ profiler å°±ä¸æ‰“ç‚¹ï¼Œä¿æŒé›¶å¼€é”€
        return

    # ---------- 1) wait_group_readyï¼šä¿ç•™ä½ ç°æœ‰çš„ç­‰å¾…ç»Ÿè®¡ ----------
    if hasattr(wsm, "_record_group_ready_event"):
        orig_rg = wsm._record_group_ready_event

        def _record_group_ready_event_patched(self, layer_idx, group, *args, **kwargs):
            # è¿™é‡Œåªè®°å½• wait è‡ªèº«çš„é˜»å¡æ—¶é—´ï¼›æ›´ç»†çš„ slack ä½ ä¹‹å‰å·²ç»åœ¨ _decode_timeline é‡Œç®—äº†
            s = time.perf_counter_ns()
            try:
                return orig_rg(layer_idx, group, *args, **kwargs)
            finally:
                e = time.perf_counter_ns()
                rec = {
                    "name": "wsm.wait_group_ready",
                    "cat": "wsm",
                    "t_start_ms": (s - prof.t0_ns) / 1e6,
                    "t_end_ms":   (e - prof.t0_ns) / 1e6,
                    "dur_ms":     (e - s) / 1e6,
                    "layer_idx":  int(layer_idx),
                    "group":      str(group),
                }
                prof.timeline.append(rec)

        wsm._record_group_ready_event = types.MethodType(_record_group_ready_event_patched, wsm)

    # ---------- 2) SSD -> pinned CPUï¼šæŒ‰ layer ç»Ÿè®¡ ----------
    def _ssd_bytes_for_layer(self, layer_idx: int) -> int:
        try:
            params = self.layers_params.get(int(layer_idx), [])
        except Exception:
            return 0
        total = 0
        for p in params:
            try:
                if p.get("policy") == "stream":
                    total += int(p.get("nbytes", 0))
            except Exception:
                pass
        return int(total)

    if getattr(wsm, "ssd_enabled", False):
        # åŒæ­¥ç‰ˆæœ¬ï¼ˆå¯èƒ½è¢« warmup ç”¨åˆ°ï¼‰
        if hasattr(wsm, "_read_layer_from_ssd"):
            orig_read = wsm._read_layer_from_ssd

            def _read_layer_from_ssd_patched(self, layer_idx: int):
                lid = int(layer_idx)
                total_bytes = _ssd_bytes_for_layer(self, lid)
                phase = getattr(self, "_phase", None) or "unknown"
                with prof.span_if_active(
                    "wsm.ssd_to_cpu_layer",
                    "io",
                    layer_idx=lid,
                    bytes=total_bytes,
                    phase=phase,
                    thread="main",
                ):
                    return orig_read(lid)

            wsm._read_layer_from_ssd = types.MethodType(_read_layer_from_ssd_patched, wsm)

        # çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼šçœŸæ­£çš„ CPU é¢„å–çº¿ç¨‹èµ°çš„æ˜¯è¿™ä¸ª
        if hasattr(wsm, "_read_layer_from_ssd_threadsafe"):
            orig_read_ts = wsm._read_layer_from_ssd_threadsafe

            def _read_layer_from_ssd_threadsafe_patched(self, layer_idx: int):
                lid = int(layer_idx)
                total_bytes = _ssd_bytes_for_layer(self, lid)
                phase = getattr(self, "_phase", None) or "unknown"
                with prof.span_if_active(
                    "wsm.ssd_to_cpu_layer",
                    "io",
                    layer_idx=lid,
                    bytes=total_bytes,
                    phase=phase,
                    thread="cpu_pf_worker",
                ):
                    return orig_read_ts(lid)

            wsm._read_layer_from_ssd_threadsafe = types.MethodType(
                _read_layer_from_ssd_threadsafe_patched, wsm
            )

    # ---------- 3) pinned CPU -> GPUï¼šç»„çº§ H2D ----------
    if hasattr(wsm, "_install_group_on_gpu"):
        orig_install = wsm._install_group_on_gpu

        def _install_group_on_gpu_patched(self, layer_idx: int, group: str, *, h2d_override=None):
            lid = int(layer_idx)
            grp = str(group)
            phase = getattr(self, "_phase", None) or "prefill"

            # ä¼°ç®—è¿™æ¬¡ H2D çš„å­—èŠ‚æ•°ï¼šåªçœ‹ CPU cache é‡Œè¿™å±‚å½“å‰ç»„çš„æƒé‡
            total_bytes = 0
            try:
                suffixes = ()
                if grp == "attn":
                    suffixes = (
                        "attention.wq.weight",
                        "attention.wk.weight",
                        "attention.wv.weight",
                        "attention.wo.weight",
                    )
                elif grp == "ffn":
                    suffixes = (
                        "feed_forward.w1.weight",
                        "feed_forward.w2.weight",
                        "feed_forward.w3.weight",
                    )

                if suffixes:
                    with self.cpu_cache_lock:
                        layer_data = dict(self.cpu_cache.get(lid, {}))

                    for suf in suffixes:
                        pname = f"layers.{lid}.{suf}"
                        t = layer_data.get(pname)
                        if t is not None and torch.is_tensor(t) and t.numel() > 0:
                            total_bytes += int(t.numel() * t.element_size())
            except Exception:
                total_bytes = 0

            with prof.span_if_active(
                "wsm.h2d_param",
                "io",
                layer_idx=lid,
                group=grp,
                bytes=int(total_bytes),
                phase=phase,
            ):
                return orig_install(layer_idx, group, h2d_override=h2d_override)

        wsm._install_group_on_gpu = types.MethodType(_install_group_on_gpu_patched, wsm)

    # ---------- 4) å…¼å®¹è€è·¯å¾„ï¼š_load_layer_to_cpu / _h2d_transfer_with_retry ----------
    # è¿™äº›åœ¨ä½ æ–°ç‰ˆ pipeline ä¸­åŸºæœ¬ä¸ä¼šèµ°åˆ°ï¼Œä½†ç•™ç€ä»¥é˜²ä»¥å fallback
    if hasattr(wsm, "_load_layer_to_cpu"):
        orig_load = wsm._load_layer_to_cpu

        def _load_layer_to_cpu_patched(self, layer_idx: int):
            lid = int(layer_idx)
            total_bytes = _ssd_bytes_for_layer(self, lid)
            phase = getattr(self, "_phase", None) or "unknown"
            with prof.span_if_active(
                "wsm.ssd_to_cpu_layer",
                "io",
                layer_idx=lid,
                bytes=total_bytes,
                phase=phase,
                thread="fallback_sync",
            ):
                return orig_load(lid)

        wsm._load_layer_to_cpu = types.MethodType(_load_layer_to_cpu_patched, wsm)

    if hasattr(wsm, "_h2d_transfer_with_retry"):
        orig_h2d = wsm._h2d_transfer_with_retry

        def _h2d_transfer_with_retry_patched(self, src_cpu_tensor, param_name, h2d_stream):
            bytes_ = 0
            try:
                if torch.is_tensor(src_cpu_tensor) and src_cpu_tensor.numel() > 0:
                    bytes_ = int(src_cpu_tensor.numel() * src_cpu_tensor.element_size())
            except Exception:
                bytes_ = 0

            pname = str(param_name)
            if "attention." in pname:
                grp = "attn"
            elif "feed_forward." in pname:
                grp = "ffn"
            else:
                grp = "other"

            phase = getattr(self, "_phase", None) or "prefill"

            with prof.span_if_active(
                "wsm.h2d_param",
                "io",
                param=pname,
                group=grp,
                bytes=bytes_,
                phase=phase,
            ):
                return orig_h2d(src_cpu_tensor, param_name, h2d_stream)

        wsm._h2d_transfer_with_retry = types.MethodType(
            _h2d_transfer_with_retry_patched, wsm
        )
        
        
def extract_kv_cache_stats(llama):
    """
    ä» LLaMA wrapper ä¸­æå– KV cache ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœä½¿ç”¨äº† KVOffloaderï¼‰ã€‚

    è¿”å›ç¤ºä¾‹ï¼š
    {
        "fetch_blocks_total": 1234,
        "hits": 1200,
        "misses": 34,
        "hit_ratio": 0.9724,
        "ssd_load_blocks_prefetch": 56,
        "evictions": 789,
    }
    """
    try:
        model = getattr(llama, "model", None)
        if model is None:
            return None

        # 1) æœ‰äº›å®ç°ä¼šæŠŠ offloader æŒ‚åœ¨ model ä¸Š
        off = getattr(model, "kv_offloader", None)

        # 2) å¦åˆ™ä»ç¬¬ä¸€å±‚ attention ä¸Šæ‰¾ offloader
        if off is None and hasattr(model, "layers"):
            for blk in getattr(model, "layers", []):
                attn = getattr(blk, "attention", None)
                if attn is None:
                    continue
                off = getattr(attn, "offloader", None)
                if off is not None:
                    break

        if off is None or not hasattr(off, "get_cache_stats"):
            return None

        stats = off.get_cache_stats()
        if not isinstance(stats, dict):
            return None

        # æ¸…æ´—æˆ JSON-friendly çš„ç®€å•ç±»å‹
        cleaned = {}
        for k, v in stats.items():
            if isinstance(v, (int, float)) or v is None:
                cleaned[k] = v
            else:
                try:
                    cleaned[k] = float(v)
                except Exception:
                    cleaned[k] = str(v)
        return cleaned

    except Exception as e:
        print(f"[KV][WARN] extract_kv_cache_stats() failed: {e}")
        return None

# ---------- è¾…åŠ©ï¼šå›ºå®šè§„åˆ™ç”Ÿæˆ JSON/CSV è·¯å¾„ ----------
def _sanitize_for_filename(s: str) -> str:
    if not s:
        return ""
    return re.sub(r"[^A-Za-z0-9_.+-]", "-", s)

def build_output_paths(log_dir: Path, run_id: str, mode: str) -> tuple[Path, Path]:
    ts = datetime.now().strftime("%Y%m%d-%H%M%S")
    tag = _sanitize_for_filename(RUN_TAG)
    stem = f"{ts}_{run_id}_{mode}" if not tag else f"{ts}_{tag}_{run_id}_{mode}"
    json_path = log_dir / f"{stem}.json"
    csv_path  = log_dir / f"{stem}.csv"
    return json_path, csv_path

# ---------- è¿è¡Œä¸»æµç¨‹ ----------
def main():
    global PROFILER

    # å›ºå®šç›®å½•ï¼šè‡ªåŠ¨åˆ›å»º
    LOG_DIR.mkdir(parents=True, exist_ok=True)

    # ä¸å†ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼›run_id å¯é™„å¸¦ RUN_TAG
    base_tag = RUN_TAG.strip() or None
    PROFILER = InferenceProfiler(run_name=base_tag)

    # åŸºç¡€ç³»ç»Ÿå¼€é”€æ”¶æ•›
    os.environ.setdefault("OMP_NUM_THREADS",  "8")
    os.environ.setdefault("MALLOC_ARENA_MAX", "2")
    # PYTORCH_CUDA_ALLOC_CONF å·²åœ¨é¡¶éƒ¨è®¾ç½®ï¼ˆå¿…é¡»åœ¨ import torch å‰ï¼‰

    # ============================================================
    # â­ å¼‚æ­¥æ»‘åŠ¨çª—å£ - RTX 5080 (16GB) + 125GB RAM å®æµ‹ä¼˜åŒ–
    # ============================================================
    # ç¡¬ä»¶å®¹é‡ï¼šGPU 12GB å¯ç”¨ â†’ 11 ç»„ | RAM 110GB å¯ç”¨ â†’ 60 å±‚
    # ç­–ç•¥ï¼šå¼‚æ­¥çª—å£ + é€‚åº¦å¹¶å‘ + RAM ç¼“å­˜ä¼˜åŒ–
    # ============================================================

    # â­â­â­ P0 ä¿®å¤: å¢åŠ  warmup å±‚æ•°ï¼Œç¡®ä¿å®Œæ•´ overlap
    # å•å±‚è®¡ç®— 100msï¼Œå¯ä»¥ overlap 4 ç»„ H2D (æ¯ç»„ 25ms)
    # Warmup è‡³å°‘éœ€è¦è¦†ç›–: åˆå§‹å±‚ + é¢„å–æ·±åº¦ = 12 å±‚
    GPU_AHEAD_LAYERS = 2# é¢„å– 6 ç»„ï¼ˆ3 å±‚ï¼‰- é€‚é… 11 ç»„å®¹é‡
    GPU_MAX_GROUPS   = 6
    GPU_WARMUP_LAYERS = 2# â­ 6 â†’ 12 å±‚ï¼ˆ24 ç»„ï¼‰ï¼Œç¡®ä¿å‰ 12 å±‚å®Œå…¨ overlap
    CPU_CACHE_LAYERS = 47# 
    DEFAULT_BATCH_SIZE  = int(os.getenv("PROMPT_BATCH", "32"))   # æ¨ç† batch size
    DEFAULT_MAX_GEN_LEN = int(os.getenv("GEN_TOKENS", "32"))    # æ¯ä¸ªæ ·æœ¬ç”Ÿæˆ token æ•°
    # âœ… å¤§ batch æ—¶ç¼©å° GPU group é¢„ç®—ï¼Œç»™æ¿€æ´»å’Œ KV è…¾æ˜¾å­˜
    if DEFAULT_BATCH_SIZE >= 64:
        GPU_MAX_GROUPS = 2
    elif DEFAULT_BATCH_SIZE >= 32:
        GPU_MAX_GROUPS = 4


    # === H2D å¹¶å‘æ§åˆ¶ï¼ˆâ­â­â­ P0 ä¼˜åŒ–ï¼šPCIe Gen5 + RTX 5080 é«˜å¸¦å®½é…ç½®ï¼‰ ===
    # PCIe Gen5 x16 å¸¦å®½: 64GB/s (Gen4çš„2å€)
    # 70Bæ¨¡å‹æ¯ç»„æƒé‡~1.5GB â†’ Gen5å•æ¬¡H2Dåªéœ€~25msï¼ˆGen4çš„ä¸€åŠï¼‰
    # æ›´é«˜å¸¦å®½æ„å‘³ç€éœ€è¦æ›´é«˜å¹¶å‘åº¦æ‰èƒ½é¥±å’ŒPCIeï¼Œé¿å…æµæ°´çº¿ç©ºéš™
    os.environ.setdefault("WSM_H2D_BASE_CONCURRENCY",  "8")   # â­ 5â†’16ï¼ˆGen5é«˜å¸¦å®½ï¼ŒåŸºç¡€å¹¶å‘ï¼‰
    os.environ.setdefault("WSM_H2D_PREFILL_MULT",      "3")  # Prefill: 32 å¹¶å‘
    os.environ.setdefault("WSM_H2D_DECODE_MULT",       "3")  # â­ 1.0â†’1.5ï¼ˆDecode: 24 å¹¶å‘ï¼‰
    os.environ.setdefault("WSM_MAX_INFLIGHT_GROUPS",   "32")   # â­ 16â†’32ï¼ˆInflight ä¸Šé™ï¼ŒåŒ¹é…å¹¶å‘ï¼‰
    os.environ.setdefault("WSM_H2D_GROUP_BACKLOG_MAX", "96")   # â­ 48â†’96ï¼ˆH2D é˜Ÿåˆ—ï¼ŒGen5éœ€è¦æ›´æ·±é˜Ÿåˆ—ï¼‰

    # === å¼‚æ­¥é€å‡ºæœºåˆ¶ ===
    os.environ.setdefault("WSM_EVICT_QUEUE_SIZE",      "96")   # é€å‡ºé˜Ÿåˆ—å®¹é‡
    os.environ.setdefault("WSM_BG_WORKERS",            "8")    # åå°çº¿ç¨‹æ± 

    # === GPU çª—å£é…ç½® ===
    os.environ.setdefault("WSM_GPU_MAX_GROUPS",        str(GPU_MAX_GROUPS))
    os.environ.setdefault("WSM_GPU_AHEAD_GROUPS",      str(GPU_AHEAD_LAYERS))

    # å•å±‚ 100ms å¯ overlap 4 ç»„ H2Dï¼Œè®¾ç½® 8 ä¿è¯å……è¶³æµæ°´çº¿
    os.environ.setdefault("WSM_GROUP_PREFETCH_DEPTH",  "4")  
    os.environ.setdefault("WSM_GPU_AHEAD",             str(GPU_AHEAD_LAYERS))
    os.environ.setdefault("WSM_GPU_BEHIND",            "2")    # ä¿ç•™æœ€è¿‘ 2 å±‚

    # === é¢„å–ç­–ç•¥ ===
    os.environ.setdefault("WSM_BALANCE_PREFETCH",      "1")
    os.environ.setdefault("WSM_PAIR_AHEAD",            "2")
    os.environ.setdefault("WSM_KIND_AHEAD_CAP",        "2")
    os.environ.setdefault("WSM_EVICT_FINISHED",        "1")    # å¯ç”¨å®Œæˆåé€å‡º
    os.environ.setdefault("WSM_CPU_EVICT_AFTER_USE",   "0")    # å¼‚æ­¥æ¨¡å¼ä¸‹ç¦ç”¨ç«‹å³é€å‡º

    # === è°ƒè¯•ä¸ç›‘æ§ ===
    os.environ.setdefault("WSM_GRP_RETAIN_MS",         "0")
    os.environ.setdefault("WSM_SKIP_PRELOAD_WAIT",     "1")    # å¯ç”¨å¼‚æ­¥é¢„åŠ è½½
    os.environ.setdefault("WSM_DEBUG_PREFETCH",        "1")    # å¯ç”¨è¯¦ç»†æ—¥å¿—
    os.environ.setdefault("WSM_VERBOSE_MISMATCH",      "0")    # ç”Ÿäº§ç¯å¢ƒå…³é—­

    # === CPU é¢„å–ä¼˜åŒ–ï¼ˆRAM å¯å®¹çº³ 60 å±‚ï¼‰ ===
    os.environ.setdefault("WSM_POOLED_CPU_READ",       "1")
    os.environ.setdefault("WSM_CPU_PF_WORKERS",        "12")   # CPU é¢„å–çº¿ç¨‹æ•°ï¼ˆ50% CPUï¼‰
    os.environ.setdefault("WSM_REBALANCE_SYNC",        "0")    # å¼‚æ­¥é‡å¹³è¡¡

    # === SSDâ†’CPU æµæ°´çº¿ ===
    os.environ.setdefault("WSM_CPU_PREFETCH_DISTANCE", str(CPU_CACHE_LAYERS))   
    os.environ.setdefault("WSM_SSD_CONCURRENCY",       "12")    # SSD å¹¶å‘è¯»å–

    # === Prefill ç‰¹å®šä¼˜åŒ– ===
    os.environ.setdefault("PREFILL_CPU_LAYERS",        str(CPU_CACHE_LAYERS))   # Prefill CPU ç¼“å­˜ 50 å±‚
    os.environ.setdefault("PREFILL_GPU_LAYERS",        str(GPU_WARMUP_LAYERS))  # â­ 6 â†’ 12 å±‚
    os.environ.setdefault("PREFILL_PREFETCH_DISTANCE", "16")   # â­ 10 â†’ 16ï¼ˆæ›´è¿œçš„é¢„å–è·ç¦»ï¼‰
    os.environ.setdefault("WSM_WARMUP_LAYERS_GPU",     str(GPU_WARMUP_LAYERS))  # â­ 6 â†’ 12 å±‚
    os.environ.setdefault("WSM_WRAPAROUND_WARMUP",     str(GPU_WARMUP_LAYERS))  # â­ 6 â†’ 12 å±‚
    
    # chunk & micro-batch å¤§å°ï¼ˆä¸å¹¶å‘åŒ¹é…ï¼‰
    os.environ.setdefault("PREFILL_T_CHUNK", str(CHUNK_SIZE)) 
    os.environ.setdefault("FFN_MICRO_B", str(MIRCO_BATCH_SIZE)) 
    os.environ.setdefault("ATTN_MICRO_B", str(ATTN_MICRO_B)) 


    # ============================================================
    # CPU çª—å£é¢å¤–é…ç½®ï¼ˆå¤ç”¨ä¸Šé¢çš„ CPU_CACHE_LAYERSï¼‰
    # ============================================================
    os.environ.setdefault("WSM_CPU_RING_MODE",     "1")
    os.environ.setdefault("WSM_CPU_RING_OFFSET",   "0")
    os.environ.setdefault("WSM_CPU_CACHE_LAYERS",  str(CPU_CACHE_LAYERS))
    os.environ.setdefault("WSM_CPU_CACHE_CAP_LAYERS", str(CPU_CACHE_LAYERS))
    os.environ.setdefault("WSM_CPU_CACHE_HWM_LAYERS", str(CPU_CACHE_LAYERS))
    os.environ.setdefault("WSM_CPU_CACHE_LWM_LAYERS", str(max(2, CPU_CACHE_LAYERS - 5)))
    os.environ.setdefault("WSM_CPU_BACK_MARGIN",   "1")
    os.environ.setdefault("WSM_KV_THROTTLE_THRESHOLD", "2")
    os.environ.setdefault("WSM_KV_THROTTLE_MS",        "16")

    # é…ç½®æ€»ç»“ï¼ˆä»…æ‰“å°ï¼‰
    print("=" * 80)
    print("ğŸš€ å¼‚æ­¥æ»‘åŠ¨çª—å£ - RTX 5080 (16GB) + 125GB RAM ä¼˜åŒ–é…ç½®")
    print("=" * 80)
    print(f"GPU é¢„å–æ·±åº¦:  {GPU_AHEAD_LAYERS} ç»„")
    print(f"GPU ç»„é¢„ç®—:    {GPU_MAX_GROUPS} ç»„ (æœ€å¤š ~9GB)")
    print(f"CPU ç¼“å­˜å®¹é‡:  {CPU_CACHE_LAYERS} å±‚ (~79.5GB)")
    print(f"H2D å¹¶å‘åº¦:    Prefill 24 | Decode 16")
    print(f"å¼‚æ­¥é€å‡ºé˜Ÿåˆ—:  64 ä»»åŠ¡")
    print(f"åå°çº¿ç¨‹æ± :    6 workers")
    print(f"CPU é¢„å–çº¿ç¨‹:  10 workers")
    print("=" * 80)
    print("âœ… å¼‚æ­¥çª—å£ç‰¹æ€§: é€å‡º/é¢„å–/CPUæ¨è¿› å…¨éƒ¨åœ¨åå°çº¿ç¨‹æ‰§è¡Œ")
    print("âœ… ä¸»çº¿ç¨‹çª—å£æ»‘åŠ¨å»¶è¿Ÿ: <1ms (vs åŒæ­¥æ¨¡å¼ ~20ms)")
    print("=" * 80)

    device = "cuda:0" if torch.cuda.is_available() else "cpu"

    # 1) è¦†ç›– pinned/æ³¨å†Œæ±  + KV æ± 
    with PROFILER.span("apply_runtime_overrides", "setup"):
        apply_runtime_overrides()
    with PROFILER.span("configure_kv_pool", "setup"):
        configure_kv_pool()
    with PROFILER.span("probe_after_runtime_clamp", "non_inference"):
        probe("after runtime clamp")

    # 2) WSMï¼ˆSSD æµå¼ï¼‰æ„é€ å‚æ•°
    PRIME_WINDOW = int(os.getenv("WSM_PRIME_WINDOW", "6"))  # ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤6
    mode_config = {
        "raw_device": RAW_DEV,
        "ssd_manifest_path": MANIFEST,
        "max_cached_layers": CPU_CACHE_LAYERS,         # âœ… ä¿®å¤: å¿…é¡»ä¸ CPU_CAP_VALUE ä¸€è‡´
        "cpu_cache_layers": CPU_CACHE_LAYERS,          # CPU ç¯å½¢å®¹é‡
        "warmup_layers": max(PRIME_WINDOW, GPU_AHEAD_LAYERS + 2),  # âœ… Fix: è‡³å°‘é¢„çƒ­ GPU_AHEAD + 2 å±‚
        "staging_mb": 64,
        "verbose": True,
        "gpu_max_groups": GPU_MAX_GROUPS,
    }

    # 3) æ„å»ºï¼ˆmeta + SSD æµå¼ï¼‰
    with PROFILER.span("probe_before_build", "non_inference"):
        probe("before LLaMA.build")
    with PROFILER.span("LLaMA.build", "setup"):
        llama = LLaMA.build(
            checkpoints_dir=CKPT_DIR,
            load_model=False,           # ä¸æŠŠ checkpoint å…¨è½½å…¥ CPU
            device=device,
            max_seq_len=4096,
            max_batch_size=32,
            topk_blk=8,
            mode="mixed",
            mode_config=mode_config
        )
        s = stream_mnt.get_streams("cuda:0")
        print("h2d_mha:", s.weight_h2d_mha)
        print("h2d_ffn:", s.weight_h2d_ffn)
        print("cmp_mha:", s.compute_mha)
        print("cmp_ffn:", s.compute_ffn)
        print("kv_h2d:", s.kv_h2d, "kv_d2h:", s.kv_d2h)
    with PROFILER.span("probe_after_build", "non_inference"):
        probe("after LLaMA.build")

    # ç»‘å®š WSM è¡¥ä¸ï¼šä»…ä¸º profiler è®¡æ—¶ï¼Œwait_group_ready çš„å¼‚æ­¥é€»è¾‘å·²åœ¨ WSM ä¸»ç±»å®ç°
    wsm = getattr(llama, "weight_streaming_manager", None)
    if wsm is not None:
        # åˆå§‹åŒ– pipeline watermark ç»Ÿè®¡å­—å…¸
        wsm._pipeline_watermark = {}

        # åŒ…è£… wait_group_ready ä»¥æ·»åŠ  profiler è®¡æ—¶
        _patch_wsm_for_profiling(wsm)
        original_wait = wsm.wait_group_ready
        wsm.wait_group_ready = types.MethodType(_wrap_wait_group_ready(original_wait), wsm)

        # ä¿ç•™ ensure_module_on_gpu çš„ CPU stub loader è¡¥ä¸
        wsm._ensure_module_on_gpu = types.MethodType(_patched_ensure_module_on_gpu, wsm)
        print("[WSM PATCH] Profiler wrapper + CPU stub loader enabled")

        # â­â­â­ P0 ä¼˜åŒ–ï¼šGPUçª—å£é¢„çƒ­ï¼ˆé¿å…å†·å¯åŠ¨ï¼Œå‰Nå±‚å¹¶è¡ŒH2Dï¼‰
        with PROFILER.span("gpu_window_warmup", "setup"):
            warmup_layers = GPU_WARMUP_LAYERS  # â­ ä½¿ç”¨é…ç½®çš„ 12 å±‚
            print(f"[WSM WARMUP] Preloading first {warmup_layers} layers to GPU...")
            for layer_idx in range(min(warmup_layers, wsm.n_layers)):
                try:
                    # å¼‚æ­¥é¢„å–attnå’Œffnç»„ï¼ˆä¸é˜»å¡ï¼Œè®©H2Dåœ¨åå°å¹¶è¡Œï¼‰
                    wsm.prefetch_group_async(layer_idx, "attn", reason="warmup")
                    wsm.prefetch_group_async(layer_idx, "ffn", reason="warmup")
                except Exception as e:
                    print(f"[WSM WARMUP] Layer {layer_idx} prefetch failed: {e}")
            print(f"[WSM WARMUP] Warmup requests sent (async), first {warmup_layers} layers (24 groups) will be ready before inference")

            # â­â­â­ é¢å¤–ä¿®å¤ï¼šç­‰å¾… warmup å®Œæˆåï¼Œç»§ç»­é¢„å–åç»­å±‚å»ºç«‹æµæ°´çº¿
            # åœ¨æ¨ç†å¼€å§‹å‰ï¼Œé¢„å– L12-L20ï¼Œç¡®ä¿ L12+ ä¹Ÿèƒ½ overlap
            # print(f"[WSM WARMUP] Extending prefetch pipeline to L{warmup_layers + 8}...")
            # for layer_idx in range(warmup_layers, min(warmup_layers + 8, wsm.n_layers)):
            #     try:
            #         wsm.prefetch_group_async(layer_idx, "attn", reason="warmup_extend")
            #         # ä¸é¢„å– ffnï¼ŒèŠ‚çœå¹¶å‘æ§½ä½
            #     except Exception as e:
            #         pass
            # print(f"[WSM WARMUP] Extended pipeline ready")

    PROFILER.wrap_model_forward(llama.model)

    # 4) è¯»å– prompt + å®‰å…¨è£å‰ªï¼ˆmax_gen_len=32ï¼‰
    batch_size = DEFAULT_BATCH_SIZE
    max_gen_len = DEFAULT_MAX_GEN_LEN

    with PROFILER.span("read_prompt_file", "prompt"):
        try:
            prompt_path = PROMPT_TXT
            file_content = prompt_path.read_text(encoding="utf-8").strip()

            # è§£æå¤šä¸ªpromptsï¼ˆæŒ‰ "===== PROMPT XXXX =====" åˆ†éš”ï¼‰
            import re
            prompt_blocks = re.split(r'=====\s*PROMPT\s+\d+\s+.*?=====\s*\n', file_content)
            # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
            prompt_blocks = [p.strip() for p in prompt_blocks if p.strip()]

            # å–å‰batch_sizeä¸ªprompts
            prompts = prompt_blocks[:batch_size]
            if len(prompts) < batch_size:
                # å¦‚æœpromptsä¸è¶³ï¼Œé‡å¤æœ€åä¸€ä¸ªpromptæ¥å¡«å……
                print(f"Warning: Only {len(prompts)} prompts found, padding to {batch_size}")
                while len(prompts) < batch_size:
                    prompts.append(prompts[-1])

            print(f"Loaded {len(prompts)} prompts for batch_size={batch_size}")

        except Exception as e:
            raise RuntimeError(f"æ— æ³•è¯»å– {prompt_path}: {e}")

    with PROFILER.span("tokenize_and_clip", "prompt"):
        max_prompt_tokens = llama.args.max_seq_len - max_gen_len

        # å¯¹æ¯ä¸ªpromptè¿›è¡Œtokenizeå’Œè£å‰ª
        clipped_prompts = []
        for prompt in prompts:
            tok = llama.tokenizer.encode(prompt, add_special_tokens=False)
            if len(tok) > max_prompt_tokens:
                tok = tok[-max_prompt_tokens:]
                prompt = llama.tokenizer.decode(tok)
            clipped_prompts.append(prompt)

        prompts = clipped_prompts
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªpromptçš„tokenæ•°ä½œä¸ºç»Ÿè®¡ï¼ˆå‡è®¾æ‰€æœ‰prompté•¿åº¦ç›¸ä¼¼ï¼‰
        tokens_in_count = len(llama.tokenizer.encode(prompts[0], add_special_tokens=False))

    # 5) çœŸæ­£æ¨ç†ï¼ˆdecodeï¼‰
    with PROFILER.span("probe_before_infer", "non_inference"):
        probe("before inference (decode)")
    with PROFILER.inference_scope():  # ç«¯åˆ°ç«¯æ¨ç†æ—¶é—´
        out_tokens, out_texts = llama.text_completion(
            prompts=prompts,
            temperature=0.0,
            max_gen_len=max_gen_len,
            batch_size=batch_size,
        )
    with PROFILER.span("probe_after_infer", "non_inference"):
        probe("after inference (decode)")

    # ==== ç»Ÿè®¡ tokens_out ====
    def _count_output_tokens(out_tokens_obj):
        try:
            if isinstance(out_tokens_obj, (list, tuple)):
                if len(out_tokens_obj) > 0 and isinstance(out_tokens_obj[0], (list, tuple)):
                    return len(out_tokens_obj[0])
                return len(out_tokens_obj)
            if torch.is_tensor(out_tokens_obj):
                return int(out_tokens_obj.numel())
        except Exception:
            pass
        return None

    tokens_out_count = _count_output_tokens(out_tokens)

    # ==== æ±‡æ€»ä¸ä¿å­˜ ====
    mode = classify_mode(llama)
    kv_stats = extract_kv_cache_stats(llama)

    # æ–°å¢ï¼šä» WSM å¯¹è±¡ä¸Šé‡‡é›†ä¸€æ¬¡è¿è¡ŒæœŸç»Ÿè®¡ï¼Œä¼ ç»™ Profiler
    wsm_runtime = None
    if hasattr(llama, "weight_streaming_manager"):
        try:
            wsm = llama.weight_streaming_manager
            rt = {}

            # pipeline æ°´ä½ï¼ˆæ¥è‡ª _wrap_wait_group_ready / _patch_wsm_for_profilingï¼‰
            pipe = getattr(wsm, "_pipeline_watermark", None)
            if isinstance(pipe, dict):
                rt["pipeline"] = dict(pipe)

            # SSD backend é™æ€ / è¿è¡ŒçŠ¶æ€
            if hasattr(wsm, "get_ssd_stats"):
                try:
                    rt["ssd"] = wsm.get_ssd_stats()
                except Exception:
                    pass

            # H2D timeout/retry ç»Ÿè®¡ï¼ˆå¦‚æœå®ç°äº†ï¼‰
            if hasattr(wsm, "get_h2d_timeout_stats"):
                try:
                    rt["h2d"] = wsm.get_h2d_timeout_stats()
                except Exception:
                    pass

            if rt:
                wsm_runtime = rt
        except Exception:
            wsm_runtime = None

    PROFILER.finalize(
        tokens_in=tokens_in_count,
        tokens_out=tokens_out_count,
        extra_meta={"llama_mode": mode, "device_str": str(device), "batch_size": batch_size},
        kv_stats=kv_stats or None,
        wsm_runtime=wsm_runtime,
    )


    # è‡ªåŠ¨ç”Ÿæˆ JSON/CSV è·¯å¾„å¹¶å„ä¿å­˜ä¸€æ¬¡
    json_path, csv_path = build_output_paths(LOG_DIR, PROFILER.run_id, mode)
    PROFILER.save(str(json_path))
    PROFILER.save(str(csv_path))
    print(f"[Profiler] JSON: {json_path}")
    print(f"[Profiler] CSV : {csv_path}")
    
    
    # æ§åˆ¶å°æ‘˜è¦ï¼Œæ–¹ä¾¿ç›´æ¥æŠ„åˆ°è®ºæ–‡è¡¨æ ¼
    summary = PROFILER.result
    t = summary.get("timings", {})
    kv = summary.get("kv_cache", {})
    print(
        "\n[SUMMARY] e2e_ms={e2e}, warmup_ms={warmup}, "
        "ftl_ms={ftl}".format(
            e2e=t.get("e2e_ms"),
            warmup=t.get("warmup_total_ms"),
            ftl=t.get("first_token_latency_ms"),
        )
    )
    if kv:
        print(
            "[SUMMARY] KV cache: hits={hits}, misses={misses}, "
            "evictions={evictions}, hit_ratio={ratio}".format(
                hits=kv.get("hits"),
                misses=kv.get("misses"),
                evictions=kv.get("evictions"),
                ratio=kv.get("hit_ratio"),
            )
        )
    else:
        print("[SUMMARY] KV cache: <no stats found on llama / kv_cache object>")

    # ==== è¾“å‡ºç”Ÿæˆæ–‡æœ¬ï¼ˆä¸å½±å“è®¡æ—¶ï¼‰====
    print(f"\n========== Generation (batch_size={batch_size}, len={max_gen_len}) ==========")
    # åªæ˜¾ç¤ºå‰3ä¸ªå’Œæœ€å1ä¸ªï¼Œé¿å…è¾“å‡ºå¤ªé•¿
    for i in [0, 1, 2, batch_size-1]:
        if i < len(out_texts):
            print(f"\n--- Batch {i} ---")
            print(out_texts[i][:200] + "..." if len(out_texts[i]) > 200 else out_texts[i])
    print("=========================================")

if __name__ == "__main__":
    main()
