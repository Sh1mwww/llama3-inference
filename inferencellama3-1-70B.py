#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäº test_70b_prefill_ssd.py çš„å®Œæ•´æ¨ç†è„šæœ¬ï¼š
- ç»´æŒç›¸åŒçš„è¿è¡Œæ—¶é…ç½®ï¼ˆWSM/SSD æµå¼æƒé‡ã€KV æ± ã€env ç­‰ï¼‰
- å°†ç”Ÿæˆé•¿åº¦æ”¹ä¸º max_gen_len=32ï¼Œå¹¶çœŸæ­£ decode è¾“å‡ºæ–‡æœ¬
- æŒ‰éœ€æ±‚ï¼šåœ¨è®¡ç®—ç¬¬ i å±‚æ—¶ï¼Œä¿è¯ i+1..i+4 çš„ç»„çº§æƒé‡å·²åœ¨ GPUï¼ˆäº‹ä»¶å°±ç»ªï¼Œæ— é˜»å¡ç­‰å¾…ï¼‰ï¼›
  DRAM ä¾§ç»´æŒ i+4 .. i+4+cap çš„ç¯å½¢çª—å£ï¼ˆå¯¹ 80 å±‚å–æ¨¡ï¼‰
"""

import os
from pathlib import Path
import torch

# ===== é¡¹ç›®å†…æ¨¡å— =====
from llama3.generator import LLaMA
from llama3.config import KVCacheArgs, load_runtime_config, runtime_config_to_dict
from llama3 import generator as _gen

# ========== build() åŒ…è£…ï¼šåªåŠ æ—¥å¿—ï¼Œä¸æ”¹åº“ ==========
_orig_build = _gen.LLaMA.build

def _debug_build(*args, **kw):
    mode       = kw.get("mode", None)
    load_model = kw.get("load_model", None)
    mode_cfg   = (kw.get("mode_config", {}) or {})
    raw_dev    = mode_cfg.get("raw_device")
    manifest   = mode_cfg.get("manifest_path") or mode_cfg.get("ssd_manifest_path")

    print(f"[MODE-DECISION] LLaMA.build(mode={mode}, load_model={load_model})")
    use_raw_ssd = (mode in {"ssd", "mixed"}) or (mode_cfg.get("weight_source") == "raw-ssd")
    print(f"[MODE-DECISION] use_raw_ssd={use_raw_ssd} raw_device={raw_dev} manifest={manifest}")

    llama = _orig_build(*args, **kw)

    has_wsm = hasattr(llama, "weight_streaming_manager")
    if has_wsm:
        wsm = llama.weight_streaming_manager
        ssd = bool(getattr(wsm, "ssd_enabled", False) or getattr(wsm, "ssd", None))
        print(f"[MODE-DECISION] built: WSM present, ssd_enabled={ssd}")
    else:
        print("[MODE-DECISION] built: NO WSM (å¯èƒ½æ˜¯ full-cpu/full-gpu/æ—§ streaming)")
    return llama

_gen.LLaMA.build = staticmethod(_debug_build)

# ===== WSM runtime monkey-patch: strict ready + CPU stub loader =====
import types

def _patched_wait_group_ready(self, layer_idx: int, group: str, compute_stream=None):
    """
    ç­‰å¾… (layer_idx, group) ç»„å°±ç»ªï¼›äº‹ä»¶ç»“æŸå**äºŒæ¬¡æ ¡éªŒ**æ˜¯å¦çœŸåœ¨ GPUã€‚
    è‹¥ä»ä¸åœ¨ï¼Œåˆ™å¼ºåˆ¶åŒæ­¥ ensure_group_on_gpu()ã€‚
    """
    kind = 'attn' if group == 'attn' else 'ffn'
    key  = (int(layer_idx), kind)

    # 0) å¿«è·¯å¾„ï¼šå·²é©»ç•™
    try:
        if self._group_is_resident(*key):
            return
    except Exception:
        pass

    # 1) è‹¥æœ‰ inflight äº‹ä»¶ï¼šç­‰å¾…
    evt = self._gpu_group_inflight.get(key)
    if evt is not None:
        if compute_stream is not None:
            # å…¼å®¹ threading.Event / torch.cuda.Event
            try:
                if hasattr(evt, "wait"):  # threading.Event
                    evt.wait()
                else:
                    compute_stream.wait_event(evt)
            except Exception:
                try:
                    evt.synchronize()
                except Exception:
                    pass
        else:
            try:
                if hasattr(evt, "wait"):
                    evt.wait()
                else:
                    evt.synchronize()
            except Exception:
                pass

        # ä» inflight è½¬å¸¸é©»
        with self._group_lock:
            self._gpu_group_inflight.pop(key, None)
            if key not in self._gpu_group_lru:
                self._gpu_group_lru.append(key)
        if getattr(self, "verbose", False):
            print(f"[WSM] H2D completed for {key}")

        # â˜… å…³é”®ï¼šäº‹ä»¶å®Œæˆåå†æ¬¡æ ¡éªŒï¼›ä¸åœ¨å°±åŒæ­¥å…œåº•æ¬è¿
        if not self._group_is_resident(*key, wait_for_event=True):
            if getattr(self, "verbose", False):
                print(f"[WSM] Ready event done but {key} not resident; forcing sync ensure")
            self.ensure_group_on_gpu(layer_idx, kind)
        return

    # 2) è‹¥åªè®°å½•äº† CUDA äº‹ä»¶ï¼šæŠŠ compute_stream æŒ‚åˆ°äº‹ä»¶ä¸Š
    cuda_evt = self._group_ready_events.get(key)
    if cuda_evt is not None:
        try:
            dev_obj = self.device if isinstance(self.device, torch.device) else torch.device(self.device)
            s = compute_stream or torch.cuda.current_stream(dev_obj)
            s.wait_event(cuda_evt)
        except Exception:
            try:
                cuda_evt.synchronize()
            except Exception:
                pass

        # å†æ¬¡æ ¡éªŒ
        if not self._group_is_resident(*key, wait_for_event=True):
            if getattr(self, "verbose", False):
                print(f"[WSM] CUDA event existed but {key} not resident; forcing sync ensure")
            self.ensure_group_on_gpu(layer_idx, kind)
        return

    # 3) æ²¡æœ‰ä»»ä½•å¯ç­‰å¾…å¯¹è±¡ï¼šç›´æ¥å…œåº•åŒæ­¥åŠ è½½
    self.ensure_group_on_gpu(layer_idx, kind)


def _patched_ensure_module_on_gpu(self, m: torch.nn.Module, layer_idx: int | None = None, module_name: str | None = None):
    """
    æ‰©å±•ï¼šæŠŠ **0-size CPU stub** å½“ä½œ meta ä¸€æ ·å¤„ç†ï¼Œä¼˜å…ˆä» CPU cache å–å›å¹¶ä¸Šå¡ã€‚
    å…¶å®ƒæƒ…å†µä»å¤ç”¨åŸå…ˆçš„ _ensure_param_on_gpu() è·¯å¾„ã€‚
    """
    params_to_replace = {}
    params_full_names = {}

    def _full_name(layer_idx: int, module_name: str, local_param_name: str) -> str:
        if module_name in ("wq", "wk", "wv", "wo"):
            parent = "attention"
        elif module_name in ("w1", "w2", "w3"):
            parent = "feed_forward"
        else:
            parent = module_name or ""
        return f"layers.{layer_idx}.{parent}.{module_name}.{local_param_name}" if parent else f"layers.{layer_idx}.{module_name}.{local_param_name}"

    def _fetch_from_cpu_cache(name: str):
        if (layer_idx is not None) and (layer_idx in self.cpu_cache):
            return self.cpu_cache[layer_idx].get(name)
        return None

    for local_param_name, p in m.named_parameters(recurse=False):
        full_name = None
        if (layer_idx is not None) and (module_name is not None):
            full_name = _full_name(layer_idx, module_name, local_param_name)

        is_meta     = (p.device.type == "meta") or getattr(p, "is_meta", False)
        is_cpu_stub = (p.device.type == "cpu")  and (p.numel() == 0)

        if (is_meta or is_cpu_stub) and self.ssd_enabled and full_name:
            # ç¡®ä¿æœ¬å±‚å·²æœ‰ CPU cacheï¼ˆæ²¡æœ‰å°±ç«‹å³åŠ è½½ï¼‰
            if (layer_idx not in self.cpu_cache):
                try:
                    self._load_layer_to_cpu(int(layer_idx))
                except Exception:
                    pass

            cached = _fetch_from_cpu_cache(full_name)
            # å½¢çŠ¶ä¿®å¤ï¼šè‹¥ cache çš„ key ä¸æœŸæœ› shape ä¸é…ï¼Œå°è¯•åŒæ—åˆ«å
            expected = tuple(getattr(getattr(m, local_param_name), "shape", ()))
            chosen_name, chosen_tensor = None, None

            def _try_pick(names: list[str]):
                nonlocal chosen_name, chosen_tensor
                for nm in names:
                    t = _fetch_from_cpu_cache(nm)
                    if t is not None and (not expected or tuple(t.shape) == expected):
                        chosen_name, chosen_tensor = nm, t
                        break

            if cached is not None and (not expected or tuple(cached.shape) == expected):
                chosen_name, chosen_tensor = full_name, cached
            else:
                cand = []
                if module_name in ("wq", "wk", "wv"):
                    cand = [f"layers.{layer_idx}.attention.{x}.{local_param_name}" for x in ("wq","wk","wv")]
                elif module_name in ("w1", "w2", "w3"):
                    cand = [f"layers.{layer_idx}.feed_forward.{x}.{local_param_name}" for x in ("w1","w2","w3")]
                else:
                    cand = [full_name]
                _try_pick(cand)
                if chosen_tensor is None and cached is not None:
                    chosen_name, chosen_tensor = full_name, cached  # é€€è€Œæ±‚å…¶æ¬¡

            if chosen_tensor is not None:
                with torch.cuda.stream(self._select_h2d_stream_for(module_name=module_name)):
                    p_gpu = chosen_tensor.to(self.device, non_blocking=True)
                params_to_replace[local_param_name] = torch.nn.Parameter(p_gpu, requires_grad=p.requires_grad)
                params_full_names[local_param_name] = chosen_name or full_name
                if getattr(self, "verbose", False):
                    print(f"[WSM DEBUG] âœ“ Loaded {'meta' if is_meta else 'stub'} param {params_full_names[local_param_name]} to GPU: {tuple(p_gpu.shape)}")
            else:
                if getattr(self, "verbose", False):
                    print(f"[WSM WARN] CPU cache miss for {full_name} (layer {layer_idx}); will rely on ensure_group_on_gpu() later")
            continue  # è¯¥å‚æ•°å¤„ç†å®Œæ¯•

        # å…¶å®ƒæƒ…å†µï¼šæ²¿ç”¨åŸæ¥çš„ CPUâ†’GPU é€»è¾‘
        self._ensure_param_on_gpu(p, layer_idx, full_name)

    # å®‰è£…æ›¿æ¢åçš„ Parameterï¼Œå¹¶ç»´æŠ¤ name æ˜ å°„
    for pname, new_param in params_to_replace.items():
        m._parameters[pname] = new_param
        full = params_full_names.get(pname)
        if full:
            try:
                pobj = getattr(m, pname)
            except Exception:
                pobj = new_param
            self.name_to_param[full] = pobj
            self.param_owner[full]   = (m, pname)

    # buffer ç»´æŒåŸæœ‰ç­–ç•¥ï¼šmetaâ†’materializeï¼ŒCPUâ†’ä¸Šå¡
    for b in m.buffers(recurse=True):
        if getattr(b, "is_meta", False):
            try:
                b = b.to_empty(device=self.device)
            except Exception:
                pass
        elif b.device.type == "cpu":
            with torch.cuda.stream(self._select_h2d_stream_for(module_name=module_name)):
                b_gpu = b.detach().to(self.device, non_blocking=True)
            try:
                b.data = b_gpu
            except Exception:
                pass

# ===== è·¯å¾„ä¸å¸¸é‡ï¼ˆæŒ‰ä½ çš„ç¯å¢ƒï¼‰ =====
PROMPT_TXT = Path("/home/roger/llama3-inference/prompts/prompts_batch512_len2048.txt")
RAW_DEV    = "/dev/nvme0n1p4"
MANIFEST   = "/data1/70b-fixed.runtime_manifest.json"
CKPT_DIR   = "/home/roger/.llama/checkpoints/Llama3.1-70B"

# ---------- ç³»ç»Ÿ/GPU å†…å­˜å¿«ç…§ ----------
def _read_status():
    def _grep(path, keys):
        out = {}
        try:
            with open(path, "r") as f:
                for line in f:
                    for k in keys:
                        if line.startswith(k + ":"):
                            out[k] = line.split(":")[1].strip()
        except Exception:
            pass
        return out
    s = _grep("/proc/self/status", ["VmRSS","VmHWM","VmLck"])
    m = _grep("/proc/meminfo", ["MemAvailable","CommitLimit","Committed_AS","Cached","Buffers"])
    return s, m

def _gpu_mem():
    if not torch.cuda.is_available():
        return {}
    dev = torch.cuda.current_device()
    st  = torch.cuda.memory_stats(dev)
    return {
        "alloc_GB": st.get("allocated_bytes.all.current", 0)/(1<<30),
        "rsrv_GB":  st.get("reserved_bytes.all.current", 0)/(1<<30),
    }

def probe(stage: str):
    s, m = _read_status()
    g    = _gpu_mem()
    print(f"\n[MEM] {stage}")
    print(f"  VmRSS={s.get('VmRSS','?')}  VmLck(pinned)={s.get('VmLck','?')}  "
          f"CommitLimit={m.get('CommitLimit','?')}  Committed_AS={m.get('Committed_AS','?')}  "
          f"MemAvailable={m.get('MemAvailable','?')}")
    if g:
        print(f"  GPU: allocated={g['alloc_GB']:.2f} GiB  reserved={g['rsrv_GB']:.2f} GiB")
    print()

# ---------- æ‰«æå‚æ•°åœ¨ meta/cpu/cuda ä¸Šçš„å ç”¨ ----------
def dump_param_inventory(model, tag):
    buckets = {"cpu":0, "cuda":0, "meta":0, "other":0}
    big_cpu = []
    for n,p in model.named_parameters(recurse=True):
        b = p.numel() * p.element_size()
        if getattr(p, "is_meta", False):
            buckets["meta"] += b
        elif hasattr(p, "device"):
            t = p.device.type
            if t == "cpu":
                buckets["cpu"] += b
                if b >= (64<<20):
                    big_cpu.append((n,b))
            elif t == "cuda":
                buckets["cuda"] += b
            else:
                buckets["other"] += b
        else:
            buckets["other"] += b
    f = lambda x: f"{x/(1<<30):.2f} GiB"
    print(f"[PARAMS] {tag}: cpu={f(buckets['cpu'])}, cuda={f(buckets['cuda'])}, meta={f(buckets['meta'])}, other={f(buckets['other'])}")
    if big_cpu:
        big_cpu.sort(key=lambda x:-x[1])
        print("  [big-cpu] top:")
        for n,b in big_cpu[:10]:
            print(f"   - {n}  {b/(1<<20):.1f} MiB")

# ---------- è¿è¡Œæ—¶è¦†ç›–ï¼šæ”¶æ•› pinned/æ³¨å†Œæ±  ----------
def apply_runtime_overrides():
    """
    æŠŠæ³¨å†Œæ€»é‡é’³åœ¨ â‰¤256MiBï¼Œå¹¶æŠŠ EXTENT_BYTES é™åˆ° 1MiBï¼Œé™ä½é«˜é˜¶é¡µ order å‹åŠ›ã€‚
    """
    cfg = load_runtime_config({
        "pinned": {
            "WEIGHT_PINNED_BYTES":      8  << 30,
            "KV_PINNED_BYTES":          6  << 30,
            "EXTENT_BYTES":             1  << 20,   # 1MiB
            "PINNED_REGISTER_CHUNK":   16  << 20,   # 16MiB
            "PINNED_REGISTER_N":            8,      # 128MiB
        },
        "regpool": {
            "REG_POOL_N_BUFFERS":           8,
            "REG_POOL_BUF_BYTES":     16 << 20,     # ~128MiB ä¼ é€å¸¦
        },
        "io": {
            "RAW_IO_QD_WRITE":             24,      # å†™é˜Ÿåˆ—æ·±åº¦
            "IO_RAW_THROTTLE_MS":          30,      # å†™å¸¦å®½çª—å£
        }
    })
    D = runtime_config_to_dict(cfg)
    p = D["pinned"]
    need  = int(p["WEIGHT_PINNED_BYTES"])
    chunk = int(p["PINNED_REGISTER_CHUNK"])
    target_total = min(need // 2, 256 << 20)  # ç›®æ ‡ â‰¤ 256MiB
    newN = max(1, target_total // chunk)
    p["PINNED_REGISTER_N"] = newN
    cfg = load_runtime_config({"pinned": p, "io": D["io"]})
    print("[RuntimeConfig] pinned =", runtime_config_to_dict(cfg)["pinned"])
    print("[RuntimeConfig] io =", runtime_config_to_dict(cfg)["io"])
    return cfg

# ---------- KV æ± ï¼šæ‡’åˆ†é… + å•å— â‰¥ å•ä¸ª KV å— ----------
def configure_kv_pool():
    # DRAM é…ç½®
    KVCacheArgs.dram_limit_gb     = 24.0
    KVCacheArgs.dram_sizing_batch = 32
    KVCacheArgs.block_bytes       = 4 * 1024 * 1024
    KVCacheArgs.preallocate       = False
    KVCacheArgs.lazy_init         = True

    # å…³é—­ push å³æ—¶é•œåƒï¼Œé‡‡ç”¨åç§»/èšåˆå†™ï¼ˆé¿å…ä¸æƒé‡ H2D å†²çªï¼‰
    KVCacheArgs.mirror_on_push = False

    # I/O èŠ‚æµä¸å†™é€Ÿç‡é…ç½®ï¼ˆä¸æƒé‡ H2D ä»²è£ï¼‰
    KVCacheArgs.IO_RAW_THROTTLE_MS     = 30
    KVCacheArgs.NVME_WRITE_TARGET_MBPS = 1200

    if hasattr(KVCacheArgs, "prefer_bf16"):
        KVCacheArgs.prefer_bf16 = True

    print(f"[KVArgs] dram_limit={KVCacheArgs.dram_limit_gb} GiB, "
          f"block_bytes={KVCacheArgs.block_bytes//(1<<20)} MiB, prealloc={KVCacheArgs.preallocate}")
    print(f"[KVArgs] mirror_on_push={KVCacheArgs.mirror_on_push}, "
          f"IO_RAW_THROTTLE_MS={KVCacheArgs.IO_RAW_THROTTLE_MS}, "
          f"NVME_WRITE_TARGET_MBPS={KVCacheArgs.NVME_WRITE_TARGET_MBPS}")

# ---------- è¯†åˆ«â€œå®é™…è¿è¡Œçš„æ¨¡å¼â€ ----------
def classify_mode(llama) -> str:
    """
    è¿”å›ï¼š'ssd-streaming' / 'cpu-gpu-streaming' / 'full-gpu' / 'full-cpu' / 'meta-only'
    å¹¶æ‰“å°åˆ¤æ®ï¼Œæ–¹ä¾¿ç¡®è®¤ç°åœ¨åˆ°åº•è·‘çš„æ˜¯ä»€ä¹ˆã€‚
    """
    m = llama.model
    # 1) æ˜¯å¦è£…äº† WSMï¼ˆå¹¶ä¸”å¸¦ SSDï¼‰
    if hasattr(llama, "weight_streaming_manager"):
        wsm = llama.weight_streaming_manager
        ssd = bool(getattr(wsm, "ssd_enabled", False) or getattr(wsm, "ssd", None))
        cpu_warm = getattr(wsm, "disable_cpu_warm", None)
        mode = "ssd-streaming" if ssd else "cpu-gpu-streaming"
        print(f"[MODE] detected={mode}  (has WSM, ssd={ssd}, disable_cpu_warm={cpu_warm})")
        return mode
    # 2) æ—  WSMï¼šçœ‹å‚æ•°åˆ†å¸ƒ
    cpu, cuda, meta = 0,0,0
    for _,p in m.named_parameters():
        b = p.numel()*p.element_size()
        if getattr(p, "is_meta", False): meta += b
        elif p.device.type == "cpu":     cpu  += b
        elif p.device.type == "cuda":    cuda += b
    if cuda > 0 and cpu == 0 and meta == 0:
        print("[MODE] detected=full-gpu"); return "full-gpu"
    if cpu  > 0 and cuda == 0 and meta == 0:
        print("[MODE] detected=full-cpu"); return "full-cpu"
    if meta > 0 and cpu == 0 and cuda == 0:
        print("[MODE] detected=meta-only"); return "meta-only"
    print("[MODE] mixed/unrecognized (check PARAMS dump below)")
    return "unknown"

def main():
    # åŸºç¡€ç³»ç»Ÿå¼€é”€æ”¶æ•›
    os.environ.setdefault("OMP_NUM_THREADS",  "8")
    os.environ.setdefault("MALLOC_ARENA_MAX", "2")

    # ============================================================
    # â­ ç»„çº§ GPU é¢„å–ï¼ˆahead=4ï¼‰+ ç»„é¢„ç®— + ç­‰æ°´ä½è°ƒåº¦
    # ============================================================
    GPU_AHEAD_LAYERS = 4
    # CRITICAL: Reduced from 11 to 6 to reserve ~2-3GB for activation tensors during prefill
    # 70B model: each group ~400-500MB, 6 groups = ~3GB weights, leaving ~12GB for activations
    GPU_MAX_GROUPS   = 6  # Reduced to prevent OOM during long-sequence prefill

    os.environ.setdefault("WSM_GPU_MAX_GROUPS",        str(GPU_MAX_GROUPS))
    os.environ.setdefault("WSM_GROUP_PREFETCH_DEPTH",  str(GPU_AHEAD_LAYERS))
    os.environ.setdefault("WSM_GPU_AHEAD",             str(GPU_AHEAD_LAYERS))  # ä¾› WSM è¯»å–
    os.environ.setdefault("WSM_BALANCE_PREFETCH",      "1")
    os.environ.setdefault("WSM_PAIR_AHEAD",            "2")  # (i+1..i+2).ffn é¡¶è¡¥
    os.environ.setdefault("WSM_KIND_AHEAD_CAP",        "2")
    os.environ.setdefault("WSM_H2D_GROUP_BACKLOG_MAX", "4")

    # è®¡ç®—ç»“æŸç«‹åˆ»é‡Šæ”¾ï¼ˆé¿å…ç»„å †ç§¯ï¼‰
    os.environ.setdefault("WSM_EVICT_FINISHED", "1")  # â† ä¿®æ­£ä¸º 1ï¼ˆä½ çš„è‰ç¨¿é‡Œè¯¯å†™æˆäº† 0ï¼‰
    os.environ.setdefault("WSM_GRP_RETAIN_MS", "3")   # æçŸ­ä¿ç•™çª—å£

    # è·³è¿‡é¢„åŠ è½½ç­‰å¾…ï¼šè¾¹è·‘è¾¹æ»šåŠ¨é¢„å–
    os.environ.setdefault("WSM_SKIP_PRELOAD_WAIT", "1")

    # ============================================================
    # â­ ç¯å½¢ CPU çª—å£ï¼ˆSSD -> pinned DRAMï¼Œ80 å±‚å–æ¨¡ï¼‰
    # ============================================================
    # CRITICAL FIX: CPUçª—å£å¿…é¡»ä» i+1 å¼€å§‹ä»¥è¦†ç›–GPUé¢„å–éœ€è¦çš„å±‚ (i+1..i+4)
    # å¦‚æœoffset=4ï¼Œåˆ™çª—å£æ˜¯[i+4..i+43]ï¼ŒGPUéœ€è¦çš„i+1,i+2,i+3ä¸åœ¨çª—å£å†…ï¼
    CPU_CAP_VALUE    = 40   # çª—å£å¤§å°ï¼š40å±‚
    CPU_RING_OFFSET  = 1    # çª—å£ä» i+1 èµ·ï¼Œç¡®ä¿GPUé¢„å–çš„i+1..i+4éƒ½åœ¨DRAMä¸­
    os.environ.setdefault("WSM_CPU_RING_MODE",     "1")
    os.environ.setdefault("WSM_CPU_RING_OFFSET",   str(CPU_RING_OFFSET))
    os.environ.setdefault("WSM_CPU_CACHE_CAP_LAYERS", str(CPU_CAP_VALUE))
    os.environ.setdefault("WSM_CPU_CACHE_HWM_LAYERS", str(CPU_CAP_VALUE + 3))
    os.environ.setdefault("WSM_CPU_CACHE_LWM_LAYERS", str(max(2, CPU_CAP_VALUE - 3)))
    os.environ.setdefault("WSM_CPU_BACK_MARGIN",   "4")

    # â€”â€” H2D/KV ä¼ è¾“ä»²è£ï¼ˆé˜²æ­¢ä¸¤è¾¹æŠ¢å¸¦å®½ï¼‰â€”â€”
    os.environ.setdefault("WSM_KV_THROTTLE_THRESHOLD", "2")
    os.environ.setdefault("WSM_KV_THROTTLE_MS",        "16")

    # é…ç½®æ€»ç»“
    print("=" * 80)
    print("ğŸ”§ ç»„çº§ GPU é¢„å–ï¼ˆahead=4ï¼‰+ ç¯å½¢ CPU çª—å£ [FIXED VERSION]")
    print("=" * 80)
    print(f"GPU é¢„å–è·ç¦»: {GPU_AHEAD_LAYERS} å±‚ (é¢„å– i+1..i+{GPU_AHEAD_LAYERS})")
    print(f"GPU ç»„é¢„ç®—:   {GPU_MAX_GROUPS} ç»„(attn/ffn)")
    print(f"CPU çª—å£å®¹é‡: {CPU_CAP_VALUE} å±‚ (ç¯å½¢ï¼Œå¯¹ 80 å±‚å–æ¨¡)")
    print(f"CPU ç¯å½¢åç§»: i+{CPU_RING_OFFSET} â­ CRITICAL: å¿…é¡»è¦†ç›–GPUé¢„å–å±‚")
    print(f"CPU çª—å£èŒƒå›´: [i+{CPU_RING_OFFSET} .. i+{CPU_RING_OFFSET + CPU_CAP_VALUE - 1}]")
    print("=" * 80)
    print(f"âš ï¸  IMPORTANT: å¦‚æœçœ‹åˆ°æ­¤æ¶ˆæ¯ä½†offset={CPU_RING_OFFSET}ï¼Œè¯´æ˜é…ç½®å·²æ­£ç¡®ï¼")
    print(f"âš ï¸  å¦‚æœä»æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥WSMæ˜¯å¦çœŸçš„åŠ è½½äº†æ–°ä»£ç ")
    print("=" * 80)

    device = "cuda:0" if torch.cuda.is_available() else "cpu"

    # 1) è¦†ç›– pinned/æ³¨å†Œæ±  + KV æ± 
    apply_runtime_overrides()
    configure_kv_pool()
    probe("after runtime clamp")

    # 2) WSMï¼ˆSSD æµå¼ï¼‰æ„é€ å‚æ•°ï¼šå…³é—­æ•´å±‚é¢„å–ï¼Œæ”¹ç”¨ç»„çº§çª—å£
    mode_config = {
        "raw_device": RAW_DEV,
        "ssd_manifest_path": MANIFEST,
        "prefetch_distance": 0,                     # å…³é—­æ•´å±‚é¢„å–
        "group_prefetch_depth": GPU_AHEAD_LAYERS,   # ç»„çº§é¢„å–æ·±åº¦ï¼ˆ=4ï¼‰
        "max_cached_layers": 8,                     # ç»„çº§èµ·ä¸»å¯¼ï¼Œè¿™é‡Œä»…ä½œä¿é™©
        "cpu_cache_layers": CPU_CAP_VALUE,          # CPU ç¯å½¢å®¹é‡
        "warmup_layers": 1,                         # è‡³å°‘é¢„çƒ­ç¬¬ 0 å±‚åˆ° CPU
        "staging_mb": 64,
        "verbose": True,
    }

    # 3) æ„å»ºï¼ˆmeta + SSD æµå¼ï¼‰ï¼Œä¸ä¼šæŠŠ 70B æƒé‡å…¨è½½å…¥ CPU
    probe("before LLaMA.build")
    print("[CHECK] calling LLaMA.build(mode='mixed', load_model=False)")
    llama = LLaMA.build(
        checkpoints_dir=CKPT_DIR,
        load_model=False,           # å…³é”®ï¼šä¸æŠŠ checkpoint è½½å…¥ CPU
        device=device,
        max_seq_len=2048,
        max_batch_size=32,
        topk_blk=8,
        mode="mixed",
        mode_config=mode_config
    )
    probe("after LLaMA.build")

    # ç»‘å®š WSM è¡¥ä¸
    wsm = getattr(llama, "weight_streaming_manager", None)
    if wsm is not None:
        wsm.wait_group_ready     = types.MethodType(_patched_wait_group_ready, wsm)
        wsm._ensure_module_on_gpu = types.MethodType(_patched_ensure_module_on_gpu, wsm)
        print("[WSM PATCH] strict group-ready + CPU stub loader enabled")

    # è¯†åˆ«/æ‰“å°"å®é™…æ¨¡å¼" + å‚æ•°åˆ†å¸ƒ
    mode = classify_mode(llama)
    dump_param_inventory(llama.model, f"after build ({mode})")

    # 4) è¯»å– prompt å¹¶åšâ€œå®‰å…¨è£å‰ªâ€ï¼ˆmax_gen_len=32ï¼‰
    try:
        prompt_path = PROMPT_TXT
        prompt = prompt_path.read_text(encoding="utf-8").strip()
    except Exception as e:
        raise RuntimeError(f"æ— æ³•è¯»å– {prompt_path}: {e}")

    # â€”â€” å®‰å…¨è£å‰ªï¼šæŒ‰ tokenizer é™åˆ¶ prompt token æ•°
    max_gen_len = 32  
    max_prompt_tokens = llama.args.max_seq_len - max_gen_len
    tok = llama.tokenizer.encode(prompt, add_special_tokens=False)
    if len(tok) > max_prompt_tokens:
        tok = tok[-max_prompt_tokens:]
        prompt = llama.tokenizer.decode(tok)

    # 5) çœŸæ­£æ¨ç†ï¼ˆdecodeï¼‰
    probe("before inference (decode)")
    out_tokens, out_texts = llama.text_completion(
        prompts=[prompt],
        temperature=0.0,
        max_gen_len=max_gen_len,
        batch_size=1,
    )
    probe("after inference (decode)")

    print(f"\n========== Generation (len={max_gen_len}) ==========")
    print(out_texts[0])
    print("=========================================")

if __name__ == "__main__":
    main()
