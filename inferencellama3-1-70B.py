#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Llama3.1-70B æ¨ç† + è½»é‡çº§ Profilerï¼ˆJSON/CSV è‡ªåŠ¨å†™å…¥å›ºå®šç›®å½•ï¼‰
- è®°å½•ä¼šå½±å“ inference çš„å…³é”®è·¯å¾„ç”¨æ—¶ï¼ˆprefill / decode per-token / e2e / FTL è¿‘ä¼¼ / ååï¼‰
- åŒæ—¶è®°å½•ä¸ä¼šå½±å“ inference çš„å‡†å¤‡/æ¢é’ˆ/æ—¥å¿—æ—¶é—´ï¼ˆnon_inference ç±»åˆ«ï¼‰
- å¯¹ WSM ä¸¤ä¸ªå…³é”®å‡½æ•°ï¼ˆwait_group_ready / _ensure_module_on_gpuï¼‰åšåŸ‹ç‚¹ç»Ÿè®¡
- é‡‡ç”¨ CUDA Events é€ token è®¡æ—¶ï¼Œç»Ÿä¸€åŒæ­¥ï¼Œå°½é‡ä½æ‰°åŠ¨
- ç”Ÿæˆç»“æœå›ºå®šå†™å…¥ LOG_DIRï¼Œè‡ªåŠ¨è¾“å‡º JSON + CSV ä¸¤ç§æ ¼å¼
"""

import os
from pathlib import Path
import types
from typing import Any, Dict, List, Optional
import json, csv, uuid, platform, math, time, re
from datetime import datetime, timezone
from contextlib import contextmanager, nullcontext

# ğŸ”¥ CUDA å†…å­˜åˆ†é…å™¨é…ç½®ï¼ˆå¿…é¡»åœ¨ import torch ä¹‹å‰ï¼‰
# expandable_segments ä¸å¼‚æ­¥æµæ“ä½œå¯èƒ½æœ‰å†²çªï¼Œæš‚æ—¶ç¦ç”¨
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"  # é™åˆ¶åˆ†å—å¤§å°ï¼Œå‡å°‘ç¢ç‰‡

# ğŸ”¥ WSM æ— å…œåº•ç­–ç•¥ï¼šé”å®šäº‹ä»¶é©±åŠ¨è°ƒåº¦ï¼Œç¦ç”¨åŒæ­¥å…œåº• (no-fallback)
os.environ["WSM_NO_FALLBACK"] = "1"

# ğŸ”¥ å¯ç”¨å±‚çº§æ€§èƒ½ profilingï¼ˆCUDA timer ç»Ÿè®¡ attn/ffn/kv_fetch ç­‰è¯¦ç»†æ—¶é—´ï¼‰
os.environ["LLM_PROFILE"] = "1"

import torch

# Optional NVTX for GPU decode-step ranges
try:
    import torch.cuda.nvtx as nvtx
    NVTX_AVAILABLE = True
except Exception:
    nvtx = None
    NVTX_AVAILABLE = False

# ===== ä½ å¯ä»¥æ”¹è¿™é‡Œï¼šæ—¥å¿—è¾“å‡ºç›®å½• & è¿è¡Œæ ‡ç­¾ï¼ˆå¯ç•™ç©ºï¼‰ =====
LOG_DIR = Path("/home/roger/logs")   # è‡ªåŠ¨åˆ›å»º
RUN_TAG = ""                         # ä¾‹å¦‚ "ablation-a1"ï¼›ç•™ç©ºåˆ™è‡ªåŠ¨ä»…ç”¨ run_id

# ===== é¡¹ç›®å†…æ¨¡å— =====
from llama3.generator import LLaMA
from llama3.config import KVCacheArgs, load_runtime_config, runtime_config_to_dict
from llama3 import generator as _gen, stream_mnt
try:
    from llama3.layers import PERF_TRACKER  # æ–°å¢ï¼šä» layers.py æ‹¿åˆ°å…¨å±€çš„æ€§èƒ½ç»Ÿè®¡å™¨
except Exception:
    PERF_TRACKER = None

# ========== build() åŒ…è£…ï¼šåªåŠ æ—¥å¿—ï¼Œä¸æ”¹åº“ ==========
_orig_build = _gen.LLaMA.build

def _debug_build(*args, **kw):
    mode       = kw.get("mode", None)
    load_model = kw.get("load_model", None)
    mode_cfg   = (kw.get("mode_config", {}) or {})
    raw_dev    = mode_cfg.get("raw_device")
    manifest   = mode_cfg.get("manifest_path") or mode_cfg.get("ssd_manifest_path")

    print(f"[MODE-DECISION] LLaMA.build(mode={mode}, load_model={load_model})")
    use_raw_ssd = (mode in {"ssd", "mixed"}) or (mode_cfg.get("weight_source") == "raw-ssd")
    print(f"[MODE-DECISION] use_raw_ssd={use_raw_ssd} raw_device={raw_dev} manifest={manifest}")

    llama = _orig_build(*args, **kw)

    has_wsm = hasattr(llama, "weight_streaming_manager")
    if has_wsm:
        wsm = llama.weight_streaming_manager
        ssd = bool(getattr(wsm, "ssd_enabled", False) or getattr(wsm, "ssd", None))
        print(f"[MODE-DECISION] built: WSM present, ssd_enabled={ssd}")
    else:
        print("[MODE-DECISION] built: NO WSM (å¯èƒ½æ˜¯ full-cpu/full-gpu/æ—§ streaming)")
    return llama

_gen.LLaMA.build = staticmethod(_debug_build)

# ======= è½»é‡çº§ Profilerï¼ˆä½æ‰°åŠ¨ï¼›CUDA Eventsï¼›ä¿å­˜ JSON/CSVï¼‰ =======
PROFILER = None  # å…¨å±€å¥æŸ„

def _now_utc():
    return datetime.now(timezone.utc).isoformat()

def _flatten_extras(extras: dict):
    out = {}
    for k,v in (extras or {}).items():
        out[k] = v if (isinstance(v,(int,float,str,bool)) or v is None) else str(v)
    return out

class InferenceProfiler:
    def __init__(self, run_name: str | None = None):
        self.run_id   = run_name or f"run-{uuid.uuid4().hex[:8]}"
        self.t0_ns    = time.perf_counter_ns()
        self.timeline = []   # å¢™é’Ÿé˜¶æ®µ
        self.active   = False
        self.cuda     = torch.cuda.is_available()
        self.forward_events = []      # GPUï¼š[(kind,batch,seqlen,start_ev,end_ev)]
        self.forward_events_cpu = []  # CPU å›é€€ï¼š[(kind,batch,seqlen,dt_ms)]
        self.bookkeep  = {}
        
        # æ–°å¢ï¼šdecode step è®¡æ•° + æ˜¯å¦å¯ç”¨ NVTX
        self.decode_step_idx = 0
        self.use_nvtx = bool(self.cuda and NVTX_AVAILABLE)

        # ä¾› finalize åˆå¹¶çš„ WSM è¿è¡ŒæœŸç»Ÿè®¡ï¼ˆç”± main() å†™å…¥ï¼‰
        self.wsm_runtime = None
        
        self.meta      = {
            "started_at_utc": _now_utc(),
            "python": platform.python_version(),
            "torch": getattr(torch, "__version__", "unknown"),
            "device": ("cuda" if self.cuda else "cpu"),
        }
        if self.cuda:
            try:
                self.meta["cuda_device_name"] = torch.cuda.get_device_name(0)
                self.meta["cuda_cc"] = ".".join(map(str, torch.cuda.get_device_capability(0)))
            except Exception:
                pass

    @contextmanager
    def span(self, name: str, category: str, **extras):
        s = time.perf_counter_ns()
        try:
            yield
        finally:
            e = time.perf_counter_ns()
            rec = {
                "name": name, "cat": category,
                "t_start_ms": (s - self.t0_ns) / 1e6,
                "t_end_ms":   (e - self.t0_ns) / 1e6,
                "dur_ms":     (e - s) / 1e6,
            }
            rec.update(_flatten_extras(extras))
            self.timeline.append(rec)
            if name == "inference_e2e":
                self.bookkeep["inference_s_ns"] = s
                self.bookkeep["inference_e_ns"] = e

    @contextmanager
    def inference_scope(self):
        self.active = True
        with self.span("inference_e2e", "inference"):
            yield
        self.active = False
    
    def now_ms(self) -> float:
        """å½“å‰ç›¸å¯¹ t0 çš„å¢™é’Ÿæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œä¾›å¤–éƒ¨è¡¥ä¸ä½¿ç”¨ã€‚"""
        return (time.perf_counter_ns() - self.t0_ns) / 1e6

    def wrap_model_forward(self, model):
        orig = model.forward

        def _classify_args(args, kwargs):
            cand = None
            for k in ("tokens", "input_ids"):
                t = kwargs.get(k, None)
                if torch.is_tensor(t) and t.dim() == 2:
                    cand = t; break
            if cand is None:
                for a in args:
                    if torch.is_tensor(a) and a.dtype in (torch.long, torch.int32, torch.int64) and a.dim() == 2:
                        cand = a; break
            if cand is None:
                return None, None
            B, T = int(cand.size(0)), int(cand.size(1))
            return B, T

        def wrapped(*args, **kwargs):
            if not self.active:
                return orig(*args, **kwargs)
            B, T = _classify_args(args, kwargs)
            kind = "prefill" if (T is not None and T > 1) else ("decode" if T == 1 else "unknown")

            if self.cuda:
                s_ev = torch.cuda.Event(enable_timing=True)
                e_ev = torch.cuda.Event(enable_timing=True)

                # æ–°å¢ï¼šdecode step çš„ NVTX æ ‡è®°ï¼Œæ–¹ä¾¿ Nsight å¯¹é½
                label = None
                if self.use_nvtx and NVTX_AVAILABLE and kind == "decode":
                    step_idx = self.decode_step_idx
                    self.decode_step_idx += 1
                    b_str = (B if B is not None else 0)
                    t_str = (T if T is not None else 0)
                    label = f"decode_step_{step_idx:04d}_B{b_str}_T{t_str}"

                if label is not None:
                    nvtx.range_push(label)
                try:
                    s_ev.record()
                    out = orig(*args, **kwargs)
                    e_ev.record()
                finally:
                    if label is not None:
                        nvtx.range_pop()

                self.forward_events.append((kind, B, T, s_ev, e_ev))
                return out
            else:
                s = time.perf_counter_ns()
                out = orig(*args, **kwargs)
                e = time.perf_counter_ns()
                self.forward_events_cpu.append((kind, B, T, (e - s) / 1e6))
                return out

        model.forward = wrapped


    # ä¾› WSM è¡¥ä¸ä½¿ç”¨
    def span_if_active(self, name, category, **extras):
        return self.span(name, category, **extras) if self is not None else nullcontext()

    def _compute_decode_stats(self, arr):
        if not arr:
            return {
                "count": 0,
                "sum_ms": 0.0,
                "mean_ms": None,
                "p50_ms": None,
                "p90_ms": None,
                "p99_ms": None,
                "decode_toks_per_s": None,
            }
        s = sorted(arr)
        q = lambda p: s[int((len(s)-1)*p)]
        total_s = sum(arr) / 1000.0  # ms to seconds
        toks_per_s = len(arr) / total_s if total_s > 0 else None
        return {
            "count": len(arr),
            "sum_ms": sum(arr),
            "mean_ms": sum(arr)/len(arr),
            "p50_ms": q(0.50),
            "p90_ms": q(0.90),
            "p99_ms": q(0.99),
            "decode_toks_per_s": toks_per_s,
        }

    def finalize(
        self,
        tokens_in: int,
        tokens_out: int,
        extra_meta: Optional[Dict[str, Any]] = None,
        kv_stats: Optional[Dict[str, Any]] = None,
        wsm_runtime: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        æŠŠ timeline + forward_events æ±‡æ€»æˆä¸€ä¸ªç»“æ„åŒ–ç»“æœï¼š
        - æ€»ä½“ e2e / prefill / decode æ—¶é—´
        - æ¯ token decode çš„è€—æ—¶ç»Ÿè®¡
        - WSM çš„ IO / ç­‰å¾…æ—¶é—´ç»Ÿè®¡
        - ï¼ˆæ–°å¢ï¼‰æ¯ä¸ª decoder layer çš„è®¡ç®—æ—¶é—´ & IO æ—¶é—´
        - warmup layer æ€»æ—¶é—´
        """
        # -------- å·¥å…·å‡½æ•° --------
        def _sum_by_name(name: str) -> float:
            return sum(ev["dur_ms"] for ev in self.timeline if ev["name"] == name)

        def _mean_or_none(values: List[float]) -> Optional[float]:
            return sum(values) / len(values) if values else None

        def _sum_io(name: str, group_key: Optional[str] = None) -> Dict[str, Any]:
            """
            ç»Ÿè®¡æŸä¸ª IO äº‹ä»¶çš„æ€»æ—¶é—´åŠåˆ†ç»„ï¼š
            name: "wsm.ssd_to_cpu_layer" / "wsm.h2d_param"
            group_key: "phase" / "thread" ç­‰
            """
            total = 0.0
            by_group: Dict[str, float] = {}
            for ev in self.timeline:
                if ev["name"] != name:
                    continue
                dur = float(ev.get("dur_ms", 0.0))
                total += dur
                if group_key is not None:
                    g = str(ev.get(group_key, "unknown"))
                    by_group[g] = by_group.get(g, 0.0) + dur
            return {"total_ms": total, "by_group": by_group}

        # -------- meta / runtime è¡¥å…… --------
        if extra_meta:
            self.meta.update(extra_meta)

        if wsm_runtime is not None:
            self.wsm_runtime = wsm_runtime
        elif self.wsm_runtime is None:
            self.wsm_runtime = {}

        # -------- prefill / decode token çº§åˆ«çš„ GPU æ—¶é—´ï¼ˆCUDA Eventï¼‰--------
        decode_ms: List[float] = []
        prefill_ms: List[float] = []
        if torch.cuda.is_available():
            for ev in self.forward_events:
                # forward_events æ ¼å¼: (kind, B, T, start_ev, end_ev)
                kind, B, T, start_evt, end_evt = ev
                if start_evt is None or end_evt is None:
                    continue
                try:
                    end_evt.synchronize()  # ç¡®ä¿äº‹ä»¶å®Œæˆ
                    dt_ms = float(start_evt.elapsed_time(end_evt))
                except Exception:
                    continue

                if kind == "decode":
                    decode_ms.append(dt_ms)
                elif kind == "prefill":
                    prefill_ms.append(dt_ms)

        # -------- ä» timeline é‡ŒæŠ½ prefill / decode / å…¶å®ƒé˜¶æ®µ --------
        # inference scope ç”¨äºåŒ…ä½æ•´ä¸ªæ¨ç†
        infer_spans = [ev for ev in self.timeline if ev["name"] == "inference_e2e"]
        if infer_spans:
            t0 = min(ev["t_start_ms"] for ev in infer_spans)
            t1 = max(ev["t_end_ms"] for ev in infer_spans)
            e2e_ms = t1 - t0
        else:
            e2e_ms = None

        # ç»Ÿè®¡ prefill å’Œ decode é˜¶æ®µçš„æ—¶é—´ï¼ˆä» CUDA eventsï¼‰
        prefill_total_ms = sum(prefill_ms) if prefill_ms else 0.0
        decode_total_ms = sum(decode_ms) if decode_ms else 0.0

        # é¢„çƒ­ GPU decoder windowï¼ˆwarmup layerï¼‰æ—¶é—´
        warmup_spans = [ev for ev in self.timeline if ev["name"] == "gpu_window_warmup"]
        warmup_total_ms = sum(ev["dur_ms"] for ev in warmup_spans)
        warmup_calls = len(warmup_spans)

        # åˆ†ç±»ç»Ÿè®¡ï¼šå„ä¸ª cat çš„æ€»æ—¶é—´
        #   - prefill, decode
        by_cat: Dict[str, float] = {}
        for ev in self.timeline:
            cat = ev.get("cat")
            dur = float(ev.get("dur_ms", 0.0))
            if not cat or dur <= 0:
                continue
            by_cat[cat] = by_cat.get(cat, 0.0) + dur

        sum_cat = {
            "total_ms": sum(by_cat.values()) if by_cat else 0.0,
            "by_cat_ms": by_cat,
        }

        # -------- decode token ç»Ÿè®¡ï¼ˆæ¯ token è€—æ—¶åˆ†å¸ƒï¼‰--------
        decode_stats = self._compute_decode_stats(decode_ms)

        # -------- slack_ms ç»Ÿè®¡ï¼ˆgroup ready åˆ°ä½¿ç”¨çš„æ—¶é—´å·®ï¼‰--------
        slack_times = []
        for ev in self.timeline:
            if ev.get("name") == "wsm.wait_group_ready":
                s = ev.get("slack_ms")
                if isinstance(s, (int, float)) and s >= 0:
                    slack_times.append(s)

        slack_stats = self._compute_decode_stats(slack_times) if slack_times else {
            "count": 0,
            "sum_ms": 0.0,
            "mean_ms": None,
            "p50_ms": None,
            "p90_ms": None,
            "p99_ms": None,
        }

        # -------- WSM é«˜å±‚ç»Ÿè®¡ï¼ˆç­‰ä½ ä¹‹å‰å·²æœ‰çš„éƒ¨åˆ†ï¼‰--------
        wait_total_ms = _sum_by_name("wsm.wait_group_ready")
        ensure_total_ms = _sum_by_name("wsm.ensure_module_on_gpu")

        # IO æŒ‰ç±»å‹ç»Ÿè®¡ï¼ˆSSD->CPU / CPU->GPUï¼‰
        ssd_io = _sum_io("wsm.ssd_to_cpu_layer", group_key="phase")
        h2d_io = _sum_io("wsm.h2d_param", group_key="phase")

        w_io = {
            "ssd_to_cpu_ms": ssd_io,
            "h2d_param_ms": h2d_io,
        }

        wsm_stats: Dict[str, Any] = {
            "wait_group_ready": {"total_ms": wait_total_ms},
            "ensure_module_on_gpu": {"total_ms": ensure_total_ms},
            "io": w_io,
            "slack_time": slack_stats,  # group ready åˆ°ä½¿ç”¨çš„æ—¶é—´å·®ç»Ÿè®¡
        }
        if self.wsm_runtime:
            wsm_stats["runtime"] = self.wsm_runtime

        # --------ï¼ˆæ–°å¢ï¼‰æ¯ä¸ª decoder layer çš„ compute / IO ç»Ÿè®¡ --------
        decoder_layers_global: Dict[str, float] = {}
        decoder_layers_per_layer: Dict[str, Any] = {}

        # 2.1 ä» PERF_TRACKER æ‹¿ GPU è®¡ç®—æ—¶é—´ï¼ˆåœ¨ layers.py é‡Œï¼‰
        perf_per_layer: Dict[int, Dict[str, float]] = {}
        if PERF_TRACKER is not None:
            try:
                perf_stats = PERF_TRACKER.get_stats()  # {"global": {...}, "per_layer": {...}}
                decoder_layers_global = dict(perf_stats.get("global", {}))
                perf_per_layer = perf_stats.get("per_layer", {}) or {}
            except Exception:
                perf_per_layer = {}
                decoder_layers_global = {}

        # 2.2 ä» WSM timeline æŒ‰ layer_idx èšåˆ IO æ—¶é—´
        layer_io: Dict[int, Dict[str, float]] = {}
        for ev in self.timeline:
            lid = ev.get("layer_idx")
            if lid is None:
                continue
            name = ev.get("name", "")
            if not (
                name.startswith("wsm.ssd_to_cpu_layer")
                or name.startswith("wsm.h2d_param")
                or name.startswith("wsm.wait_group_ready")
            ):
                continue

            lid = int(lid)
            entry = layer_io.setdefault(
                lid,
                {
                    "ssd_to_cpu_ms": 0.0,
                    "h2d_param_ms": 0.0,
                    "wait_group_ready_ms": 0.0,
                },
            )
            dur = float(ev.get("dur_ms", 0.0))
            if name.startswith("wsm.ssd_to_cpu_layer"):
                entry["ssd_to_cpu_ms"] += dur
            elif name.startswith("wsm.h2d_param"):
                entry["h2d_param_ms"] += dur
            elif name.startswith("wsm.wait_group_ready"):
                entry["wait_group_ready_ms"] += dur

        # 2.3 åˆå¹¶ per-layer compute + IO
        all_layer_ids = sorted(
            set(list(perf_per_layer.keys()) + list(layer_io.keys()))
        )
        for lid in all_layer_ids:
            lp = perf_per_layer.get(lid, {})  # æ¥è‡ª cuda_timer çš„å„ç±» us ç»Ÿè®¡
            li = layer_io.get(lid, {})

            attn_us = float(lp.get("attn_us", 0.0))
            ffn_us = float(lp.get("ffn_us", 0.0))
            total_forward_us = float(lp.get("total_forward_us", 0.0))
            if not total_forward_us and (attn_us or ffn_us):
                total_forward_us = attn_us + ffn_us

            kv_fetch_us = float(lp.get("kv_fetch_us", 0.0))
            mem_us = float(lp.get("memory_alloc_us", 0.0))
            weights_hbm_us = float(lp.get("weights_hbm_us", 0.0))

            io_total_ms = sum(li.values()) if li else 0.0

            decoder_layers_per_layer[str(lid)] = {
                "compute_us": {
                    "attn_us": attn_us,  # CUDA timer: çº¯ attention è®¡ç®—æ—¶é—´
                    "ffn_us": ffn_us,  # CUDA timer: çº¯ FFN è®¡ç®—æ—¶é—´
                    "kv_fetch_us": kv_fetch_us,  # CUDA timer: KV cache è·å–æ—¶é—´
                    "total_forward_us": total_forward_us,  # CUDA timer: æ•´ä¸ª forward çš„ GPU è®¡ç®—æ—¶é—´ï¼ˆä¸å« I/O ç­‰å¾…ï¼‰
                    "memory_alloc_us": mem_us,  # CUDA timer: å†…å­˜åˆ†é…æ—¶é—´
                    "weights_hbm_us": weights_hbm_us,  # CUDA timer: æƒé‡ HBM ä¼ è¾“æ—¶é—´
                },
                "io_ms": li,  # WSM timeline: è¯¥å±‚çš„ I/O æ—¶é—´ï¼ˆSSDâ†’CPU, CPUâ†’GPU, waitï¼‰
                "summary": {
                    "compute_ms_pure": (
                        total_forward_us / 1000.0 if total_forward_us else None
                    ),  # çº¯ GPU è®¡ç®—æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
                    "io_ms_total": io_total_ms,  # æ€» I/O æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
                },
            }

        decoder_layers = {
            "global": decoder_layers_global,
            "per_layer": decoder_layers_per_layer,
        }

        # -------- æœ€ç»ˆ timings / throughput --------
        # è®¡ç®— First Token Latency (FTL): prefill + ç¬¬ä¸€ä¸ª decode token
        ftl_ms = None
        if prefill_total_ms and decode_ms:
            ftl_ms = prefill_total_ms + decode_ms[0]
        elif prefill_total_ms:
            ftl_ms = prefill_total_ms

        timings: Dict[str, Any] = {
            "e2e_ms": e2e_ms,
            "prefill_total_ms": prefill_total_ms,
            "first_token_latency_ms": ftl_ms,
            "decode": decode_stats,
            "warmup_total_ms": warmup_total_ms,
            "warmup_calls": warmup_calls,
            "by_category_ms": sum_cat,
        }

        throughput = {
            "prefill_toks_per_s": (
                tokens_in / (prefill_total_ms / 1000.0)
                if prefill_total_ms and tokens_in > 0
                else None
            ),
            "decode_toks_per_s": decode_stats.get("decode_toks_per_s"),
        }

        # -------- å†…å­˜å³°å€¼ç»Ÿè®¡ --------
        memory_stats = {}
        if torch.cuda.is_available():
            try:
                memory_stats["gpu_peak_allocated_gb"] = torch.cuda.max_memory_allocated() / (1 << 30)
                memory_stats["gpu_peak_reserved_gb"] = torch.cuda.max_memory_reserved() / (1 << 30)
            except Exception:
                pass

        # -------- æ±‡æ€»æˆ result --------
        self.result = {
            "run": self.meta
            | {
                "run_id": self.run_id,
                "finished_at_utc": _now_utc(),
            },
            "counts": {
                "tokens_in": tokens_in,
                "tokens_out": tokens_out,
            },
            "timings": timings,  # e2e, prefill, decode ç»Ÿè®¡ï¼Œwarmup æ—¶é—´ç­‰
            "throughput": throughput,  # tokens/s ååé‡
            "wsm": wsm_stats,  # WSM I/O å’Œç­‰å¾…æ—¶é—´ç»Ÿè®¡
            "decoder_layers": decoder_layers,  # æ¯å±‚çš„ CUDA è®¡ç®—æ—¶é—´ + I/O æ—¶é—´è¯¦ç»†åˆ†è§£
            "decode_step_ms": decode_ms,  # æ¯ä¸ª decode step çš„è€—æ—¶æ•°ç»„ï¼ˆCUDA eventsï¼‰
            "timeline": self.timeline,  # å®Œæ•´çš„äº‹ä»¶æ—¶é—´çº¿ï¼ˆç”¨äºè¯¦ç»†åˆ†æï¼‰
            "memory": memory_stats,  # GPU å†…å­˜å³°å€¼ç»Ÿè®¡
        }

        if kv_stats is not None:
            self.result["kv_cache"] = kv_stats

        return self.result



    def save(self, path: str):
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        if path.lower().endswith(".json"):
            with open(path, "w", encoding="utf-8") as f:
                json.dump(self.result, f, ensure_ascii=False, indent=2)
        elif path.lower().endswith(".csv"):
            rows = []
            for ev in self.timeline:
                r = {"kind":"span","name":ev["name"],"cat":ev["cat"],
                     "t_start_ms":ev["t_start_ms"],"t_end_ms":ev["t_end_ms"],"dur_ms":ev["dur_ms"]}
                rows.append(r)
            for i,dt in enumerate(self.result.get("decode_step_ms", [])):
                rows.append({"kind":"decode_step","name":f"decode_{i:04d}","cat":"inference","t_start_ms":"", "t_end_ms":"", "dur_ms":dt})
            with open(path, "w", newline="", encoding="utf-8") as f:
                w = csv.DictWriter(f, fieldnames=["kind","name","cat","t_start_ms","t_end_ms","dur_ms"])
                w.writeheader(); w.writerows(rows)
        else:
            with open(path + ".json", "w", encoding="utf-8") as f:
                json.dump(self.result, f, ensure_ascii=False, indent=2)

# ===== è·¯å¾„ä¸å¸¸é‡ï¼ˆæŒ‰ä½ çš„ç¯å¢ƒï¼‰ =====
PROMPT_TXT = Path("/home/roger/llama3-inference/prompts/prompts_batch512_len2048.txt")
RAW_DEV    = "/dev/nvme0n1p4"
MANIFEST   = "/data1/70b-fixed.runtime_manifest.json"
CKPT_DIR   = "/home/roger/.llama/checkpoints/Llama3.1-70B"

# ---------- ç³»ç»Ÿ/GPU å†…å­˜å¿«ç…§ ----------
def _read_status():
    def _grep(path, keys):
        out = {}
        try:
            with open(path, "r") as f:
                for line in f:
                    for k in keys:
                        if line.startswith(k + ":"):
                            out[k] = line.split(":")[1].strip()
        except Exception:
            pass
        return out
    s = _grep("/proc/self/status", ["VmRSS","VmHWM","VmLck"])
    m = _grep("/proc/meminfo", ["MemAvailable","CommitLimit","Committed_AS","Cached","Buffers"])
    return s, m

def _gpu_mem():
    if not torch.cuda.is_available():
        return {}
    dev = torch.cuda.current_device()
    st  = torch.cuda.memory_stats(dev)
    return {
        "alloc_GB": st.get("allocated_bytes.all.current", 0)/(1<<30),
        "rsrv_GB":  st.get("reserved_bytes.all.current", 0)/(1<<30),
    }

def probe(stage: str):
    s, m = _read_status()
    g    = _gpu_mem()
    print(f"\n[MEM] {stage}")
    print(f"  VmRSS={s.get('VmRSS','?')}  VmLck(pinned)={s.get('VmLck','?')}  "
          f"CommitLimit={m.get('CommitLimit','?')}  Committed_AS={m.get('Committed_AS','?')}  "
          f"MemAvailable={m.get('MemAvailable','?')}")
    if g:
        print(f"  GPU: allocated={g['alloc_GB']:.2f} GiB  reserved={g['rsrv_GB']:.2f} GiB")
    print()

# ---------- æ‰«æå‚æ•°åœ¨ meta/cpu/cuda ä¸Šçš„å ç”¨ ----------
def dump_param_inventory(model, tag):
    buckets = {"cpu":0, "cuda":0, "meta":0, "other":0}
    big_cpu = []
    for n,p in model.named_parameters(recurse=True):
        b = p.numel() * p.element_size()
        if getattr(p, "is_meta", False):
            buckets["meta"] += b
        elif hasattr(p, "device"):
            t = p.device.type
            if t == "cpu":
                buckets["cpu"] += b
                if b >= (64<<20):
                    big_cpu.append((n,b))
            elif t == "cuda":
                buckets["cuda"] += b
            else:
                buckets["other"] += b
        else:
            buckets["other"] += b
    f = lambda x: f"{x/(1<<30):.2f} GiB"
    print(f"[PARAMS] {tag}: cpu={f(buckets['cpu'])}, cuda={f(buckets['cuda'])}, meta={f(buckets['meta'])}, other={f(buckets['other'])}")
    if big_cpu:
        big_cpu.sort(key=lambda x:-x[1])
        print("  [big-cpu] top:")
        for n,b in big_cpu[:10]:
            print(f"   - {n}  {b/(1<<20):.1f} MiB")

# ---------- è¿è¡Œæ—¶è¦†ç›–ï¼šæ”¶æ•› pinned/æ³¨å†Œæ±  ----------
def apply_runtime_overrides():
    """
    æŠŠæ³¨å†Œæ€»é‡é’³åœ¨ â‰¤256MiBï¼Œå¹¶æŠŠ EXTENT_BYTES é™åˆ° 1MiBï¼Œé™ä½é«˜é˜¶é¡µ order å‹åŠ›ã€‚
    """
    cfg = load_runtime_config({
        "pinned": {
            "WEIGHT_PINNED_BYTES":      8  << 30,
            "KV_PINNED_BYTES":          8  << 30,
            "EXTENT_BYTES":             1  << 20,   # 1MiB
            "PINNED_REGISTER_CHUNK":   16  << 20,   # 16MiB
            "PINNED_REGISTER_N":            8,      # 128MiB
        },
        "regpool": {
            "REG_POOL_N_BUFFERS":           8,
            "REG_POOL_BUF_BYTES":     16 << 20,     # ~128MiB ä¼ é€å¸¦
        },
        "io": {
            "RAW_IO_QD_WRITE":             24,      # å†™é˜Ÿåˆ—æ·±åº¦
            "IO_RAW_THROTTLE_MS":          30,      # å†™å¸¦å®½çª—å£
        }
    })
    D = runtime_config_to_dict(cfg)
    p = D["pinned"]
    need  = int(p["WEIGHT_PINNED_BYTES"])
    chunk = int(p["PINNED_REGISTER_CHUNK"])
    target_total = min(need // 2, 256 << 20)  # ç›®æ ‡ â‰¤ 256MiB
    newN = max(1, target_total // chunk)
    p["PINNED_REGISTER_N"] = newN
    cfg = load_runtime_config({"pinned": p, "io": D["io"]})
    print("[RuntimeConfig] pinned =", runtime_config_to_dict(cfg)["pinned"])
    print("[RuntimeConfig] io =", runtime_config_to_dict(cfg)["io"])
    return cfg

# ---------- KV æ± ï¼šæ‡’åˆ†é… + å•å— â‰¥ å•ä¸ª KV å— ----------
def configure_kv_pool():
    # DRAM é…ç½®
    KVCacheArgs.dram_limit_gb     = 32.0
    KVCacheArgs.dram_sizing_batch = 32
    KVCacheArgs.block_bytes       = 1 * 1024 * 1024
    KVCacheArgs.preallocate       = False
    KVCacheArgs.lazy_init         = True

    # å…³é—­ push å³æ—¶é•œåƒï¼Œé‡‡ç”¨åç§»/èšåˆå†™ï¼ˆé¿å…ä¸æƒé‡ H2D å†²çªï¼‰
    KVCacheArgs.mirror_on_push = False

    # I/O èŠ‚æµä¸å†™é€Ÿç‡é…ç½®ï¼ˆä¸æƒé‡ H2D ä»²è£ï¼‰
    KVCacheArgs.IO_RAW_THROTTLE_MS     = 25
    KVCacheArgs.NVME_WRITE_TARGET_MBPS = 1500

    if hasattr(KVCacheArgs, "prefer_bf16"):
        KVCacheArgs.prefer_bf16 = True

    print(f"[KVArgs] dram_limit={KVCacheArgs.dram_limit_gb} GiB, "
          f"block_bytes={KVCacheArgs.block_bytes//(1<<20)} MiB, prealloc={KVCacheArgs.preallocate}")
    print(f"[KVArgs] mirror_on_push={KVCacheArgs.mirror_on_push}, "
          f"IO_RAW_THROTTLE_MS={KVCacheArgs.IO_RAW_THROTTLE_MS}, "
          f"NVME_WRITE_TARGET_MBPS={KVCacheArgs.NVME_WRITE_TARGET_MBPS}")

# ---------- è¯†åˆ«â€œå®é™…è¿è¡Œçš„æ¨¡å¼â€ ----------
def classify_mode(llama) -> str:
    """
    è¿”å›ï¼š'ssd-streaming' / 'cpu-gpu-streaming' / 'full-gpu' / 'full-cpu' / 'meta-only'
    """
    m = llama.model
    if hasattr(llama, "weight_streaming_manager"):
        wsm = llama.weight_streaming_manager
        ssd = bool(getattr(wsm, "ssd_enabled", False) or getattr(wsm, "ssd", None))
        cpu_warm = getattr(wsm, "disable_cpu_warm", None)
        mode = "ssd-streaming" if ssd else "cpu-gpu-streaming"
        print(f"[MODE] detected={mode}  (has WSM, ssd={ssd}, disable_cpu_warm={cpu_warm})")
        return mode
    cpu, cuda, meta = 0,0,0
    for _,p in m.named_parameters():
        b = p.numel()*p.element_size()
        if getattr(p, "is_meta", False): meta += b
        elif p.device.type == "cpu":     cpu  += b
        elif p.device.type == "cuda":    cuda += b
    if cuda > 0 and cpu == 0 and meta == 0:
        print("[MODE] detected=full-gpu"); return "full-gpu"
    if cpu  > 0 and cuda == 0 and meta == 0:
        print("[MODE] detected=full-cpu"); return "full-cpu"
    if meta > 0 and cpu == 0 and cuda == 0:
        print("[MODE] detected=meta-only"); return "meta-only"
    print("[MODE] mixed/unrecognized (check PARAMS dump below)")
    return "unknown"


# ===== WSM wait_group_ready åŒ…è£…ï¼ˆä»…æ·»åŠ  Profiler è®¡æ—¶ï¼Œä¸æ”¹é€»è¾‘ï¼‰ =====
def _wrap_wait_group_ready(original_method):
    """
    åŒ…è£… WSM.wait_group_readyï¼Œæ·»åŠ ï¼š
      - profiler è®¡æ—¶åŸ‹ç‚¹ï¼›
      - ä» group ready åˆ°é¦–æ¬¡ compute ä½¿ç”¨çš„ slack_ms ä¼°è®¡ï¼›
      - pipeline æ°´ä½ï¼ˆring / inflight / in_use çš„æœ€å¤§å€¼ï¼‰ã€‚
    """
    def wrapped(self, layer_idx: int, group: str, compute_stream=None):
        prof = globals().get("PROFILER")
        extras = {
            "layer_idx": int(layer_idx),
            "group": str(group),
        }

        # 1) slack_ms = ç°åœ¨æ—¶é—´ - è¯¥ group ready äº‹ä»¶è®°å½•æ—¶é—´
        if prof is not None and hasattr(self, "_group_ready_wallclock"):
            try:
                key = (int(layer_idx), str(group))
                ready_ms = self._group_ready_wallclock.get(key)
                if isinstance(ready_ms, (int, float)):
                    now_ms = prof.now_ms()
                    extras["slack_ms"] = max(0.0, now_ms - float(ready_ms))
            except Exception:
                pass

        # 2) pipeline æ°´ä½ç»Ÿè®¡ï¼ˆæœ€å¤§ ring/inflight/in_useï¼‰
        if hasattr(self, "_pipeline_watermark"):
            try:
                lock = getattr(self, "_group_lock", None)
                if lock is not None:
                    with lock:
                        ring_len     = len(getattr(self, "_gpu_group_ring", []))
                        inflight_len = len(getattr(self, "_gpu_group_inflight", set()))
                        in_use_len   = len(getattr(self, "_gpu_group_in_use", {}))
                else:
                    ring_len     = len(getattr(self, "_gpu_group_ring", []))
                    inflight_len = len(getattr(self, "_gpu_group_inflight", set()))
                    in_use_len   = len(getattr(self, "_gpu_group_in_use", {}))

                wm = self._pipeline_watermark
                wm["max_gpu_ring"]  = max(wm.get("max_gpu_ring", 0), ring_len)
                wm["max_inflight"]  = max(wm.get("max_inflight", 0), inflight_len)
                wm["max_in_use"]    = max(wm.get("max_in_use", 0), in_use_len)
            except Exception:
                pass

        ctx = prof.span("wsm.wait_group_ready", "wsm", **extras) if prof is not None else nullcontext()
        with ctx:
            return original_method(layer_idx, group, compute_stream)

    return wrapped



def _patched_ensure_module_on_gpu(self, m: torch.nn.Module, layer_idx: int | None = None, module_name: str | None = None):
    """
    æ‰©å±•ï¼šæŠŠ **0-size CPU stub** å½“ä½œ meta ä¸€æ ·å¤„ç†ï¼Œä¼˜å…ˆä» CPU cache å–å›å¹¶ä¸Šå¡ã€‚
    å…¶å®ƒæƒ…å†µä»å¤ç”¨åŸå…ˆçš„ _ensure_param_on_gpu() è·¯å¾„ã€‚
    """
    with (PROFILER.span("wsm.ensure_module_on_gpu", "wsm", layer_idx=(None if layer_idx is None else int(layer_idx)), module=str(module_name))
          if (globals().get("PROFILER") is not None) else nullcontext()):
        params_to_replace = {}
        params_full_names = {}

        def _full_name(layer_idx: int, module_name: str, local_param_name: str) -> str:
            if module_name in ("wq", "wk", "wv", "wo"):
                parent = "attention"
            elif module_name in ("w1", "w2", "w3"):
                parent = "feed_forward"
            else:
                parent = module_name or ""
            return f"layers.{layer_idx}.{parent}.{module_name}.{local_param_name}" if parent else f"layers.{layer_idx}.{module_name}.{local_param_name}"

        def _fetch_from_cpu_cache(name: str):
            if (layer_idx is not None) and (layer_idx in self.cpu_cache):
                return self.cpu_cache[layer_idx].get(name)
            return None

        for local_param_name, p in m.named_parameters(recurse=False):
            full_name = None
            if (layer_idx is not None) and (module_name is not None):
                full_name = _full_name(layer_idx, module_name, local_param_name)

            is_meta     = (p.device.type == "meta") or getattr(p, "is_meta", False)
            is_cpu_stub = (p.device.type == "cpu")  and (p.numel() == 0)

            if (is_meta or is_cpu_stub) and self.ssd_enabled and full_name:
                # ç¡®ä¿æœ¬å±‚å·²æœ‰ CPU cacheï¼ˆæ²¡æœ‰å°±ç«‹å³åŠ è½½ï¼‰
                if (layer_idx not in self.cpu_cache):
                    try:
                        self._load_layer_to_cpu(int(layer_idx))
                    except Exception:
                        pass

                cached = _fetch_from_cpu_cache(full_name)
                expected = tuple(getattr(getattr(m, local_param_name), "shape", ()))
                chosen_name, chosen_tensor = None, None

                def _try_pick(names: list[str]):
                    nonlocal chosen_name, chosen_tensor
                    for nm in names:
                        t = _fetch_from_cpu_cache(nm)
                        if t is not None and (not expected or tuple(t.shape) == expected):
                            chosen_name, chosen_tensor = nm, t
                            break

                if cached is not None and (not expected or tuple(cached.shape) == expected):
                    chosen_name, chosen_tensor = full_name, cached
                else:
                    cand = []
                    if module_name in ("wq", "wk", "wv"):
                        cand = [f"layers.{layer_idx}.attention.{x}.{local_param_name}" for x in ("wq","wk","wv")]
                    elif module_name in ("w1", "w2", "w3"):
                        cand = [f"layers.{layer_idx}.feed_forward.{x}.{local_param_name}" for x in ("w1","w2","w3")]
                    else:
                        cand = [full_name]
                    _try_pick(cand)
                    if chosen_tensor is None and cached is not None:
                        chosen_name, chosen_tensor = full_name, cached  # é€€è€Œæ±‚å…¶æ¬¡

                if chosen_tensor is not None:
                    with torch.cuda.stream(self._select_h2d_stream_for(module_name=module_name)):
                        p_gpu = chosen_tensor.to(self.device, non_blocking=True)
                    params_to_replace[local_param_name] = torch.nn.Parameter(p_gpu, requires_grad=p.requires_grad)
                    params_full_names[local_param_name] = chosen_name or full_name
                    if getattr(self, "verbose", False):
                        print(f"[WSM DEBUG] âœ“ Loaded {'meta' if is_meta else 'stub'} param {params_full_names[local_param_name]} to GPU: {tuple(p_gpu.shape)}")
                else:
                    if getattr(self, "verbose", False):
                        print(f"[WSM WARN] CPU cache miss for {full_name} (layer {layer_idx}); will rely on ensure_group_on_gpu() later")
                continue  # è¯¥å‚æ•°å¤„ç†å®Œæ¯•

            # å…¶å®ƒæƒ…å†µï¼šæ²¿ç”¨åŸæ¥çš„ CPUâ†’GPU é€»è¾‘
            self._ensure_param_on_gpu(p, layer_idx, full_name)

        # å®‰è£…æ›¿æ¢åçš„ Parameterï¼Œå¹¶ç»´æŠ¤ name æ˜ å°„
        for pname, new_param in params_to_replace.items():
            m._parameters[pname] = new_param
            full = params_full_names.get(pname)
            if full:
                try:
                    pobj = getattr(m, pname)
                except Exception:
                    pobj = new_param
                self.name_to_param[full] = pobj
                self.param_owner[full]   = (m, pname)

        # buffer ç»´æŒåŸæœ‰ç­–ç•¥ï¼šmetaâ†’materializeï¼ŒCPUâ†’ä¸Šå¡
        for b in m.buffers(recurse=True):
            if getattr(b, "is_meta", False):
                try:
                    b = b.to_empty(device=self.device)
                except Exception:
                    pass
            elif b.device.type == "cpu":
                with torch.cuda.stream(self._select_h2d_stream_for(module_name=module_name)):
                    b_gpu = b.detach().to(self.device, non_blocking=True)
                try:
                    b.data = b_gpu
                except Exception:
                    pass
                
def _patch_wsm_for_profiling(wsm):
    """
    ç»™ WeightStreamingManager æ‰“è¡¥ä¸ï¼Œè¡¥é½ï¼š
      - SSD -> pinned CPU è¯»å±‚æ—¶é—´ï¼ˆå«å­—èŠ‚æ•°ï¼‰
      - pinned CPU -> GPU ç»„çº§ H2D æ—¶é—´ï¼ˆå«å­—èŠ‚æ•°ã€attn/ffnï¼‰
      - wait_group_ready äº‹ä»¶ï¼ˆä¿æŒåŸæ¥ç»Ÿè®¡ï¼‰
    æ³¨æ„ï¼šåªä¾èµ– run æ—¶çš„ wsm å®ä¾‹ï¼Œä¸æ”¹ wsm æºç ã€‚
    """
    global PROFILER
    prof = PROFILER
    if prof is None:
        # æ²¡å¼€ profiler å°±ä¸æ‰“ç‚¹ï¼Œä¿æŒé›¶å¼€é”€
        return

    # ---------- 1) wait_group_readyï¼šä¿ç•™ä½ ç°æœ‰çš„ç­‰å¾…ç»Ÿè®¡ ----------
    if hasattr(wsm, "_record_group_ready_event"):
        orig_rg = wsm._record_group_ready_event

        def _record_group_ready_event_patched(self, layer_idx, group, *args, **kwargs):
            # è¿™é‡Œåªè®°å½• wait è‡ªèº«çš„é˜»å¡æ—¶é—´ï¼›æ›´ç»†çš„ slack ä½ ä¹‹å‰å·²ç»åœ¨ _decode_timeline é‡Œç®—äº†
            s = time.perf_counter_ns()
            try:
                return orig_rg(layer_idx, group, *args, **kwargs)
            finally:
                e = time.perf_counter_ns()
                rec = {
                    "name": "wsm.wait_group_ready",
                    "cat": "wsm",
                    "t_start_ms": (s - prof.t0_ns) / 1e6,
                    "t_end_ms":   (e - prof.t0_ns) / 1e6,
                    "dur_ms":     (e - s) / 1e6,
                    "layer_idx":  int(layer_idx),
                    "group":      str(group),
                }
                prof.timeline.append(rec)

        wsm._record_group_ready_event = types.MethodType(_record_group_ready_event_patched, wsm)

    # ---------- 2) SSD -> pinned CPUï¼šæŒ‰ layer ç»Ÿè®¡ ----------
    def _ssd_bytes_for_layer(self, layer_idx: int) -> int:
        try:
            params = self.layers_params.get(int(layer_idx), [])
        except Exception:
            return 0
        total = 0
        for p in params:
            try:
                if p.get("policy") == "stream":
                    total += int(p.get("nbytes", 0))
            except Exception:
                pass
        return int(total)

    if getattr(wsm, "ssd_enabled", False):
        # åŒæ­¥ç‰ˆæœ¬ï¼ˆå¯èƒ½è¢« warmup ç”¨åˆ°ï¼‰
        if hasattr(wsm, "_read_layer_from_ssd"):
            orig_read = wsm._read_layer_from_ssd

            def _read_layer_from_ssd_patched(self, layer_idx: int):
                lid = int(layer_idx)
                total_bytes = _ssd_bytes_for_layer(self, lid)
                phase = getattr(self, "_phase", None) or "unknown"
                with prof.span_if_active(
                    "wsm.ssd_to_cpu_layer",
                    "io",
                    layer_idx=lid,
                    bytes=total_bytes,
                    phase=phase,
                    thread="main",
                ):
                    return orig_read(lid)

            wsm._read_layer_from_ssd = types.MethodType(_read_layer_from_ssd_patched, wsm)

        # çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼šçœŸæ­£çš„ CPU é¢„å–çº¿ç¨‹èµ°çš„æ˜¯è¿™ä¸ª
        if hasattr(wsm, "_read_layer_from_ssd_threadsafe"):
            orig_read_ts = wsm._read_layer_from_ssd_threadsafe

            def _read_layer_from_ssd_threadsafe_patched(self, layer_idx: int):
                lid = int(layer_idx)
                total_bytes = _ssd_bytes_for_layer(self, lid)
                phase = getattr(self, "_phase", None) or "unknown"
                with prof.span_if_active(
                    "wsm.ssd_to_cpu_layer",
                    "io",
                    layer_idx=lid,
                    bytes=total_bytes,
                    phase=phase,
                    thread="cpu_pf_worker",
                ):
                    return orig_read_ts(lid)

            wsm._read_layer_from_ssd_threadsafe = types.MethodType(
                _read_layer_from_ssd_threadsafe_patched, wsm
            )

    # ---------- 3) pinned CPU -> GPUï¼šç»„çº§ H2D ----------
    if hasattr(wsm, "_install_group_on_gpu"):
        orig_install = wsm._install_group_on_gpu

        def _install_group_on_gpu_patched(self, layer_idx: int, group: str, *, h2d_override=None):
            lid = int(layer_idx)
            grp = str(group)
            phase = getattr(self, "_phase", None) or "prefill"

            # ä¼°ç®—è¿™æ¬¡ H2D çš„å­—èŠ‚æ•°ï¼šåªçœ‹ CPU cache é‡Œè¿™å±‚å½“å‰ç»„çš„æƒé‡
            total_bytes = 0
            try:
                suffixes = ()
                if grp == "attn":
                    suffixes = (
                        "attention.wq.weight",
                        "attention.wk.weight",
                        "attention.wv.weight",
                        "attention.wo.weight",
                    )
                elif grp == "ffn":
                    suffixes = (
                        "feed_forward.w1.weight",
                        "feed_forward.w2.weight",
                        "feed_forward.w3.weight",
                    )

                if suffixes:
                    with self.cpu_cache_lock:
                        layer_data = dict(self.cpu_cache.get(lid, {}))

                    for suf in suffixes:
                        pname = f"layers.{lid}.{suf}"
                        t = layer_data.get(pname)
                        if t is not None and torch.is_tensor(t) and t.numel() > 0:
                            total_bytes += int(t.numel() * t.element_size())
            except Exception:
                total_bytes = 0

            with prof.span_if_active(
                "wsm.h2d_param",
                "io",
                layer_idx=lid,
                group=grp,
                bytes=int(total_bytes),
                phase=phase,
            ):
                return orig_install(layer_idx, group, h2d_override=h2d_override)

        wsm._install_group_on_gpu = types.MethodType(_install_group_on_gpu_patched, wsm)

    # ---------- 4) å…¼å®¹è€è·¯å¾„ï¼š_load_layer_to_cpu / _h2d_transfer_with_retry ----------
    # è¿™äº›åœ¨ä½ æ–°ç‰ˆ pipeline ä¸­åŸºæœ¬ä¸ä¼šèµ°åˆ°ï¼Œä½†ç•™ç€ä»¥é˜²ä»¥å fallback
    if hasattr(wsm, "_load_layer_to_cpu"):
        orig_load = wsm._load_layer_to_cpu

        def _load_layer_to_cpu_patched(self, layer_idx: int):
            lid = int(layer_idx)
            total_bytes = _ssd_bytes_for_layer(self, lid)
            phase = getattr(self, "_phase", None) or "unknown"
            with prof.span_if_active(
                "wsm.ssd_to_cpu_layer",
                "io",
                layer_idx=lid,
                bytes=total_bytes,
                phase=phase,
                thread="fallback_sync",
            ):
                return orig_load(lid)

        wsm._load_layer_to_cpu = types.MethodType(_load_layer_to_cpu_patched, wsm)

    if hasattr(wsm, "_h2d_transfer_with_retry"):
        orig_h2d = wsm._h2d_transfer_with_retry

        def _h2d_transfer_with_retry_patched(self, src_cpu_tensor, param_name, h2d_stream):
            bytes_ = 0
            try:
                if torch.is_tensor(src_cpu_tensor) and src_cpu_tensor.numel() > 0:
                    bytes_ = int(src_cpu_tensor.numel() * src_cpu_tensor.element_size())
            except Exception:
                bytes_ = 0

            pname = str(param_name)
            if "attention." in pname:
                grp = "attn"
            elif "feed_forward." in pname:
                grp = "ffn"
            else:
                grp = "other"

            phase = getattr(self, "_phase", None) or "prefill"

            with prof.span_if_active(
                "wsm.h2d_param",
                "io",
                param=pname,
                group=grp,
                bytes=bytes_,
                phase=phase,
            ):
                return orig_h2d(src_cpu_tensor, param_name, h2d_stream)

        wsm._h2d_transfer_with_retry = types.MethodType(
            _h2d_transfer_with_retry_patched, wsm
        )
        
        
def extract_kv_cache_stats(llama):
    """
    ä» LLaMA wrapper ä¸­æå– KV cache ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœä½¿ç”¨äº† KVOffloaderï¼‰ã€‚

    è¿”å›ç¤ºä¾‹ï¼š
    {
        "fetch_blocks_total": 1234,
        "hits": 1200,
        "misses": 34,
        "hit_ratio": 0.9724,
        "ssd_load_blocks_prefetch": 56,
        "evictions": 789,
    }
    """
    try:
        model = getattr(llama, "model", None)
        if model is None:
            return None

        # 1) æœ‰äº›å®ç°ä¼šæŠŠ offloader æŒ‚åœ¨ model ä¸Š
        off = getattr(model, "kv_offloader", None)

        # 2) å¦åˆ™ä»ç¬¬ä¸€å±‚ attention ä¸Šæ‰¾ offloader
        if off is None and hasattr(model, "layers"):
            for blk in getattr(model, "layers", []):
                attn = getattr(blk, "attention", None)
                if attn is None:
                    continue
                off = getattr(attn, "offloader", None)
                if off is not None:
                    break

        if off is None or not hasattr(off, "get_cache_stats"):
            return None

        stats = off.get_cache_stats()
        if not isinstance(stats, dict):
            return None

        # æ¸…æ´—æˆ JSON-friendly çš„ç®€å•ç±»å‹
        cleaned = {}
        for k, v in stats.items():
            if isinstance(v, (int, float)) or v is None:
                cleaned[k] = v
            else:
                try:
                    cleaned[k] = float(v)
                except Exception:
                    cleaned[k] = str(v)
        return cleaned

    except Exception as e:
        print(f"[KV][WARN] extract_kv_cache_stats() failed: {e}")
        return None

# ---------- è¾…åŠ©ï¼šå›ºå®šè§„åˆ™ç”Ÿæˆ JSON/CSV è·¯å¾„ ----------
def _sanitize_for_filename(s: str) -> str:
    if not s:
        return ""
    return re.sub(r"[^A-Za-z0-9_.+-]", "-", s)

def build_output_paths(log_dir: Path, run_id: str, mode: str) -> tuple[Path, Path]:
    ts = datetime.now().strftime("%Y%m%d-%H%M%S")
    tag = _sanitize_for_filename(RUN_TAG)
    stem = f"{ts}_{run_id}_{mode}" if not tag else f"{ts}_{tag}_{run_id}_{mode}"
    json_path = log_dir / f"{stem}.json"
    csv_path  = log_dir / f"{stem}.csv"
    return json_path, csv_path

# ---------- è¿è¡Œä¸»æµç¨‹ ----------
def main():
    global PROFILER

    # å›ºå®šç›®å½•ï¼šè‡ªåŠ¨åˆ›å»º
    LOG_DIR.mkdir(parents=True, exist_ok=True)

    # ä¸å†ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼›run_id å¯é™„å¸¦ RUN_TAG
    base_tag = RUN_TAG.strip() or None
    PROFILER = InferenceProfiler(run_name=base_tag)

    # åŸºç¡€ç³»ç»Ÿå¼€é”€æ”¶æ•›
    os.environ.setdefault("OMP_NUM_THREADS",  "8")
    os.environ.setdefault("MALLOC_ARENA_MAX", "2")
    # PYTORCH_CUDA_ALLOC_CONF å·²åœ¨é¡¶éƒ¨è®¾ç½®ï¼ˆå¿…é¡»åœ¨ import torch å‰ï¼‰

    # ============================================================
    # â­ å¼‚æ­¥æ»‘åŠ¨çª—å£ - RTX 5080 (16GB) + 125GB RAM å®æµ‹ä¼˜åŒ–
    # ============================================================
    # ç¡¬ä»¶å®¹é‡ï¼šGPU 12GB å¯ç”¨ â†’ 11 ç»„ | RAM 110GB å¯ç”¨ â†’ 60 å±‚
    # ç­–ç•¥ï¼šå¼‚æ­¥çª—å£ + é€‚åº¦å¹¶å‘ + RAM ç¼“å­˜ä¼˜åŒ–
    # ============================================================

    # â­â­â­ P0 ä¿®å¤: å¢åŠ  warmup å±‚æ•°ï¼Œç¡®ä¿å®Œæ•´ overlap
    # å•å±‚è®¡ç®— 100msï¼Œå¯ä»¥ overlap 4 ç»„ H2D (æ¯ç»„ 25ms)
    # Warmup è‡³å°‘éœ€è¦è¦†ç›–: åˆå§‹å±‚ + é¢„å–æ·±åº¦ = 12 å±‚
    GPU_AHEAD_LAYERS = 6# é¢„å– 6 ç»„ï¼ˆ3 å±‚ï¼‰- é€‚é… 11 ç»„å®¹é‡
    GPU_MAX_GROUPS   = 12
    GPU_WARMUP_LAYERS = 6# â­ 6 â†’ 12 å±‚ï¼ˆ24 ç»„ï¼‰ï¼Œç¡®ä¿å‰ 12 å±‚å®Œå…¨ overlap
    CPU_CACHE_LAYERS = 47# CPU ç¼“å­˜ 50 å±‚ï¼ˆ79.5GBï¼Œå®‰å…¨ä½™é‡ï¼‰

    # === H2D å¹¶å‘æ§åˆ¶ï¼ˆâ­â­â­ P0 ä¼˜åŒ–ï¼šPCIe Gen5 + RTX 5080 é«˜å¸¦å®½é…ç½®ï¼‰ ===
    # PCIe Gen5 x16 å¸¦å®½: 64GB/s (Gen4çš„2å€)
    # 70Bæ¨¡å‹æ¯ç»„æƒé‡~1.5GB â†’ Gen5å•æ¬¡H2Dåªéœ€~25msï¼ˆGen4çš„ä¸€åŠï¼‰
    # æ›´é«˜å¸¦å®½æ„å‘³ç€éœ€è¦æ›´é«˜å¹¶å‘åº¦æ‰èƒ½é¥±å’ŒPCIeï¼Œé¿å…æµæ°´çº¿ç©ºéš™
    os.environ.setdefault("WSM_H2D_BASE_CONCURRENCY",  "8")   # â­ 5â†’16ï¼ˆGen5é«˜å¸¦å®½ï¼ŒåŸºç¡€å¹¶å‘ï¼‰
    os.environ.setdefault("WSM_H2D_PREFILL_MULT",      "3")  # Prefill: 32 å¹¶å‘
    os.environ.setdefault("WSM_H2D_DECODE_MULT",       "3")  # â­ 1.0â†’1.5ï¼ˆDecode: 24 å¹¶å‘ï¼‰
    os.environ.setdefault("WSM_MAX_INFLIGHT_GROUPS",   "32")   # â­ 16â†’32ï¼ˆInflight ä¸Šé™ï¼ŒåŒ¹é…å¹¶å‘ï¼‰
    os.environ.setdefault("WSM_H2D_GROUP_BACKLOG_MAX", "96")   # â­ 48â†’96ï¼ˆH2D é˜Ÿåˆ—ï¼ŒGen5éœ€è¦æ›´æ·±é˜Ÿåˆ—ï¼‰

    # === å¼‚æ­¥é€å‡ºæœºåˆ¶ ===
    os.environ.setdefault("WSM_EVICT_QUEUE_SIZE",      "96")   # é€å‡ºé˜Ÿåˆ—å®¹é‡
    os.environ.setdefault("WSM_BG_WORKERS",            "8")    # åå°çº¿ç¨‹æ± 

    # === GPU çª—å£é…ç½® ===
    os.environ.setdefault("WSM_GPU_MAX_GROUPS",        str(GPU_MAX_GROUPS))
    os.environ.setdefault("WSM_GPU_AHEAD_GROUPS",      str(GPU_AHEAD_LAYERS))

    # å•å±‚ 100ms å¯ overlap 4 ç»„ H2Dï¼Œè®¾ç½® 8 ä¿è¯å……è¶³æµæ°´çº¿
    os.environ.setdefault("WSM_GROUP_PREFETCH_DEPTH",  "6")  # â­ 6 â†’ 8
    os.environ.setdefault("WSM_GPU_AHEAD",             str(GPU_AHEAD_LAYERS))
    os.environ.setdefault("WSM_GPU_BEHIND",            "2")    # ä¿ç•™æœ€è¿‘ 2 å±‚

    # === é¢„å–ç­–ç•¥ ===
    os.environ.setdefault("WSM_BALANCE_PREFETCH",      "1")
    os.environ.setdefault("WSM_PAIR_AHEAD",            "2")
    os.environ.setdefault("WSM_KIND_AHEAD_CAP",        "2")
    os.environ.setdefault("WSM_EVICT_FINISHED",        "1")    # å¯ç”¨å®Œæˆåé€å‡º
    os.environ.setdefault("WSM_CPU_EVICT_AFTER_USE",   "0")    # å¼‚æ­¥æ¨¡å¼ä¸‹ç¦ç”¨ç«‹å³é€å‡º

    # === è°ƒè¯•ä¸ç›‘æ§ ===
    os.environ.setdefault("WSM_GRP_RETAIN_MS",         "0")
    os.environ.setdefault("WSM_SKIP_PRELOAD_WAIT",     "1")    # å¯ç”¨å¼‚æ­¥é¢„åŠ è½½
    os.environ.setdefault("WSM_DEBUG_PREFETCH",        "1")    # å¯ç”¨è¯¦ç»†æ—¥å¿—
    os.environ.setdefault("WSM_VERBOSE_MISMATCH",      "0")    # ç”Ÿäº§ç¯å¢ƒå…³é—­

    # === CPU é¢„å–ä¼˜åŒ–ï¼ˆRAM å¯å®¹çº³ 60 å±‚ï¼‰ ===
    os.environ.setdefault("WSM_POOLED_CPU_READ",       "1")
    os.environ.setdefault("WSM_CPU_PF_WORKERS",        "12")   # CPU é¢„å–çº¿ç¨‹æ•°ï¼ˆ50% CPUï¼‰
    os.environ.setdefault("WSM_REBALANCE_SYNC",        "0")    # å¼‚æ­¥é‡å¹³è¡¡

    # === SSDâ†’CPU æµæ°´çº¿ ===
    os.environ.setdefault("WSM_CPU_PREFETCH_DISTANCE", str(CPU_CACHE_LAYERS))   # CPU é¢„å– 50 å±‚
    os.environ.setdefault("WSM_SSD_CONCURRENCY",       "12")    # SSD å¹¶å‘è¯»å–

    # === Prefill ç‰¹å®šä¼˜åŒ– ===
    os.environ.setdefault("PREFILL_CPU_LAYERS",        str(CPU_CACHE_LAYERS))   # Prefill CPU ç¼“å­˜ 50 å±‚
    os.environ.setdefault("PREFILL_GPU_LAYERS",        str(GPU_WARMUP_LAYERS))  # â­ 6 â†’ 12 å±‚
    os.environ.setdefault("PREFILL_PREFETCH_DISTANCE", "16")   # â­ 10 â†’ 16ï¼ˆæ›´è¿œçš„é¢„å–è·ç¦»ï¼‰
    os.environ.setdefault("WSM_WARMUP_LAYERS_GPU",     str(GPU_WARMUP_LAYERS))  # â­ 6 â†’ 12 å±‚
    os.environ.setdefault("WSM_WRAPAROUND_WARMUP",     str(GPU_WARMUP_LAYERS))  # â­ 6 â†’ 12 å±‚
    
    
  

    # ============================================================
    # CPU çª—å£é¢å¤–é…ç½®ï¼ˆå¤ç”¨ä¸Šé¢çš„ CPU_CACHE_LAYERSï¼‰
    # ============================================================
    os.environ.setdefault("WSM_CPU_RING_MODE",     "1")
    os.environ.setdefault("WSM_CPU_RING_OFFSET",   "0")
    os.environ.setdefault("WSM_CPU_CACHE_LAYERS",  str(CPU_CACHE_LAYERS))
    os.environ.setdefault("WSM_CPU_CACHE_CAP_LAYERS", str(CPU_CACHE_LAYERS))
    os.environ.setdefault("WSM_CPU_CACHE_HWM_LAYERS", str(CPU_CACHE_LAYERS))
    os.environ.setdefault("WSM_CPU_CACHE_LWM_LAYERS", str(max(2, CPU_CACHE_LAYERS - 5)))
    os.environ.setdefault("WSM_CPU_BACK_MARGIN",   "1")
    os.environ.setdefault("WSM_KV_THROTTLE_THRESHOLD", "2")
    os.environ.setdefault("WSM_KV_THROTTLE_MS",        "16")

    # é…ç½®æ€»ç»“ï¼ˆä»…æ‰“å°ï¼‰
    print("=" * 80)
    print("ğŸš€ å¼‚æ­¥æ»‘åŠ¨çª—å£ - RTX 5080 (16GB) + 125GB RAM ä¼˜åŒ–é…ç½®")
    print("=" * 80)
    print(f"GPU é¢„å–æ·±åº¦:  {GPU_AHEAD_LAYERS} ç»„")
    print(f"GPU ç»„é¢„ç®—:    {GPU_MAX_GROUPS} ç»„ (æœ€å¤š ~9GB)")
    print(f"CPU ç¼“å­˜å®¹é‡:  {CPU_CACHE_LAYERS} å±‚ (~79.5GB)")
    print(f"H2D å¹¶å‘åº¦:    Prefill 24 | Decode 16")
    print(f"å¼‚æ­¥é€å‡ºé˜Ÿåˆ—:  64 ä»»åŠ¡")
    print(f"åå°çº¿ç¨‹æ± :    6 workers")
    print(f"CPU é¢„å–çº¿ç¨‹:  10 workers")
    print("=" * 80)
    print("âœ… å¼‚æ­¥çª—å£ç‰¹æ€§: é€å‡º/é¢„å–/CPUæ¨è¿› å…¨éƒ¨åœ¨åå°çº¿ç¨‹æ‰§è¡Œ")
    print("âœ… ä¸»çº¿ç¨‹çª—å£æ»‘åŠ¨å»¶è¿Ÿ: <1ms (vs åŒæ­¥æ¨¡å¼ ~20ms)")
    print("=" * 80)

    device = "cuda:0" if torch.cuda.is_available() else "cpu"

    # 1) è¦†ç›– pinned/æ³¨å†Œæ±  + KV æ± 
    with PROFILER.span("apply_runtime_overrides", "setup"):
        apply_runtime_overrides()
    with PROFILER.span("configure_kv_pool", "setup"):
        configure_kv_pool()
    with PROFILER.span("probe_after_runtime_clamp", "non_inference"):
        probe("after runtime clamp")

    # 2) WSMï¼ˆSSD æµå¼ï¼‰æ„é€ å‚æ•°
    PRIME_WINDOW = int(os.getenv("WSM_PRIME_WINDOW", "12"))  # ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤6
    mode_config = {
        "raw_device": RAW_DEV,
        "ssd_manifest_path": MANIFEST,
        "max_cached_layers": CPU_CACHE_LAYERS,         # âœ… ä¿®å¤: å¿…é¡»ä¸ CPU_CAP_VALUE ä¸€è‡´
        "cpu_cache_layers": CPU_CACHE_LAYERS,          # CPU ç¯å½¢å®¹é‡
        "warmup_layers": max(PRIME_WINDOW, GPU_AHEAD_LAYERS + 2),  # âœ… Fix: è‡³å°‘é¢„çƒ­ GPU_AHEAD + 2 å±‚
        "staging_mb": 64,
        "verbose": True,
    }

    # 3) æ„å»ºï¼ˆmeta + SSD æµå¼ï¼‰
    with PROFILER.span("probe_before_build", "non_inference"):
        probe("before LLaMA.build")
    with PROFILER.span("LLaMA.build", "setup"):
        llama = LLaMA.build(
            checkpoints_dir=CKPT_DIR,
            load_model=False,           # ä¸æŠŠ checkpoint å…¨è½½å…¥ CPU
            device=device,
            max_seq_len=4096,
            max_batch_size=32,
            topk_blk=8,
            mode="mixed",
            mode_config=mode_config
        )
        s = stream_mnt.get_streams("cuda:0")
        print("h2d_mha:", s.weight_h2d_mha)
        print("h2d_ffn:", s.weight_h2d_ffn)
        print("cmp_mha:", s.compute_mha)
        print("cmp_ffn:", s.compute_ffn)
        print("kv_h2d:", s.kv_h2d, "kv_d2h:", s.kv_d2h)
    with PROFILER.span("probe_after_build", "non_inference"):
        probe("after LLaMA.build")

    # ç»‘å®š WSM è¡¥ä¸ï¼šä»…ä¸º profiler è®¡æ—¶ï¼Œwait_group_ready çš„å¼‚æ­¥é€»è¾‘å·²åœ¨ WSM ä¸»ç±»å®ç°
    wsm = getattr(llama, "weight_streaming_manager", None)
    if wsm is not None:
        # åˆå§‹åŒ– pipeline watermark ç»Ÿè®¡å­—å…¸
        wsm._pipeline_watermark = {}

        # åŒ…è£… wait_group_ready ä»¥æ·»åŠ  profiler è®¡æ—¶
        _patch_wsm_for_profiling(wsm)
        original_wait = wsm.wait_group_ready
        wsm.wait_group_ready = types.MethodType(_wrap_wait_group_ready(original_wait), wsm)

        # ä¿ç•™ ensure_module_on_gpu çš„ CPU stub loader è¡¥ä¸
        wsm._ensure_module_on_gpu = types.MethodType(_patched_ensure_module_on_gpu, wsm)
        print("[WSM PATCH] Profiler wrapper + CPU stub loader enabled")

        # â­â­â­ P0 ä¼˜åŒ–ï¼šGPUçª—å£é¢„çƒ­ï¼ˆé¿å…å†·å¯åŠ¨ï¼Œå‰Nå±‚å¹¶è¡ŒH2Dï¼‰
        with PROFILER.span("gpu_window_warmup", "setup"):
            warmup_layers = GPU_WARMUP_LAYERS  # â­ ä½¿ç”¨é…ç½®çš„ 12 å±‚
            print(f"[WSM WARMUP] Preloading first {warmup_layers} layers to GPU...")
            for layer_idx in range(min(warmup_layers, wsm.n_layers)):
                try:
                    # å¼‚æ­¥é¢„å–attnå’Œffnç»„ï¼ˆä¸é˜»å¡ï¼Œè®©H2Dåœ¨åå°å¹¶è¡Œï¼‰
                    wsm.prefetch_group_async(layer_idx, "attn", reason="warmup")
                    wsm.prefetch_group_async(layer_idx, "ffn", reason="warmup")
                except Exception as e:
                    print(f"[WSM WARMUP] Layer {layer_idx} prefetch failed: {e}")
            print(f"[WSM WARMUP] Warmup requests sent (async), first {warmup_layers} layers (24 groups) will be ready before inference")

            # â­â­â­ é¢å¤–ä¿®å¤ï¼šç­‰å¾… warmup å®Œæˆåï¼Œç»§ç»­é¢„å–åç»­å±‚å»ºç«‹æµæ°´çº¿
            # åœ¨æ¨ç†å¼€å§‹å‰ï¼Œé¢„å– L12-L20ï¼Œç¡®ä¿ L12+ ä¹Ÿèƒ½ overlap
            # print(f"[WSM WARMUP] Extending prefetch pipeline to L{warmup_layers + 8}...")
            # for layer_idx in range(warmup_layers, min(warmup_layers + 8, wsm.n_layers)):
            #     try:
            #         wsm.prefetch_group_async(layer_idx, "attn", reason="warmup_extend")
            #         # ä¸é¢„å– ffnï¼ŒèŠ‚çœå¹¶å‘æ§½ä½
            #     except Exception as e:
            #         pass
            # print(f"[WSM WARMUP] Extended pipeline ready")

    PROFILER.wrap_model_forward(llama.model)

    # 4) è¯»å– prompt + å®‰å…¨è£å‰ªï¼ˆmax_gen_len=32ï¼‰
    batch_size = 1
    max_gen_len = 32

    with PROFILER.span("read_prompt_file", "prompt"):
        try:
            prompt_path = PROMPT_TXT
            file_content = prompt_path.read_text(encoding="utf-8").strip()

            # è§£æå¤šä¸ªpromptsï¼ˆæŒ‰ "===== PROMPT XXXX =====" åˆ†éš”ï¼‰
            import re
            prompt_blocks = re.split(r'=====\s*PROMPT\s+\d+\s+.*?=====\s*\n', file_content)
            # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
            prompt_blocks = [p.strip() for p in prompt_blocks if p.strip()]

            # å–å‰batch_sizeä¸ªprompts
            prompts = prompt_blocks[:batch_size]
            if len(prompts) < batch_size:
                # å¦‚æœpromptsä¸è¶³ï¼Œé‡å¤æœ€åä¸€ä¸ªpromptæ¥å¡«å……
                print(f"Warning: Only {len(prompts)} prompts found, padding to {batch_size}")
                while len(prompts) < batch_size:
                    prompts.append(prompts[-1])

            print(f"Loaded {len(prompts)} prompts for batch_size={batch_size}")

        except Exception as e:
            raise RuntimeError(f"æ— æ³•è¯»å– {prompt_path}: {e}")

    with PROFILER.span("tokenize_and_clip", "prompt"):
        max_prompt_tokens = llama.args.max_seq_len - max_gen_len

        # å¯¹æ¯ä¸ªpromptè¿›è¡Œtokenizeå’Œè£å‰ª
        clipped_prompts = []
        for prompt in prompts:
            tok = llama.tokenizer.encode(prompt, add_special_tokens=False)
            if len(tok) > max_prompt_tokens:
                tok = tok[-max_prompt_tokens:]
                prompt = llama.tokenizer.decode(tok)
            clipped_prompts.append(prompt)

        prompts = clipped_prompts
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªpromptçš„tokenæ•°ä½œä¸ºç»Ÿè®¡ï¼ˆå‡è®¾æ‰€æœ‰prompté•¿åº¦ç›¸ä¼¼ï¼‰
        tokens_in_count = len(llama.tokenizer.encode(prompts[0], add_special_tokens=False))

    # 5) çœŸæ­£æ¨ç†ï¼ˆdecodeï¼‰
    with PROFILER.span("probe_before_infer", "non_inference"):
        probe("before inference (decode)")
    with PROFILER.inference_scope():  # ç«¯åˆ°ç«¯æ¨ç†æ—¶é—´
        out_tokens, out_texts = llama.text_completion(
            prompts=prompts,
            temperature=0.0,
            max_gen_len=max_gen_len,
            batch_size=batch_size,
        )
    with PROFILER.span("probe_after_infer", "non_inference"):
        probe("after inference (decode)")

    # ==== ç»Ÿè®¡ tokens_out ====
    def _count_output_tokens(out_tokens_obj):
        try:
            if isinstance(out_tokens_obj, (list, tuple)):
                if len(out_tokens_obj) > 0 and isinstance(out_tokens_obj[0], (list, tuple)):
                    return len(out_tokens_obj[0])
                return len(out_tokens_obj)
            if torch.is_tensor(out_tokens_obj):
                return int(out_tokens_obj.numel())
        except Exception:
            pass
        return None

    tokens_out_count = _count_output_tokens(out_tokens)

    # ==== æ±‡æ€»ä¸ä¿å­˜ ====
    mode = classify_mode(llama)
    kv_stats = extract_kv_cache_stats(llama)

    # æ–°å¢ï¼šä» WSM å¯¹è±¡ä¸Šé‡‡é›†ä¸€æ¬¡è¿è¡ŒæœŸç»Ÿè®¡ï¼Œä¼ ç»™ Profiler
    wsm_runtime = None
    if hasattr(llama, "weight_streaming_manager"):
        try:
            wsm = llama.weight_streaming_manager
            rt = {}

            # pipeline æ°´ä½ï¼ˆæ¥è‡ª _wrap_wait_group_ready / _patch_wsm_for_profilingï¼‰
            pipe = getattr(wsm, "_pipeline_watermark", None)
            if isinstance(pipe, dict):
                rt["pipeline"] = dict(pipe)

            # SSD backend é™æ€ / è¿è¡ŒçŠ¶æ€
            if hasattr(wsm, "get_ssd_stats"):
                try:
                    rt["ssd"] = wsm.get_ssd_stats()
                except Exception:
                    pass

            # H2D timeout/retry ç»Ÿè®¡ï¼ˆå¦‚æœå®ç°äº†ï¼‰
            if hasattr(wsm, "get_h2d_timeout_stats"):
                try:
                    rt["h2d"] = wsm.get_h2d_timeout_stats()
                except Exception:
                    pass

            if rt:
                wsm_runtime = rt
        except Exception:
            wsm_runtime = None

    PROFILER.finalize(
        tokens_in=tokens_in_count,
        tokens_out=tokens_out_count,
        extra_meta={"llama_mode": mode, "device_str": str(device)},
        kv_stats=kv_stats or None,
        wsm_runtime=wsm_runtime,
    )


    # è‡ªåŠ¨ç”Ÿæˆ JSON/CSV è·¯å¾„å¹¶å„ä¿å­˜ä¸€æ¬¡
    json_path, csv_path = build_output_paths(LOG_DIR, PROFILER.run_id, mode)
    PROFILER.save(str(json_path))
    PROFILER.save(str(csv_path))
    print(f"[Profiler] JSON: {json_path}")
    print(f"[Profiler] CSV : {csv_path}")
    
    
    # æ§åˆ¶å°æ‘˜è¦ï¼Œæ–¹ä¾¿ç›´æ¥æŠ„åˆ°è®ºæ–‡è¡¨æ ¼
    summary = PROFILER.result
    t = summary.get("timings", {})
    kv = summary.get("kv_cache", {})
    print(
        "\n[SUMMARY] e2e_ms={e2e}, warmup_ms={warmup}, "
        "ftl_ms={ftl}".format(
            e2e=t.get("e2e_ms"),
            warmup=t.get("warmup_total_ms"),
            ftl=t.get("first_token_latency_ms"),
        )
    )
    if kv:
        print(
            "[SUMMARY] KV cache: hits={hits}, misses={misses}, "
            "evictions={evictions}, hit_ratio={ratio}".format(
                hits=kv.get("hits"),
                misses=kv.get("misses"),
                evictions=kv.get("evictions"),
                ratio=kv.get("hit_ratio"),
            )
        )
    else:
        print("[SUMMARY] KV cache: <no stats found on llama / kv_cache object>")

    # ==== è¾“å‡ºç”Ÿæˆæ–‡æœ¬ï¼ˆä¸å½±å“è®¡æ—¶ï¼‰====
    print(f"\n========== Generation (batch_size={batch_size}, len={max_gen_len}) ==========")
    # åªæ˜¾ç¤ºå‰3ä¸ªå’Œæœ€å1ä¸ªï¼Œé¿å…è¾“å‡ºå¤ªé•¿
    for i in [0, 1, 2, batch_size-1]:
        if i < len(out_texts):
            print(f"\n--- Batch {i} ---")
            print(out_texts[i][:200] + "..." if len(out_texts[i]) > 200 else out_texts[i])
    print("=========================================")

if __name__ == "__main__":
    main()
