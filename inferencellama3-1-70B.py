#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Llama3.1-70B æ¨ç† + è½»é‡çº§ Profilerï¼ˆJSON/CSV è‡ªåŠ¨å†™å…¥å›ºå®šç›®å½•ï¼‰
- è®°å½•ä¼šå½±å“ inference çš„å…³é”®è·¯å¾„ç”¨æ—¶ï¼ˆprefill / decode per-token / e2e / FTL è¿‘ä¼¼ / ååï¼‰
- åŒæ—¶è®°å½•ä¸ä¼šå½±å“ inference çš„å‡†å¤‡/æ¢é’ˆ/æ—¥å¿—æ—¶é—´ï¼ˆnon_inference ç±»åˆ«ï¼‰
- å¯¹ WSM ä¸¤ä¸ªå…³é”®å‡½æ•°ï¼ˆwait_group_ready / _ensure_module_on_gpuï¼‰åšåŸ‹ç‚¹ç»Ÿè®¡
- é‡‡ç”¨ CUDA Events é€ token è®¡æ—¶ï¼Œç»Ÿä¸€åŒæ­¥ï¼Œå°½é‡ä½æ‰°åŠ¨
- ç”Ÿæˆç»“æœå›ºå®šå†™å…¥ LOG_DIRï¼Œè‡ªåŠ¨è¾“å‡º JSON + CSV ä¸¤ç§æ ¼å¼
"""

import os
from pathlib import Path
import types
import json, csv, uuid, platform, math, time, re
from datetime import datetime, timezone
from contextlib import contextmanager, nullcontext

# ğŸ”¥ CUDA å†…å­˜åˆ†é…å™¨é…ç½®ï¼ˆå¿…é¡»åœ¨ import torch ä¹‹å‰ï¼‰
# expandable_segments ä¸å¼‚æ­¥æµæ“ä½œå¯èƒ½æœ‰å†²çªï¼Œæš‚æ—¶ç¦ç”¨
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"  # é™åˆ¶åˆ†å—å¤§å°ï¼Œå‡å°‘ç¢ç‰‡

# ğŸ”¥ WSM æ— å…œåº•ç­–ç•¥ï¼šé”å®šäº‹ä»¶é©±åŠ¨è°ƒåº¦ï¼Œç¦ç”¨åŒæ­¥å…œåº• (no-fallback)
os.environ["WSM_NO_FALLBACK"] = "1"

import torch

# ===== ä½ å¯ä»¥æ”¹è¿™é‡Œï¼šæ—¥å¿—è¾“å‡ºç›®å½• & è¿è¡Œæ ‡ç­¾ï¼ˆå¯ç•™ç©ºï¼‰ =====
LOG_DIR = Path("/home/roger/logs")   # è‡ªåŠ¨åˆ›å»º
RUN_TAG = ""                         # ä¾‹å¦‚ "ablation-a1"ï¼›ç•™ç©ºåˆ™è‡ªåŠ¨ä»…ç”¨ run_id

# ===== é¡¹ç›®å†…æ¨¡å— =====
from llama3.generator import LLaMA
from llama3.config import KVCacheArgs, load_runtime_config, runtime_config_to_dict
from llama3 import generator as _gen, stream_mnt

# ========== build() åŒ…è£…ï¼šåªåŠ æ—¥å¿—ï¼Œä¸æ”¹åº“ ==========
_orig_build = _gen.LLaMA.build

def _debug_build(*args, **kw):
    mode       = kw.get("mode", None)
    load_model = kw.get("load_model", None)
    mode_cfg   = (kw.get("mode_config", {}) or {})
    raw_dev    = mode_cfg.get("raw_device")
    manifest   = mode_cfg.get("manifest_path") or mode_cfg.get("ssd_manifest_path")

    print(f"[MODE-DECISION] LLaMA.build(mode={mode}, load_model={load_model})")
    use_raw_ssd = (mode in {"ssd", "mixed"}) or (mode_cfg.get("weight_source") == "raw-ssd")
    print(f"[MODE-DECISION] use_raw_ssd={use_raw_ssd} raw_device={raw_dev} manifest={manifest}")

    llama = _orig_build(*args, **kw)

    has_wsm = hasattr(llama, "weight_streaming_manager")
    if has_wsm:
        wsm = llama.weight_streaming_manager
        ssd = bool(getattr(wsm, "ssd_enabled", False) or getattr(wsm, "ssd", None))
        print(f"[MODE-DECISION] built: WSM present, ssd_enabled={ssd}")
    else:
        print("[MODE-DECISION] built: NO WSM (å¯èƒ½æ˜¯ full-cpu/full-gpu/æ—§ streaming)")
    return llama

_gen.LLaMA.build = staticmethod(_debug_build)

# ======= è½»é‡çº§ Profilerï¼ˆä½æ‰°åŠ¨ï¼›CUDA Eventsï¼›ä¿å­˜ JSON/CSVï¼‰ =======
PROFILER = None  # å…¨å±€å¥æŸ„

def _now_utc():
    return datetime.now(timezone.utc).isoformat()

def _flatten_extras(extras: dict):
    out = {}
    for k,v in (extras or {}).items():
        out[k] = v if (isinstance(v,(int,float,str,bool)) or v is None) else str(v)
    return out

class InferenceProfiler:
    def __init__(self, run_name: str | None = None):
        self.run_id   = run_name or f"run-{uuid.uuid4().hex[:8]}"
        self.t0_ns    = time.perf_counter_ns()
        self.timeline = []   # å¢™é’Ÿé˜¶æ®µ
        self.active   = False
        self.cuda     = torch.cuda.is_available()
        self.forward_events = []      # GPUï¼š[(kind,batch,seqlen,start_ev,end_ev)]
        self.forward_events_cpu = []  # CPU å›é€€ï¼š[(kind,batch,seqlen,dt_ms)]
        self.bookkeep  = {}
        self.meta      = {
            "started_at_utc": _now_utc(),
            "python": platform.python_version(),
            "torch": getattr(torch, "__version__", "unknown"),
            "device": ("cuda" if self.cuda else "cpu"),
        }
        if self.cuda:
            try:
                self.meta["cuda_device_name"] = torch.cuda.get_device_name(0)
                self.meta["cuda_cc"] = ".".join(map(str, torch.cuda.get_device_capability(0)))
            except Exception:
                pass

    @contextmanager
    def span(self, name: str, category: str, **extras):
        s = time.perf_counter_ns()
        try:
            yield
        finally:
            e = time.perf_counter_ns()
            rec = {
                "name": name, "cat": category,
                "t_start_ms": (s - self.t0_ns) / 1e6,
                "t_end_ms":   (e - self.t0_ns) / 1e6,
                "dur_ms":     (e - s) / 1e6,
            }
            rec.update(_flatten_extras(extras))
            self.timeline.append(rec)
            if name == "inference_e2e":
                self.bookkeep["inference_s_ns"] = s
                self.bookkeep["inference_e_ns"] = e

    @contextmanager
    def inference_scope(self):
        self.active = True
        with self.span("inference_e2e", "inference"):
            yield
        self.active = False

    def wrap_model_forward(self, model):
        orig = model.forward

        def _classify_args(args, kwargs):
            cand = None
            for k in ("tokens", "input_ids"):
                t = kwargs.get(k, None)
                if torch.is_tensor(t) and t.dim() == 2:
                    cand = t; break
            if cand is None:
                for a in args:
                    if torch.is_tensor(a) and a.dtype in (torch.long, torch.int32, torch.int64) and a.dim() == 2:
                        cand = a; break
            if cand is None:
                return None, None
            B, T = int(cand.size(0)), int(cand.size(1))
            return B, T

        def wrapped(*args, **kwargs):
            if not self.active:
                return orig(*args, **kwargs)
            B, T = _classify_args(args, kwargs)
            kind = "prefill" if (T is not None and T > 1) else ("decode" if T == 1 else "unknown")

            if self.cuda:
                s_ev = torch.cuda.Event(enable_timing=True)
                e_ev = torch.cuda.Event(enable_timing=True)
                s_ev.record()
                out = orig(*args, **kwargs)
                e_ev.record()
                self.forward_events.append((kind, B, T, s_ev, e_ev))
                return out
            else:
                s = time.perf_counter_ns()
                out = orig(*args, **kwargs)
                e = time.perf_counter_ns()
                self.forward_events_cpu.append((kind, B, T, (e - s) / 1e6))
                return out

        model.forward = wrapped

    # ä¾› WSM è¡¥ä¸ä½¿ç”¨
    def span_if_active(self, name, category, **extras):
        return self.span(name, category, **extras) if self is not None else nullcontext()

    def _compute_decode_stats(self, arr):
        if not arr:
            return {"count": 0}
        s = sorted(arr)
        q = lambda p: s[int((len(s)-1)*p)]
        return {
            "count": len(arr),
            "sum_ms": sum(arr),
            "mean_ms": sum(arr)/len(arr),
            "p50_ms": q(0.50),
            "p90_ms": q(0.90),
            "p99_ms": q(0.99),
        }

    def finalize(self, tokens_in: int | None, tokens_out: int | None, extra_meta: dict | None = None):
        if extra_meta: self.meta.update(_flatten_extras(extra_meta))
        # ç»Ÿä¸€åŒæ­¥åè¯»å– CUDA Event
        decode_ms = []
        prefill_total = 0.0
        if self.cuda and self.forward_events:
            torch.cuda.synchronize()
            for kind, B, T, s_ev, e_ev in self.forward_events:
                dt = float(s_ev.elapsed_time(e_ev))  # ms
                if kind == "prefill": prefill_total += dt
                elif kind == "decode": decode_ms.append(dt)
        elif self.forward_events_cpu:
            for kind, B, T, dt in self.forward_events_cpu:
                if kind == "prefill": prefill_total += dt
                elif kind == "decode": decode_ms.append(dt)

        inf_span = next((x for x in self.timeline if x["name"]=="inference_e2e"), None)
        e2e_ms = inf_span["dur_ms"] if inf_span else None
        first_decode_ms = decode_ms[0] if decode_ms else None
        ftl_approx = (prefill_total + first_decode_ms) if (first_decode_ms is not None) else None

        # åˆ†ç±»èšåˆ
        sum_cat = {}
        for ev in self.timeline:
            sum_cat.setdefault(ev["cat"], 0.0)
            sum_cat[ev["cat"]] += float(ev["dur_ms"])

        def _sum_by_name_prefix(prefix):
            items = [ev for ev in self.timeline if ev["name"].startswith(prefix)]
            return {"calls": len(items), "total_ms": sum(float(ev["dur_ms"]) for ev in items)}
        wsm_stats = {
            "wait_group_ready": {"calls": _sum_by_name_prefix("wsm.wait_group_ready")["calls"],
                                 "total_ms": _sum_by_name_prefix("wsm.wait_group_ready")["total_ms"]},
            "ensure_module_on_gpu": {"calls": _sum_by_name_prefix("wsm.ensure_module_on_gpu")["calls"],
                                     "total_ms": _sum_by_name_prefix("wsm.ensure_module_on_gpu")["total_ms"]},
        }

        # åå
        prefill_tps = (float(tokens_in)/ (prefill_total/1000.0)) if (tokens_in and prefill_total>0) else None
        decode_tps  = (float(tokens_out)/ (sum(decode_ms)/1000.0)) if (tokens_out and decode_ms) else None

        self.result = {
            "run": self.meta | {"run_id": self.run_id, "finished_at_utc": _now_utc()},
            "counts": {"tokens_in": tokens_in, "tokens_out": tokens_out},
            "timings": {
                "inference_e2e_ms": e2e_ms,
                "prefill_total_ms": prefill_total if prefill_total>0 else None,
                "first_decode_forward_ms": first_decode_ms,
                "first_token_latency_ms_approx": ftl_approx,
                "decode_stats": self._compute_decode_stats(decode_ms),
                "by_category_ms": sum_cat,
            },
            "throughput": {
                "prefill_toks_per_s": prefill_tps,
                "decode_toks_per_s": decode_tps,
            },
            "wsm": wsm_stats,
            "decode_step_ms": decode_ms,   # å®Œæ•´åºåˆ—
            "timeline": self.timeline,     # æ–¹ä¾¿æº¯æº
        }

    def save(self, path: str):
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        if path.lower().endswith(".json"):
            with open(path, "w", encoding="utf-8") as f:
                json.dump(self.result, f, ensure_ascii=False, indent=2)
        elif path.lower().endswith(".csv"):
            rows = []
            for ev in self.timeline:
                r = {"kind":"span","name":ev["name"],"cat":ev["cat"],
                     "t_start_ms":ev["t_start_ms"],"t_end_ms":ev["t_end_ms"],"dur_ms":ev["dur_ms"]}
                rows.append(r)
            for i,dt in enumerate(self.result.get("decode_step_ms", [])):
                rows.append({"kind":"decode_step","name":f"decode_{i:04d}","cat":"inference","t_start_ms":"", "t_end_ms":"", "dur_ms":dt})
            with open(path, "w", newline="", encoding="utf-8") as f:
                w = csv.DictWriter(f, fieldnames=["kind","name","cat","t_start_ms","t_end_ms","dur_ms"])
                w.writeheader(); w.writerows(rows)
        else:
            with open(path + ".json", "w", encoding="utf-8") as f:
                json.dump(self.result, f, ensure_ascii=False, indent=2)

# ===== è·¯å¾„ä¸å¸¸é‡ï¼ˆæŒ‰ä½ çš„ç¯å¢ƒï¼‰ =====
PROMPT_TXT = Path("/home/roger/llama3-inference/prompts/prompts_batch512_len2048.txt")
RAW_DEV    = "/dev/nvme0n1p4"
MANIFEST   = "/data1/70b-fixed.runtime_manifest.json"
CKPT_DIR   = "/home/roger/.llama/checkpoints/Llama3.1-70B"

# ---------- ç³»ç»Ÿ/GPU å†…å­˜å¿«ç…§ ----------
def _read_status():
    def _grep(path, keys):
        out = {}
        try:
            with open(path, "r") as f:
                for line in f:
                    for k in keys:
                        if line.startswith(k + ":"):
                            out[k] = line.split(":")[1].strip()
        except Exception:
            pass
        return out
    s = _grep("/proc/self/status", ["VmRSS","VmHWM","VmLck"])
    m = _grep("/proc/meminfo", ["MemAvailable","CommitLimit","Committed_AS","Cached","Buffers"])
    return s, m

def _gpu_mem():
    if not torch.cuda.is_available():
        return {}
    dev = torch.cuda.current_device()
    st  = torch.cuda.memory_stats(dev)
    return {
        "alloc_GB": st.get("allocated_bytes.all.current", 0)/(1<<30),
        "rsrv_GB":  st.get("reserved_bytes.all.current", 0)/(1<<30),
    }

def probe(stage: str):
    s, m = _read_status()
    g    = _gpu_mem()
    print(f"\n[MEM] {stage}")
    print(f"  VmRSS={s.get('VmRSS','?')}  VmLck(pinned)={s.get('VmLck','?')}  "
          f"CommitLimit={m.get('CommitLimit','?')}  Committed_AS={m.get('Committed_AS','?')}  "
          f"MemAvailable={m.get('MemAvailable','?')}")
    if g:
        print(f"  GPU: allocated={g['alloc_GB']:.2f} GiB  reserved={g['rsrv_GB']:.2f} GiB")
    print()

# ---------- æ‰«æå‚æ•°åœ¨ meta/cpu/cuda ä¸Šçš„å ç”¨ ----------
def dump_param_inventory(model, tag):
    buckets = {"cpu":0, "cuda":0, "meta":0, "other":0}
    big_cpu = []
    for n,p in model.named_parameters(recurse=True):
        b = p.numel() * p.element_size()
        if getattr(p, "is_meta", False):
            buckets["meta"] += b
        elif hasattr(p, "device"):
            t = p.device.type
            if t == "cpu":
                buckets["cpu"] += b
                if b >= (64<<20):
                    big_cpu.append((n,b))
            elif t == "cuda":
                buckets["cuda"] += b
            else:
                buckets["other"] += b
        else:
            buckets["other"] += b
    f = lambda x: f"{x/(1<<30):.2f} GiB"
    print(f"[PARAMS] {tag}: cpu={f(buckets['cpu'])}, cuda={f(buckets['cuda'])}, meta={f(buckets['meta'])}, other={f(buckets['other'])}")
    if big_cpu:
        big_cpu.sort(key=lambda x:-x[1])
        print("  [big-cpu] top:")
        for n,b in big_cpu[:10]:
            print(f"   - {n}  {b/(1<<20):.1f} MiB")

# ---------- è¿è¡Œæ—¶è¦†ç›–ï¼šæ”¶æ•› pinned/æ³¨å†Œæ±  ----------
def apply_runtime_overrides():
    """
    æŠŠæ³¨å†Œæ€»é‡é’³åœ¨ â‰¤256MiBï¼Œå¹¶æŠŠ EXTENT_BYTES é™åˆ° 1MiBï¼Œé™ä½é«˜é˜¶é¡µ order å‹åŠ›ã€‚
    """
    cfg = load_runtime_config({
        "pinned": {
            "WEIGHT_PINNED_BYTES":      8  << 30,
            "KV_PINNED_BYTES":          8  << 30,
            "EXTENT_BYTES":             1  << 20,   # 1MiB
            "PINNED_REGISTER_CHUNK":   16  << 20,   # 16MiB
            "PINNED_REGISTER_N":            8,      # 128MiB
        },
        "regpool": {
            "REG_POOL_N_BUFFERS":           8,
            "REG_POOL_BUF_BYTES":     16 << 20,     # ~128MiB ä¼ é€å¸¦
        },
        "io": {
            "RAW_IO_QD_WRITE":             24,      # å†™é˜Ÿåˆ—æ·±åº¦
            "IO_RAW_THROTTLE_MS":          30,      # å†™å¸¦å®½çª—å£
        }
    })
    D = runtime_config_to_dict(cfg)
    p = D["pinned"]
    need  = int(p["WEIGHT_PINNED_BYTES"])
    chunk = int(p["PINNED_REGISTER_CHUNK"])
    target_total = min(need // 2, 256 << 20)  # ç›®æ ‡ â‰¤ 256MiB
    newN = max(1, target_total // chunk)
    p["PINNED_REGISTER_N"] = newN
    cfg = load_runtime_config({"pinned": p, "io": D["io"]})
    print("[RuntimeConfig] pinned =", runtime_config_to_dict(cfg)["pinned"])
    print("[RuntimeConfig] io =", runtime_config_to_dict(cfg)["io"])
    return cfg

# ---------- KV æ± ï¼šæ‡’åˆ†é… + å•å— â‰¥ å•ä¸ª KV å— ----------
def configure_kv_pool():
    # DRAM é…ç½®
    KVCacheArgs.dram_limit_gb     = 32.0
    KVCacheArgs.dram_sizing_batch = 32
    KVCacheArgs.block_bytes       = 1 * 1024 * 1024
    KVCacheArgs.preallocate       = False
    KVCacheArgs.lazy_init         = True

    # å…³é—­ push å³æ—¶é•œåƒï¼Œé‡‡ç”¨åç§»/èšåˆå†™ï¼ˆé¿å…ä¸æƒé‡ H2D å†²çªï¼‰
    KVCacheArgs.mirror_on_push = False

    # I/O èŠ‚æµä¸å†™é€Ÿç‡é…ç½®ï¼ˆä¸æƒé‡ H2D ä»²è£ï¼‰
    KVCacheArgs.IO_RAW_THROTTLE_MS     = 25
    KVCacheArgs.NVME_WRITE_TARGET_MBPS = 1500

    if hasattr(KVCacheArgs, "prefer_bf16"):
        KVCacheArgs.prefer_bf16 = True

    print(f"[KVArgs] dram_limit={KVCacheArgs.dram_limit_gb} GiB, "
          f"block_bytes={KVCacheArgs.block_bytes//(1<<20)} MiB, prealloc={KVCacheArgs.preallocate}")
    print(f"[KVArgs] mirror_on_push={KVCacheArgs.mirror_on_push}, "
          f"IO_RAW_THROTTLE_MS={KVCacheArgs.IO_RAW_THROTTLE_MS}, "
          f"NVME_WRITE_TARGET_MBPS={KVCacheArgs.NVME_WRITE_TARGET_MBPS}")

# ---------- è¯†åˆ«â€œå®é™…è¿è¡Œçš„æ¨¡å¼â€ ----------
def classify_mode(llama) -> str:
    """
    è¿”å›ï¼š'ssd-streaming' / 'cpu-gpu-streaming' / 'full-gpu' / 'full-cpu' / 'meta-only'
    """
    m = llama.model
    if hasattr(llama, "weight_streaming_manager"):
        wsm = llama.weight_streaming_manager
        ssd = bool(getattr(wsm, "ssd_enabled", False) or getattr(wsm, "ssd", None))
        cpu_warm = getattr(wsm, "disable_cpu_warm", None)
        mode = "ssd-streaming" if ssd else "cpu-gpu-streaming"
        print(f"[MODE] detected={mode}  (has WSM, ssd={ssd}, disable_cpu_warm={cpu_warm})")
        return mode
    cpu, cuda, meta = 0,0,0
    for _,p in m.named_parameters():
        b = p.numel()*p.element_size()
        if getattr(p, "is_meta", False): meta += b
        elif p.device.type == "cpu":     cpu  += b
        elif p.device.type == "cuda":    cuda += b
    if cuda > 0 and cpu == 0 and meta == 0:
        print("[MODE] detected=full-gpu"); return "full-gpu"
    if cpu  > 0 and cuda == 0 and meta == 0:
        print("[MODE] detected=full-cpu"); return "full-cpu"
    if meta > 0 and cpu == 0 and cuda == 0:
        print("[MODE] detected=meta-only"); return "meta-only"
    print("[MODE] mixed/unrecognized (check PARAMS dump below)")
    return "unknown"

# ===== WSM wait_group_ready åŒ…è£…ï¼ˆä»…æ·»åŠ  Profiler è®¡æ—¶ï¼Œä¸æ”¹é€»è¾‘ï¼‰ =====
def _wrap_wait_group_ready(original_method):
    """
    åŒ…è£… WSM.wait_group_readyï¼Œæ·»åŠ  profiler è®¡æ—¶åŸ‹ç‚¹ã€‚
    å®Œå…¨å¼‚æ­¥çš„ç­‰å¾…é€»è¾‘å·²åœ¨ WSM ä¸»ç±»ä¸­å®ç°ã€‚
    """
    def wrapped(self, layer_idx: int, group: str, compute_stream=None):
        with (PROFILER.span("wsm.wait_group_ready", "wsm", layer_idx=int(layer_idx), group=str(group))
              if (globals().get("PROFILER") is not None) else nullcontext()):
            # è°ƒç”¨ WSM åŸç”Ÿçš„å®Œå…¨å¼‚æ­¥ wait_group_ready
            return original_method(layer_idx, group, compute_stream)
    return wrapped


def _patched_ensure_module_on_gpu(self, m: torch.nn.Module, layer_idx: int | None = None, module_name: str | None = None):
    """
    æ‰©å±•ï¼šæŠŠ **0-size CPU stub** å½“ä½œ meta ä¸€æ ·å¤„ç†ï¼Œä¼˜å…ˆä» CPU cache å–å›å¹¶ä¸Šå¡ã€‚
    å…¶å®ƒæƒ…å†µä»å¤ç”¨åŸå…ˆçš„ _ensure_param_on_gpu() è·¯å¾„ã€‚
    """
    with (PROFILER.span("wsm.ensure_module_on_gpu", "wsm", layer_idx=(None if layer_idx is None else int(layer_idx)), module=str(module_name))
          if (globals().get("PROFILER") is not None) else nullcontext()):
        params_to_replace = {}
        params_full_names = {}

        def _full_name(layer_idx: int, module_name: str, local_param_name: str) -> str:
            if module_name in ("wq", "wk", "wv", "wo"):
                parent = "attention"
            elif module_name in ("w1", "w2", "w3"):
                parent = "feed_forward"
            else:
                parent = module_name or ""
            return f"layers.{layer_idx}.{parent}.{module_name}.{local_param_name}" if parent else f"layers.{layer_idx}.{module_name}.{local_param_name}"

        def _fetch_from_cpu_cache(name: str):
            if (layer_idx is not None) and (layer_idx in self.cpu_cache):
                return self.cpu_cache[layer_idx].get(name)
            return None

        for local_param_name, p in m.named_parameters(recurse=False):
            full_name = None
            if (layer_idx is not None) and (module_name is not None):
                full_name = _full_name(layer_idx, module_name, local_param_name)

            is_meta     = (p.device.type == "meta") or getattr(p, "is_meta", False)
            is_cpu_stub = (p.device.type == "cpu")  and (p.numel() == 0)

            if (is_meta or is_cpu_stub) and self.ssd_enabled and full_name:
                # ç¡®ä¿æœ¬å±‚å·²æœ‰ CPU cacheï¼ˆæ²¡æœ‰å°±ç«‹å³åŠ è½½ï¼‰
                if (layer_idx not in self.cpu_cache):
                    try:
                        self._load_layer_to_cpu(int(layer_idx))
                    except Exception:
                        pass

                cached = _fetch_from_cpu_cache(full_name)
                expected = tuple(getattr(getattr(m, local_param_name), "shape", ()))
                chosen_name, chosen_tensor = None, None

                def _try_pick(names: list[str]):
                    nonlocal chosen_name, chosen_tensor
                    for nm in names:
                        t = _fetch_from_cpu_cache(nm)
                        if t is not None and (not expected or tuple(t.shape) == expected):
                            chosen_name, chosen_tensor = nm, t
                            break

                if cached is not None and (not expected or tuple(cached.shape) == expected):
                    chosen_name, chosen_tensor = full_name, cached
                else:
                    cand = []
                    if module_name in ("wq", "wk", "wv"):
                        cand = [f"layers.{layer_idx}.attention.{x}.{local_param_name}" for x in ("wq","wk","wv")]
                    elif module_name in ("w1", "w2", "w3"):
                        cand = [f"layers.{layer_idx}.feed_forward.{x}.{local_param_name}" for x in ("w1","w2","w3")]
                    else:
                        cand = [full_name]
                    _try_pick(cand)
                    if chosen_tensor is None and cached is not None:
                        chosen_name, chosen_tensor = full_name, cached  # é€€è€Œæ±‚å…¶æ¬¡

                if chosen_tensor is not None:
                    with torch.cuda.stream(self._select_h2d_stream_for(module_name=module_name)):
                        p_gpu = chosen_tensor.to(self.device, non_blocking=True)
                    params_to_replace[local_param_name] = torch.nn.Parameter(p_gpu, requires_grad=p.requires_grad)
                    params_full_names[local_param_name] = chosen_name or full_name
                    if getattr(self, "verbose", False):
                        print(f"[WSM DEBUG] âœ“ Loaded {'meta' if is_meta else 'stub'} param {params_full_names[local_param_name]} to GPU: {tuple(p_gpu.shape)}")
                else:
                    if getattr(self, "verbose", False):
                        print(f"[WSM WARN] CPU cache miss for {full_name} (layer {layer_idx}); will rely on ensure_group_on_gpu() later")
                continue  # è¯¥å‚æ•°å¤„ç†å®Œæ¯•

            # å…¶å®ƒæƒ…å†µï¼šæ²¿ç”¨åŸæ¥çš„ CPUâ†’GPU é€»è¾‘
            self._ensure_param_on_gpu(p, layer_idx, full_name)

        # å®‰è£…æ›¿æ¢åçš„ Parameterï¼Œå¹¶ç»´æŠ¤ name æ˜ å°„
        for pname, new_param in params_to_replace.items():
            m._parameters[pname] = new_param
            full = params_full_names.get(pname)
            if full:
                try:
                    pobj = getattr(m, pname)
                except Exception:
                    pobj = new_param
                self.name_to_param[full] = pobj
                self.param_owner[full]   = (m, pname)

        # buffer ç»´æŒåŸæœ‰ç­–ç•¥ï¼šmetaâ†’materializeï¼ŒCPUâ†’ä¸Šå¡
        for b in m.buffers(recurse=True):
            if getattr(b, "is_meta", False):
                try:
                    b = b.to_empty(device=self.device)
                except Exception:
                    pass
            elif b.device.type == "cpu":
                with torch.cuda.stream(self._select_h2d_stream_for(module_name=module_name)):
                    b_gpu = b.detach().to(self.device, non_blocking=True)
                try:
                    b.data = b_gpu
                except Exception:
                    pass

# ---------- è¾…åŠ©ï¼šå›ºå®šè§„åˆ™ç”Ÿæˆ JSON/CSV è·¯å¾„ ----------
def _sanitize_for_filename(s: str) -> str:
    if not s:
        return ""
    return re.sub(r"[^A-Za-z0-9_.+-]", "-", s)

def build_output_paths(log_dir: Path, run_id: str, mode: str) -> tuple[Path, Path]:
    ts = datetime.now().strftime("%Y%m%d-%H%M%S")
    tag = _sanitize_for_filename(RUN_TAG)
    stem = f"{ts}_{run_id}_{mode}" if not tag else f"{ts}_{tag}_{run_id}_{mode}"
    json_path = log_dir / f"{stem}.json"
    csv_path  = log_dir / f"{stem}.csv"
    return json_path, csv_path

# ---------- è¿è¡Œä¸»æµç¨‹ ----------
def main():
    global PROFILER

    # å›ºå®šç›®å½•ï¼šè‡ªåŠ¨åˆ›å»º
    LOG_DIR.mkdir(parents=True, exist_ok=True)

    # ä¸å†ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼›run_id å¯é™„å¸¦ RUN_TAG
    base_tag = RUN_TAG.strip() or None
    PROFILER = InferenceProfiler(run_name=base_tag)

    # åŸºç¡€ç³»ç»Ÿå¼€é”€æ”¶æ•›
    os.environ.setdefault("OMP_NUM_THREADS",  "8")
    os.environ.setdefault("MALLOC_ARENA_MAX", "2")
    # PYTORCH_CUDA_ALLOC_CONF å·²åœ¨é¡¶éƒ¨è®¾ç½®ï¼ˆå¿…é¡»åœ¨ import torch å‰ï¼‰

    # ============================================================
    # â­ å¼‚æ­¥æ»‘åŠ¨çª—å£ - RTX 5080 (16GB) + 125GB RAM å®æµ‹ä¼˜åŒ–
    # ============================================================
    # ç¡¬ä»¶å®¹é‡ï¼šGPU 12GB å¯ç”¨ â†’ 11 ç»„ | RAM 110GB å¯ç”¨ â†’ 60 å±‚
    # ç­–ç•¥ï¼šå¼‚æ­¥çª—å£ + é€‚åº¦å¹¶å‘ + RAM ç¼“å­˜ä¼˜åŒ–
    # ============================================================

    # â­â­â­ P0 ä¿®å¤: å¢åŠ  warmup å±‚æ•°ï¼Œç¡®ä¿å®Œæ•´ overlap
    # å•å±‚è®¡ç®— 100msï¼Œå¯ä»¥ overlap 4 ç»„ H2D (æ¯ç»„ 25ms)
    # Warmup è‡³å°‘éœ€è¦è¦†ç›–: åˆå§‹å±‚ + é¢„å–æ·±åº¦ = 12 å±‚
    GPU_AHEAD_LAYERS = 6# é¢„å– 6 ç»„ï¼ˆ3 å±‚ï¼‰- é€‚é… 11 ç»„å®¹é‡
    GPU_MAX_GROUPS   = 12
    GPU_WARMUP_LAYERS = 6# â­ 6 â†’ 12 å±‚ï¼ˆ24 ç»„ï¼‰ï¼Œç¡®ä¿å‰ 12 å±‚å®Œå…¨ overlap
    CPU_CACHE_LAYERS = 40# CPU ç¼“å­˜ 50 å±‚ï¼ˆ79.5GBï¼Œå®‰å…¨ä½™é‡ï¼‰

    # === H2D å¹¶å‘æ§åˆ¶ï¼ˆâ­â­â­ P0 ä¼˜åŒ–ï¼šPCIe Gen5 + RTX 5080 é«˜å¸¦å®½é…ç½®ï¼‰ ===
    # PCIe Gen5 x16 å¸¦å®½: 64GB/s (Gen4çš„2å€)
    # 70Bæ¨¡å‹æ¯ç»„æƒé‡~1.5GB â†’ Gen5å•æ¬¡H2Dåªéœ€~25msï¼ˆGen4çš„ä¸€åŠï¼‰
    # æ›´é«˜å¸¦å®½æ„å‘³ç€éœ€è¦æ›´é«˜å¹¶å‘åº¦æ‰èƒ½é¥±å’ŒPCIeï¼Œé¿å…æµæ°´çº¿ç©ºéš™
    os.environ.setdefault("WSM_H2D_BASE_CONCURRENCY",  "8")   # â­ 5â†’16ï¼ˆGen5é«˜å¸¦å®½ï¼ŒåŸºç¡€å¹¶å‘ï¼‰
    os.environ.setdefault("WSM_H2D_PREFILL_MULT",      "3")  # Prefill: 32 å¹¶å‘
    os.environ.setdefault("WSM_H2D_DECODE_MULT",       "2")  # â­ 1.0â†’1.5ï¼ˆDecode: 24 å¹¶å‘ï¼‰
    os.environ.setdefault("WSM_MAX_INFLIGHT_GROUPS",   "32")   # â­ 16â†’32ï¼ˆInflight ä¸Šé™ï¼ŒåŒ¹é…å¹¶å‘ï¼‰
    os.environ.setdefault("WSM_H2D_GROUP_BACKLOG_MAX", "96")   # â­ 48â†’96ï¼ˆH2D é˜Ÿåˆ—ï¼ŒGen5éœ€è¦æ›´æ·±é˜Ÿåˆ—ï¼‰

    # === å¼‚æ­¥é€å‡ºæœºåˆ¶ ===
    os.environ.setdefault("WSM_EVICT_QUEUE_SIZE",      "96")   # é€å‡ºé˜Ÿåˆ—å®¹é‡
    os.environ.setdefault("WSM_BG_WORKERS",            "8")    # åå°çº¿ç¨‹æ± 

    # === GPU çª—å£é…ç½® ===
    os.environ.setdefault("WSM_GPU_MAX_GROUPS",        str(GPU_MAX_GROUPS))
    os.environ.setdefault("WSM_GPU_AHEAD_GROUPS",      str(GPU_AHEAD_LAYERS))
    # â­â­â­ P0 ä¿®å¤: é¢„å–æ·±åº¦å¿…é¡»åŒ¹é…è®¡ç®—æ—¶é—´çª—å£
    # å•å±‚ 100ms å¯ overlap 4 ç»„ H2Dï¼Œè®¾ç½® 8 ä¿è¯å……è¶³æµæ°´çº¿
    os.environ.setdefault("WSM_GROUP_PREFETCH_DEPTH",  "6")  # â­ 6 â†’ 8
    os.environ.setdefault("WSM_GPU_AHEAD",             str(GPU_AHEAD_LAYERS))
    os.environ.setdefault("WSM_GPU_BEHIND",            "2")    # ä¿ç•™æœ€è¿‘ 2 å±‚

    # === é¢„å–ç­–ç•¥ ===
    os.environ.setdefault("WSM_BALANCE_PREFETCH",      "1")
    os.environ.setdefault("WSM_PAIR_AHEAD",            "2")
    os.environ.setdefault("WSM_KIND_AHEAD_CAP",        "2")
    os.environ.setdefault("WSM_EVICT_FINISHED",        "1")    # å¯ç”¨å®Œæˆåé€å‡º
    os.environ.setdefault("WSM_CPU_EVICT_AFTER_USE",   "0")    # å¼‚æ­¥æ¨¡å¼ä¸‹ç¦ç”¨ç«‹å³é€å‡º

    # === è°ƒè¯•ä¸ç›‘æ§ ===
    os.environ.setdefault("WSM_GRP_RETAIN_MS",         "0")
    os.environ.setdefault("WSM_SKIP_PRELOAD_WAIT",     "1")    # å¯ç”¨å¼‚æ­¥é¢„åŠ è½½
    os.environ.setdefault("WSM_DEBUG_PREFETCH",        "1")    # å¯ç”¨è¯¦ç»†æ—¥å¿—
    os.environ.setdefault("WSM_VERBOSE_MISMATCH",      "0")    # ç”Ÿäº§ç¯å¢ƒå…³é—­

    # === CPU é¢„å–ä¼˜åŒ–ï¼ˆRAM å¯å®¹çº³ 60 å±‚ï¼‰ ===
    os.environ.setdefault("WSM_POOLED_CPU_READ",       "1")
    os.environ.setdefault("WSM_CPU_PF_WORKERS",        "12")   # CPU é¢„å–çº¿ç¨‹æ•°ï¼ˆ50% CPUï¼‰
    os.environ.setdefault("WSM_REBALANCE_SYNC",        "0")    # å¼‚æ­¥é‡å¹³è¡¡

    # === SSDâ†’CPU æµæ°´çº¿ ===
    os.environ.setdefault("WSM_CPU_PREFETCH_DISTANCE", str(CPU_CACHE_LAYERS))   # CPU é¢„å– 50 å±‚
    os.environ.setdefault("WSM_SSD_CONCURRENCY",       "12")    # SSD å¹¶å‘è¯»å–

    # === Prefill ç‰¹å®šä¼˜åŒ– ===
    os.environ.setdefault("PREFILL_CPU_LAYERS",        str(CPU_CACHE_LAYERS))   # Prefill CPU ç¼“å­˜ 50 å±‚
    os.environ.setdefault("PREFILL_GPU_LAYERS",        str(GPU_WARMUP_LAYERS))  # â­ 6 â†’ 12 å±‚
    os.environ.setdefault("PREFILL_PREFETCH_DISTANCE", "16")   # â­ 10 â†’ 16ï¼ˆæ›´è¿œçš„é¢„å–è·ç¦»ï¼‰
    os.environ.setdefault("WSM_WARMUP_LAYERS_GPU",     str(GPU_WARMUP_LAYERS))  # â­ 6 â†’ 12 å±‚
    os.environ.setdefault("WSM_WRAPAROUND_WARMUP",     str(GPU_WARMUP_LAYERS))  # â­ 6 â†’ 12 å±‚
    
    
  

    # ============================================================
    # CPU çª—å£é¢å¤–é…ç½®ï¼ˆå¤ç”¨ä¸Šé¢çš„ CPU_CACHE_LAYERSï¼‰
    # ============================================================
    os.environ.setdefault("WSM_CPU_RING_MODE",     "1")
    os.environ.setdefault("WSM_CPU_RING_OFFSET",   "0")
    os.environ.setdefault("WSM_CPU_CACHE_LAYERS",  str(CPU_CACHE_LAYERS))
    os.environ.setdefault("WSM_CPU_CACHE_CAP_LAYERS", str(CPU_CACHE_LAYERS))
    os.environ.setdefault("WSM_CPU_CACHE_HWM_LAYERS", str(CPU_CACHE_LAYERS))
    os.environ.setdefault("WSM_CPU_CACHE_LWM_LAYERS", str(max(2, CPU_CACHE_LAYERS - 5)))
    os.environ.setdefault("WSM_CPU_BACK_MARGIN",   "1")
    os.environ.setdefault("WSM_KV_THROTTLE_THRESHOLD", "2")
    os.environ.setdefault("WSM_KV_THROTTLE_MS",        "16")

    # é…ç½®æ€»ç»“ï¼ˆä»…æ‰“å°ï¼‰
    print("=" * 80)
    print("ğŸš€ å¼‚æ­¥æ»‘åŠ¨çª—å£ - RTX 5080 (16GB) + 125GB RAM ä¼˜åŒ–é…ç½®")
    print("=" * 80)
    print(f"GPU é¢„å–æ·±åº¦:  {GPU_AHEAD_LAYERS} ç»„")
    print(f"GPU ç»„é¢„ç®—:    {GPU_MAX_GROUPS} ç»„ (æœ€å¤š ~9GB)")
    print(f"CPU ç¼“å­˜å®¹é‡:  {CPU_CACHE_LAYERS} å±‚ (~79.5GB)")
    print(f"H2D å¹¶å‘åº¦:    Prefill 24 | Decode 16")
    print(f"å¼‚æ­¥é€å‡ºé˜Ÿåˆ—:  64 ä»»åŠ¡")
    print(f"åå°çº¿ç¨‹æ± :    6 workers")
    print(f"CPU é¢„å–çº¿ç¨‹:  10 workers")
    print("=" * 80)
    print("âœ… å¼‚æ­¥çª—å£ç‰¹æ€§: é€å‡º/é¢„å–/CPUæ¨è¿› å…¨éƒ¨åœ¨åå°çº¿ç¨‹æ‰§è¡Œ")
    print("âœ… ä¸»çº¿ç¨‹çª—å£æ»‘åŠ¨å»¶è¿Ÿ: <1ms (vs åŒæ­¥æ¨¡å¼ ~20ms)")
    print("=" * 80)

    device = "cuda:0" if torch.cuda.is_available() else "cpu"

    # 1) è¦†ç›– pinned/æ³¨å†Œæ±  + KV æ± 
    with PROFILER.span("apply_runtime_overrides", "setup"):
        apply_runtime_overrides()
    with PROFILER.span("configure_kv_pool", "setup"):
        configure_kv_pool()
    with PROFILER.span("probe_after_runtime_clamp", "non_inference"):
        probe("after runtime clamp")

    # 2) WSMï¼ˆSSD æµå¼ï¼‰æ„é€ å‚æ•°
    PRIME_WINDOW = int(os.getenv("WSM_PRIME_WINDOW", "12"))  # ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤6
    mode_config = {
        "raw_device": RAW_DEV,
        "ssd_manifest_path": MANIFEST,
        "max_cached_layers": CPU_CACHE_LAYERS,         # âœ… ä¿®å¤: å¿…é¡»ä¸ CPU_CAP_VALUE ä¸€è‡´
        "cpu_cache_layers": CPU_CACHE_LAYERS,          # CPU ç¯å½¢å®¹é‡
        "warmup_layers": max(PRIME_WINDOW, GPU_AHEAD_LAYERS + 2),  # âœ… Fix: è‡³å°‘é¢„çƒ­ GPU_AHEAD + 2 å±‚
        "staging_mb": 64,
        "verbose": True,
    }

    # 3) æ„å»ºï¼ˆmeta + SSD æµå¼ï¼‰
    with PROFILER.span("probe_before_build", "non_inference"):
        probe("before LLaMA.build")
    with PROFILER.span("LLaMA.build", "setup"):
        llama = LLaMA.build(
            checkpoints_dir=CKPT_DIR,
            load_model=False,           # ä¸æŠŠ checkpoint å…¨è½½å…¥ CPU
            device=device,
            max_seq_len=4096,
            max_batch_size=32,
            topk_blk=8,
            mode="mixed",
            mode_config=mode_config
        )
        s = stream_mnt.get_streams("cuda:0")
        print("h2d_mha:", s.weight_h2d_mha)
        print("h2d_ffn:", s.weight_h2d_ffn)
        print("cmp_mha:", s.compute_mha)
        print("cmp_ffn:", s.compute_ffn)
        print("kv_h2d:", s.kv_h2d, "kv_d2h:", s.kv_d2h)
    with PROFILER.span("probe_after_build", "non_inference"):
        probe("after LLaMA.build")

    # ç»‘å®š WSM è¡¥ä¸ï¼šä»…ä¸º profiler è®¡æ—¶ï¼Œwait_group_ready çš„å¼‚æ­¥é€»è¾‘å·²åœ¨ WSM ä¸»ç±»å®ç°
    wsm = getattr(llama, "weight_streaming_manager", None)
    if wsm is not None:
        # åŒ…è£… wait_group_ready ä»¥æ·»åŠ  profiler è®¡æ—¶
        original_wait = wsm.wait_group_ready
        wsm.wait_group_ready = types.MethodType(_wrap_wait_group_ready(original_wait), wsm)

        # ä¿ç•™ ensure_module_on_gpu çš„ CPU stub loader è¡¥ä¸
        wsm._ensure_module_on_gpu = types.MethodType(_patched_ensure_module_on_gpu, wsm)
        print("[WSM PATCH] Profiler wrapper + CPU stub loader enabled")

        # â­â­â­ P0 ä¼˜åŒ–ï¼šGPUçª—å£é¢„çƒ­ï¼ˆé¿å…å†·å¯åŠ¨ï¼Œå‰Nå±‚å¹¶è¡ŒH2Dï¼‰
        with PROFILER.span("gpu_window_warmup", "setup"):
            warmup_layers = GPU_WARMUP_LAYERS  # â­ ä½¿ç”¨é…ç½®çš„ 12 å±‚
            print(f"[WSM WARMUP] Preloading first {warmup_layers} layers to GPU...")
            for layer_idx in range(min(warmup_layers, wsm.n_layers)):
                try:
                    # å¼‚æ­¥é¢„å–attnå’Œffnç»„ï¼ˆä¸é˜»å¡ï¼Œè®©H2Dåœ¨åå°å¹¶è¡Œï¼‰
                    wsm.prefetch_group_async(layer_idx, "attn", reason="warmup")
                    wsm.prefetch_group_async(layer_idx, "ffn", reason="warmup")
                except Exception as e:
                    print(f"[WSM WARMUP] Layer {layer_idx} prefetch failed: {e}")
            print(f"[WSM WARMUP] Warmup requests sent (async), first {warmup_layers} layers (24 groups) will be ready before inference")

            # â­â­â­ é¢å¤–ä¿®å¤ï¼šç­‰å¾… warmup å®Œæˆåï¼Œç»§ç»­é¢„å–åç»­å±‚å»ºç«‹æµæ°´çº¿
            # åœ¨æ¨ç†å¼€å§‹å‰ï¼Œé¢„å– L12-L20ï¼Œç¡®ä¿ L12+ ä¹Ÿèƒ½ overlap
            # print(f"[WSM WARMUP] Extending prefetch pipeline to L{warmup_layers + 8}...")
            # for layer_idx in range(warmup_layers, min(warmup_layers + 8, wsm.n_layers)):
            #     try:
            #         wsm.prefetch_group_async(layer_idx, "attn", reason="warmup_extend")
            #         # ä¸é¢„å– ffnï¼ŒèŠ‚çœå¹¶å‘æ§½ä½
            #     except Exception as e:
            #         pass
            # print(f"[WSM WARMUP] Extended pipeline ready")

    PROFILER.wrap_model_forward(llama.model)

    # 4) è¯»å– prompt + å®‰å…¨è£å‰ªï¼ˆmax_gen_len=32ï¼‰
    batch_size = 1
    max_gen_len = 32

    with PROFILER.span("read_prompt_file", "prompt"):
        try:
            prompt_path = PROMPT_TXT
            file_content = prompt_path.read_text(encoding="utf-8").strip()

            # è§£æå¤šä¸ªpromptsï¼ˆæŒ‰ "===== PROMPT XXXX =====" åˆ†éš”ï¼‰
            import re
            prompt_blocks = re.split(r'=====\s*PROMPT\s+\d+\s+.*?=====\s*\n', file_content)
            # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
            prompt_blocks = [p.strip() for p in prompt_blocks if p.strip()]

            # å–å‰batch_sizeä¸ªprompts
            prompts = prompt_blocks[:batch_size]
            if len(prompts) < batch_size:
                # å¦‚æœpromptsä¸è¶³ï¼Œé‡å¤æœ€åä¸€ä¸ªpromptæ¥å¡«å……
                print(f"Warning: Only {len(prompts)} prompts found, padding to {batch_size}")
                while len(prompts) < batch_size:
                    prompts.append(prompts[-1])

            print(f"Loaded {len(prompts)} prompts for batch_size={batch_size}")

        except Exception as e:
            raise RuntimeError(f"æ— æ³•è¯»å– {prompt_path}: {e}")

    with PROFILER.span("tokenize_and_clip", "prompt"):
        max_prompt_tokens = llama.args.max_seq_len - max_gen_len

        # å¯¹æ¯ä¸ªpromptè¿›è¡Œtokenizeå’Œè£å‰ª
        clipped_prompts = []
        for prompt in prompts:
            tok = llama.tokenizer.encode(prompt, add_special_tokens=False)
            if len(tok) > max_prompt_tokens:
                tok = tok[-max_prompt_tokens:]
                prompt = llama.tokenizer.decode(tok)
            clipped_prompts.append(prompt)

        prompts = clipped_prompts
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªpromptçš„tokenæ•°ä½œä¸ºç»Ÿè®¡ï¼ˆå‡è®¾æ‰€æœ‰prompté•¿åº¦ç›¸ä¼¼ï¼‰
        tokens_in_count = len(llama.tokenizer.encode(prompts[0], add_special_tokens=False))

    # 5) çœŸæ­£æ¨ç†ï¼ˆdecodeï¼‰
    with PROFILER.span("probe_before_infer", "non_inference"):
        probe("before inference (decode)")
    with PROFILER.inference_scope():  # ç«¯åˆ°ç«¯æ¨ç†æ—¶é—´
        out_tokens, out_texts = llama.text_completion(
            prompts=prompts,
            temperature=0.0,
            max_gen_len=max_gen_len,
            batch_size=batch_size,
        )
    with PROFILER.span("probe_after_infer", "non_inference"):
        probe("after inference (decode)")

    # ==== ç»Ÿè®¡ tokens_out ====
    def _count_output_tokens(out_tokens_obj):
        try:
            if isinstance(out_tokens_obj, (list, tuple)):
                if len(out_tokens_obj) > 0 and isinstance(out_tokens_obj[0], (list, tuple)):
                    return len(out_tokens_obj[0])
                return len(out_tokens_obj)
            if torch.is_tensor(out_tokens_obj):
                return int(out_tokens_obj.numel())
        except Exception:
            pass
        return None

    tokens_out_count = _count_output_tokens(out_tokens)

    # ==== æ±‡æ€»ä¸ä¿å­˜ ====
    mode = classify_mode(llama)
    PROFILER.finalize(
        tokens_in=tokens_in_count,
        tokens_out=tokens_out_count,
        extra_meta={"llama_mode": mode, "device_str": str(device)}
    )

    # è‡ªåŠ¨ç”Ÿæˆ JSON/CSV è·¯å¾„å¹¶å„ä¿å­˜ä¸€æ¬¡
    json_path, csv_path = build_output_paths(LOG_DIR, PROFILER.run_id, mode)
    PROFILER.save(str(json_path))
    PROFILER.save(str(csv_path))
    print(f"[Profiler] JSON: {json_path}")
    print(f"[Profiler] CSV : {csv_path}")

    # ==== è¾“å‡ºç”Ÿæˆæ–‡æœ¬ï¼ˆä¸å½±å“è®¡æ—¶ï¼‰====
    print(f"\n========== Generation (batch_size={batch_size}, len={max_gen_len}) ==========")
    # åªæ˜¾ç¤ºå‰3ä¸ªå’Œæœ€å1ä¸ªï¼Œé¿å…è¾“å‡ºå¤ªé•¿
    for i in [0, 1, 2, batch_size-1]:
        if i < len(out_texts):
            print(f"\n--- Batch {i} ---")
            print(out_texts[i][:200] + "..." if len(out_texts[i]) > 200 else out_texts[i])
    print("=========================================")

if __name__ == "__main__":
    main()
