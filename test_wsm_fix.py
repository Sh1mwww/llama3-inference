#!/usr/bin/env python3
"""
æµ‹è¯• Weight Streaming Manager è®¾å¤‡é—®é¢˜ä¿®å¤
"""

import sys
import os
sys.path.append('/home/roger/llama3_project')

def test_basic_inference():
    """æµ‹è¯•åŸºæœ¬æ¨ç†åŠŸèƒ½"""
    print("ğŸ§ª Testing WSM device fix with basic inference...")
    
    # ä½¿ç”¨ä¿®å¤åçš„ profile_pipeline è¿›è¡Œæµ‹è¯•
    import subprocess
    
    # ç®€å•çš„æµ‹è¯•å‘½ä»¤
    cmd = [
        'python3', '/home/roger/llama3_project/scripts/profile_pipeline.py',
        '--model-path', '/mnt/model/llama/checkpoints/Llama3.2-3B',
        '--prompt', 'Hello world',
        '--max-gen-len', '10',
        '--batch-size', '1',
        '--device', 'cuda',
        '--verbose'
    ]
    
    print(f"Running command: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        
        print("\nğŸ“¤ STDOUT:")
        print(result.stdout)
        
        if result.stderr:
            print("\nğŸ“¥ STDERR:")  
            print(result.stderr)
        
        if result.returncode == 0:
            print("\nâœ… Test completed successfully!")
            return True
        else:
            print(f"\nâŒ Test failed with return code {result.returncode}")
            return False
            
    except subprocess.TimeoutExpired:
        print("\nâ° Test timed out after 120 seconds")
        return False
    except Exception as e:
        print(f"\nğŸ’¥ Test crashed: {e}")
        return False

if __name__ == "__main__":
    success = test_basic_inference()
    sys.exit(0 if success else 1)