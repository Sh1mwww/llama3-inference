#!/usr/bin/env python3
"""
åˆ†æ layer_timings_70B.csv çš„å·¥å…·è„šæœ¬
"""
import pandas as pd
import numpy as np
from pathlib import Path

def analyze_timings(csv_path="layer_timings_70B.csv"):
    """åˆ†æå±‚çº§æ—¶é—´æ•°æ®"""
    if not Path(csv_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return

    # è¯»å– CSV
    df = pd.read_csv(csv_path)
    print(f"\nğŸ“Š åŠ è½½äº† {len(df)} æ¡è®°å½•")
    print(f"åˆ—: {list(df.columns)}")

    # å…¼å®¹æ—§æ ¼å¼ï¼šå¦‚æœæœ‰ elapsed_ms ä½†æ²¡æœ‰ total_msï¼Œé‡å‘½å
    if 'elapsed_ms' in df.columns and 'total_ms' not in df.columns:
        df['total_ms'] = df['elapsed_ms']

    # æ£€æŸ¥æ•°æ®æ ¼å¼ï¼ˆæ–°æ ¼å¼ï¼šlayer_idx=-1 è¡¨ç¤ºæ•´ä¸ª tokenï¼‰
    is_token_level = (df['layer_idx'] == -1).any()
    if is_token_level:
        print("âœ… æ£€æµ‹åˆ° Token çº§åˆ«è®¡æ—¶ï¼ˆæ¯ä¸ª token ä¸€æ¡è®°å½•ï¼‰")
        # åªä¿ç•™ token çº§åˆ«çš„è®°å½•
        df = df[df['layer_idx'] == -1].copy()
    else:
        print("âš ï¸  æ£€æµ‹åˆ°æ—§æ ¼å¼ï¼ˆæ¯å±‚ä¸€æ¡è®°å½•ï¼Œå¯èƒ½æœ‰é‡å¤è®¡æ—¶é—®é¢˜ï¼‰")

    # æ£€æŸ¥æ˜¯å¦æœ‰è¯¦ç»†æ—¶é—´åˆ†è§£
    has_detailed_timing = 'compute_ms' in df.columns and 'weight_load_ms' in df.columns

    # â˜… ä¿®æ­£æ—§æ•°æ®çš„å•ä½é—®é¢˜ï¼šå¦‚æœ compute_ms å¼‚å¸¸å¤§ï¼ˆå¯èƒ½æ˜¯å¾®ç§’ï¼‰ï¼Œè‡ªåŠ¨è½¬æ¢
    if has_detailed_timing and 'compute_ms' in df.columns:
        compute_df = df[df['compute_ms'].notna()]
        if len(compute_df) > 0:
            avg_compute = compute_df['compute_ms'].mean()
            avg_total = df['total_ms'].mean()

            # å¦‚æœ compute_ms å¹³å‡å€¼ > total_msï¼Œè¯´æ˜å•ä½å¯èƒ½æ˜¯ us
            if avg_compute > avg_total * 10:  # æ˜æ˜¾å¼‚å¸¸
                print("âš ï¸  æ£€æµ‹åˆ°æ—§æ•°æ®å•ä½é—®é¢˜ï¼ˆcompute_ms å®é™…æ˜¯ usï¼‰ï¼Œè‡ªåŠ¨è½¬æ¢ä¸º ms")
                df['compute_ms'] = df['compute_ms'] / 1000.0
                df['weight_load_ms'] = df['weight_load_ms'] / 1000.0

    # åŸºç¡€ç»Ÿè®¡
    print("\n" + "=" * 70)
    print("æ•´ä½“ç»Ÿè®¡ (Total Time)")
    print("=" * 70)
    print(f"æ€»æ—¶é—´:       {df['total_ms'].sum():.2f} ms")
    print(f"å¹³å‡æ—¶é—´:     {df['total_ms'].mean():.4f} ms")
    print(f"ä¸­ä½æ•°:       {df['total_ms'].median():.4f} ms")
    print(f"æ ‡å‡†å·®:       {df['total_ms'].std():.4f} ms")
    print(f"æœ€å°å€¼:       {df['total_ms'].min():.4f} ms")
    print(f"æœ€å¤§å€¼:       {df['total_ms'].max():.4f} ms")

    # â˜… æ–°å¢ï¼šæ—¶é—´åˆ†è§£ç»Ÿè®¡
    if has_detailed_timing:
        print("\n" + "=" * 70)
        print("æ—¶é—´åˆ†è§£ç»Ÿè®¡ (Compute vs Weight Load)")
        print("=" * 70)
        compute_df = df[df['compute_ms'].notna()]
        weight_df = df[df['weight_load_ms'].notna()]

        if len(compute_df) > 0:
            print(f"\nè®¡ç®—æ—¶é—´ (Compute):")
            print(f"  å¹³å‡: {compute_df['compute_ms'].mean():.4f} ms")
            print(f"  å æ¯”: {compute_df['compute_ms'].mean() / df['total_ms'].mean() * 100:.1f}%")

        if len(weight_df) > 0:
            print(f"\næƒé‡åŠ è½½æ—¶é—´ (Weight Load):")
            print(f"  å¹³å‡: {weight_df['weight_load_ms'].mean():.4f} ms")
            print(f"  å æ¯”: {weight_df['weight_load_ms'].mean() / df['total_ms'].mean() * 100:.1f}%")

        if len(compute_df) > 0 and len(weight_df) > 0:
            print(f"\nç“¶é¢ˆåˆ†æ:")
            compute_avg = compute_df['compute_ms'].mean()
            weight_avg = weight_df['weight_load_ms'].mean()
            if weight_avg > compute_avg:
                ratio = weight_avg / compute_avg
                print(f"  âš ï¸  æƒé‡åŠ è½½æ˜¯ç“¶é¢ˆ ({ratio:.2f}x æ…¢äºè®¡ç®—)")
                print(f"      å»ºè®®ï¼šä¼˜åŒ– SSD â†’ CPU â†’ GPU æµæ°´çº¿")
            else:
                ratio = compute_avg / weight_avg
                print(f"  âœ… è®¡ç®—æ˜¯ç“¶é¢ˆ ({ratio:.2f}x æ…¢äºæƒé‡åŠ è½½)")
                print(f"      GPU åˆ©ç”¨ç‡è‰¯å¥½")

    # æŒ‰å±‚ç»Ÿè®¡ï¼ˆä»…åœ¨æ—§æ ¼å¼æ—¶æœ‰æ„ä¹‰ï¼‰
    if not is_token_level and 'layer_idx' in df.columns:
        print("\n" + "=" * 70)
        print("æŒ‰å±‚ç»Ÿè®¡ (Top 10 æœ€æ…¢)")
        print("=" * 70)
        layer_stats = df.groupby('layer_idx')['total_ms'].agg(['mean', 'std', 'count'])
        layer_stats = layer_stats.sort_values('mean', ascending=False)
        print(layer_stats.head(10).to_string())

    # æŒ‰ token ä½ç½®ç»Ÿè®¡ï¼ˆprefill vs decodeï¼‰
    print("\n" + "=" * 70)
    print("æŒ‰ token ä½ç½®ç»Ÿè®¡")
    print("=" * 70)
    token_stats = df.groupby('token_idx')['total_ms'].agg(['mean', 'std', 'count'])
    print(f"Prefill é˜¶æ®µ (token 0-10):")
    print(token_stats.head(10).to_string())

    if len(token_stats) > 10:
        print(f"\nDecode é˜¶æ®µ (token 10+):")
        print(token_stats.tail(10).to_string())

    # æ‰¾å‡ºå¼‚å¸¸æ…¢çš„è®°å½•
    print("\n" + "=" * 70)
    print("å¼‚å¸¸æ…¢çš„è®°å½• (> mean + 2*std)")
    print("=" * 70)
    threshold = df['total_ms'].mean() + 2 * df['total_ms'].std()
    outliers = df[df['total_ms'] > threshold]
    if len(outliers) > 0:
        print(f"æ‰¾åˆ° {len(outliers)} æ¡å¼‚å¸¸è®°å½•:")
        cols_to_show = ['batch_idx', 'layer_idx', 'token_idx', 'total_ms']
        if has_detailed_timing:
            cols_to_show.extend(['compute_ms', 'weight_load_ms'])
        print(outliers[cols_to_show].to_string())
    else:
        print("æ²¡æœ‰å¼‚å¸¸æ…¢çš„è®°å½•")

    # Prefill vs Decode å¯¹æ¯”
    print("\n" + "=" * 70)
    print("Prefill vs Decode å¯¹æ¯”")
    print("=" * 70)
    # å‡è®¾ token_idx < 10 æ˜¯ prefillï¼Œ>= 10 æ˜¯ decode
    prefill = df[df['token_idx'] < 10]
    decode = df[df['token_idx'] >= 10]

    if len(prefill) > 0 and len(decode) > 0:
        print(f"Prefill: {prefill['total_ms'].mean():.4f} ms (avg), {len(prefill)} records")
        print(f"Decode:  {decode['total_ms'].mean():.4f} ms (avg), {len(decode)} records")
        print(f"Speedup: {prefill['total_ms'].mean() / decode['total_ms'].mean():.2f}x")

    # ===== Token çº§åˆ«åˆ†æ =====
    if is_token_level:
        print("\n" + "=" * 70)
        print("Token ç”Ÿæˆæ—¶é—´è¶‹åŠ¿")
        print("=" * 70)

        # å‰ 10 ä¸ª token vs å 10 ä¸ª token
        if len(df) >= 20:
            first_10 = df.nsmallest(10, 'token_idx')
            last_10 = df.nlargest(10, 'token_idx')

            print(f"å‰ 10 ä¸ª token å¹³å‡æ—¶é—´: {first_10['total_ms'].mean():.4f} ms")
            print(f"å 10 ä¸ª token å¹³å‡æ—¶é—´: {last_10['total_ms'].mean():.4f} ms")

            if last_10['total_ms'].mean() > first_10['total_ms'].mean():
                print(f"âš ï¸  åæœŸå˜æ…¢: {(last_10['total_ms'].mean() / first_10['total_ms'].mean() - 1) * 100:.1f}%")
            else:
                print(f"âœ… åæœŸåŠ é€Ÿ: {(1 - last_10['total_ms'].mean() / first_10['total_ms'].mean()) * 100:.1f}%")
    else:
        # æ—§æ ¼å¼ï¼šLayer 0 åˆ†æ
        print("\n" + "=" * 70)
        print("Layer 0 æ€§èƒ½åˆ†æï¼ˆå†·å¯åŠ¨æ•ˆåº”ï¼‰")
        print("=" * 70)
        layer0 = df[df['layer_idx'] == 0]
        if len(layer0) > 0:
            print(f"Layer 0 å¹³å‡: {layer0['total_ms'].mean():.4f} ms")
            print(f"æ‰€æœ‰å±‚å¹³å‡:   {df['total_ms'].mean():.4f} ms")
            print(f"Layer 0 æ…¢:   {(layer0['total_ms'].mean() / df['total_ms'].mean() - 1) * 100:.1f}%")

    print("\n" + "=" * 70)
    print("Decode é˜¶æ®µç¨³å®šæ€§ï¼ˆæ’é™¤ prefill å’Œ layer 0ï¼‰")
    print("=" * 70)
    # åªçœ‹ decode é˜¶æ®µï¼ˆtoken >= 2017ï¼‰ä¸”é layer 0
    stable_decode = df[(df['token_idx'] >= 2017) & (df['layer_idx'] > 0)]
    if len(stable_decode) > 0:
        print(f"ç¨³å®š decode è®°å½•æ•°: {len(stable_decode)}")
        print(f"å¹³å‡æ—¶é—´:           {stable_decode['total_ms'].mean():.4f} ms")
        print(f"æ ‡å‡†å·®:             {stable_decode['total_ms'].std():.4f} ms")
        print(f"å˜å¼‚ç³»æ•° (CV):      {stable_decode['total_ms'].std() / stable_decode['total_ms'].mean() * 100:.2f}%")
        print(f"æœ€å°å€¼:             {stable_decode['total_ms'].min():.4f} ms")
        print(f"æœ€å¤§å€¼:             {stable_decode['total_ms'].max():.4f} ms")

    print("\n" + "=" * 70)
    print("ååé‡ä¼°ç®—")
    print("=" * 70)
    if len(decode) > 0:
        if is_token_level:
            # æ–°æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨ token æ—¶é—´
            avg_time_per_token = decode['total_ms'].mean() / 1000  # è½¬æ¢ä¸ºç§’
            tokens_per_sec = 1 / avg_time_per_token if avg_time_per_token > 0 else 0

            print(f"å¹³å‡æ¯ token æ—¶é—´:  {decode['total_ms'].mean():.4f} ms")
            print(f"âœ… çœŸå®ååé‡:       {tokens_per_sec:.4f} tokens/s")
            print(f"                    ({tokens_per_sec * 60:.2f} tokens/min)")

            if len(prefill) > 0:
                print(f"Prefill å»¶è¿Ÿ:       {prefill['total_ms'].mean():.2f} msï¼ˆé¦–ä¸ª tokenï¼‰")
        else:
            # æ—§æ ¼å¼ï¼šè­¦å‘Šå¯èƒ½ä¸å‡†ç¡®
            avg_time_per_layer = decode['total_ms'].mean()
            n_layers = 80
            total_time_per_token = avg_time_per_layer * n_layers / 1000  # è½¬æ¢ä¸ºç§’
            tokens_per_sec = 1 / total_time_per_token

            print(f"âš ï¸  è­¦å‘Šï¼šæ—§æ ¼å¼æ•°æ®ï¼Œå¯èƒ½ä¸å‡†ç¡®")
            print(f"æ¯å±‚å¹³å‡æ—¶é—´:       {avg_time_per_layer:.4f} ms")
            print(f"æ€»å±‚æ•°:             {n_layers}")
            print(f"æ¯ token æ€»æ—¶é—´:    {total_time_per_token:.4f} s")
            print(f"é€å±‚ç´¯åŠ ååé‡:     {tokens_per_sec:.4f} tokens/s")
            print(f"    (æ³¨æ„ï¼šè¿™å¯èƒ½ä¸æ˜¯çœŸå®ååé‡)")
            print(f"å»¶è¿Ÿ (TTFT):        ~{prefill['total_ms'].mean() * n_layers / 1000:.2f} sï¼ˆé¦–ä¸ª tokenï¼‰")

    print("\n" + "=" * 70)
    print("æ€§èƒ½ç“¶é¢ˆè¯Šæ–­")
    print("=" * 70)

    if is_token_level:
        # Token çº§åˆ«ï¼šåˆ†ææ—¶é—´æŠ–åŠ¨
        std_ratio = df['total_ms'].std() / df['total_ms'].mean() * 100
        print(f"æ—¶é—´å˜å¼‚ç³»æ•° (CV): {std_ratio:.2f}%")

        if std_ratio > 20:
            print(f"âš ï¸  æ—¶é—´æŠ–åŠ¨è¾ƒå¤§ (CV > 20%)")
            print(f"    å¯èƒ½åŸå› ï¼šæƒé‡åŠ è½½/KV I/O ç«äº‰")
        else:
            print(f"âœ… æ—¶é—´ç¨³å®š (CV < 20%)")

        # æŸ¥æ‰¾å¼‚å¸¸æ…¢çš„ token
        threshold = df['total_ms'].mean() + 2 * df['total_ms'].std()
        slow_tokens = df[df['total_ms'] > threshold]
        if len(slow_tokens) > 0:
            print(f"\nâš ï¸  å¼‚å¸¸æ…¢çš„ tokenï¼ˆ> mean + 2Ïƒï¼‰: {len(slow_tokens)} ä¸ª")
            print(f"   Token ä½ç½®: {slow_tokens['token_idx'].tolist()[:10]}")
    else:
        # æ—§æ ¼å¼ï¼šæŒ‰å±‚åˆ†æ
        layer_avg = df.groupby('layer_idx')['total_ms'].mean()
        overall_avg = df['total_ms'].mean()
        slow_layers = layer_avg[layer_avg > overall_avg * 1.05]  # æ…¢ 5% ä»¥ä¸Š

        if len(slow_layers) > 0:
            print(f"âš ï¸  æŒç»­æ…¢çš„å±‚ï¼ˆ> å¹³å‡å€¼ 5%ï¼‰ï¼š")
            for lid, avg_time in slow_layers.items():
                print(f"  Layer {lid:2d}: {avg_time:.4f} ms ({(avg_time / overall_avg - 1) * 100:.1f}% æ…¢)")
        else:
            print("âœ… æ‰€æœ‰å±‚æ€§èƒ½å‡è¡¡ï¼Œæ— æ˜æ˜¾ç“¶é¢ˆå±‚")

if __name__ == "__main__":
    import sys
    csv_file = sys.argv[1] if len(sys.argv) > 1 else "layer_timings_70B.csv"
    analyze_timings(csv_file)
