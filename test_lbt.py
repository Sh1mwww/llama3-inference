#!/usr/bin/env python3
"""
æµ‹è¯• Layer Block Table (LBT) åŠŸèƒ½
"""

import json
import sys
import time
from pathlib import Path

import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from llama3.weight_lbt import (
    build_layer_block_table,
    build_all_layer_block_tables,
    load_layer_from_raw_block,
    GroupBlockDescriptor,
    LayerBlockTable,
)
from llama3.weights_io_ssd_dram import DirectIOFile, alloc_pinned_aligned


def test_build_block_table(manifest_path: str):
    """æµ‹è¯•æ„å»ºå—è¡¨"""
    print("=" * 80)
    print("TEST 1: Build Layer Block Table")
    print("=" * 80)

    with open(manifest_path, 'r') as f:
        manifest = json.load(f)

    # æ„å»ºæ‰€æœ‰å±‚çš„å—è¡¨
    print("\nğŸ“Š Building block tables for all layers...")
    tables = build_all_layer_block_tables(manifest)

    print(f"âœ… Built {len(tables)} layer block tables")
    print()

    # ç»Ÿè®¡
    total_current_ios = sum(t.total_ios_current() for t in tables.values())
    total_optimized_ios = sum(t.total_ios_optimized() for t in tables.values())
    reduction = (total_current_ios - total_optimized_ios) / total_current_ios * 100

    print(f"ğŸ“ˆ Overall Statistics:")
    print(f"   Current IOs:    {total_current_ios}")
    print(f"   Optimized IOs:  {total_optimized_ios}")
    print(f"   Reduction:      {reduction:.1f}%")
    print()

    # æ˜¾ç¤ºå‰ 5 å±‚çš„è¯¦ç»†ä¿¡æ¯
    print("ğŸ“‹ Sample Layers (0-4):")
    print(f"{'Layer':<8} {'Mode':<10} {'ATTN IOs':<12} {'FFN IOs':<12} {'Total Reduction':<15}")
    print("-" * 70)

    for layer_id in range(min(5, len(tables))):
        table = tables[layer_id]

        attn_mode = "âœ… Block" if table.attn_block.can_merge() else "âŒ Scatter"
        ffn_mode = "âœ… Block" if table.ffn_block.can_merge() else "âŒ Scatter"

        attn_ios = f"{len(table.attn_block.params)} â†’ 1" if table.attn_block.can_merge() else f"{len(table.attn_block.params)}"
        ffn_ios = f"{len(table.ffn_block.params)} â†’ 1" if table.ffn_block.can_merge() else f"{len(table.ffn_block.params)}"

        mode_str = f"{attn_mode}/{ffn_mode}"
        reduction_str = f"{table.io_reduction() * 100:.1f}%"

        print(f"{layer_id:<8} {mode_str:<10} {attn_ios:<12} {ffn_ios:<12} {reduction_str:<15}")

    print()
    return tables


def test_load_layer_optimized(manifest_path: str, layer_id: int = 5):
    """æµ‹è¯•ä½¿ç”¨ LBT åŠ è½½å±‚"""
    print("=" * 80)
    print(f"TEST 2: Load Layer {layer_id} Using LBT (Optimized)")
    print("=" * 80)

    with open(manifest_path, 'r') as f:
        manifest = json.load(f)

    raw_device = manifest["raw_device"]
    block_size = manifest["block_size"]

    # æ‰“å¼€ raw device
    print(f"\nğŸ“‚ Opening raw device: {raw_device}")
    dio = DirectIOFile(raw_device, mode="r", block_size=block_size)

    # æ„å»ºå—è¡¨
    print(f"ğŸ”§ Building block table for layer {layer_id}...")
    layer_table = build_layer_block_table(manifest, layer_id)

    # æ‰“å°å—è¡¨ä¿¡æ¯
    print(f"\nğŸ“Š Layer {layer_id} Block Table:")
    print(f"   ATTN Group:")
    print(f"      Mode:         {layer_table.attn_block.mode}")
    print(f"      Params:       {len(layer_table.attn_block.params)}")
    print(f"      Total span:   {layer_table.attn_block.total_span / (1024**2):.2f} MB")
    print(f"      Useful bytes: {layer_table.attn_block.useful_bytes / (1024**2):.2f} MB")
    print(f"      Fragmentation: {layer_table.attn_block.fragmentation * 100:.2f}%")
    print()
    print(f"   FFN Group:")
    print(f"      Mode:         {layer_table.ffn_block.mode}")
    print(f"      Params:       {len(layer_table.ffn_block.params)}")
    print(f"      Total span:   {layer_table.ffn_block.total_span / (1024**2):.2f} MB")
    print(f"      Useful bytes: {layer_table.ffn_block.useful_bytes / (1024**2):.2f} MB")
    print(f"      Fragmentation: {layer_table.ffn_block.fragmentation * 100:.2f}%")
    print()

    # åˆ†é… staging bufferï¼ˆéœ€è¦èƒ½å®¹çº³ FFN ç»„ï¼Œå®ƒæ›´å¤§ï¼‰
    max_span = max(layer_table.attn_block.total_span, layer_table.ffn_block.total_span)
    staging_size = ((max_span + block_size - 1) // block_size) * block_size

    print(f"ğŸ’¾ Allocating staging buffer: {staging_size / (1024**2):.2f} MB")
    staging_buffer = alloc_pinned_aligned(staging_size, block_size)

    # åŠ è½½å±‚ï¼ˆä½¿ç”¨ LBT ä¼˜åŒ–ï¼‰
    print(f"\nâš¡ Loading layer {layer_id} with LBT optimization...")
    start = time.time()

    layer_weights = load_layer_from_raw_block(
        layer_table, dio, staging_buffer, block_size
    )

    elapsed = time.time() - start

    print(f"âœ… Loaded {len(layer_weights)} parameters in {elapsed*1000:.2f} ms")
    print()

    # æ˜¾ç¤ºåŠ è½½çš„å‚æ•°
    print("ğŸ“¦ Loaded Parameters:")
    total_bytes = 0
    for name, tensor in sorted(layer_weights.items()):
        param_bytes = tensor.numel() * tensor.element_size()
        total_bytes += param_bytes
        print(f"   {name:<50} {str(tensor.shape):<25} {param_bytes/(1024**2):>8.2f} MB")

    print()
    print(f"   Total: {total_bytes / (1024**2):.2f} MB")
    print()

    # å…³é—­æ–‡ä»¶
    dio.close()

    return layer_weights


def test_load_layer_baseline(manifest_path: str, layer_id: int = 5):
    """æµ‹è¯•ä¼ ç»Ÿé€å‚æ•°åŠ è½½ï¼ˆä½œä¸ºå¯¹æ¯”åŸºå‡†ï¼‰"""
    print("=" * 80)
    print(f"TEST 3: Load Layer {layer_id} Using Baseline (Per-Param)")
    print("=" * 80)

    with open(manifest_path, 'r') as f:
        manifest = json.load(f)

    from llama3.weights_io_ssd_dram import DTYPE_MAP

    raw_device = manifest["raw_device"]
    block_size = manifest["block_size"]

    # æ‰“å¼€ raw device
    print(f"\nğŸ“‚ Opening raw device: {raw_device}")
    dio = DirectIOFile(raw_device, mode="r", block_size=block_size)

    # æå–è¯¥å±‚çš„å‚æ•°
    layer_params = [p for p in manifest["params"]
                   if p.get("layer") == layer_id and p.get("policy") == "stream"]

    print(f"ğŸ“‹ Found {len(layer_params)} stream parameters for layer {layer_id}")
    print()

    # åˆ†é… staging buffer
    max_stride = max(p["stride"] for p in layer_params)
    staging_size = ((max_stride + block_size - 1) // block_size) * block_size

    print(f"ğŸ’¾ Allocating staging buffer: {staging_size / (1024**2):.2f} MB")
    staging_buffer = alloc_pinned_aligned(staging_size, block_size)

    # é€å‚æ•°åŠ è½½
    print(f"\nğŸŒ Loading layer {layer_id} with baseline (per-param)...")
    start = time.time()

    layer_weights = {}
    for param_info in layer_params:
        offset = param_info["offset"]
        stride = param_info["stride"]
        nbytes = param_info["nbytes"]
        param_name = param_info["name"]

        # å•ä¸ªå‚æ•°è¯»å–
        dio.pread_into_tensor(staging_buffer, stride, offset)

        # åˆ›å»ºç›®æ ‡ tensor
        param_tensor = torch.empty(
            param_info["shape"],
            dtype=DTYPE_MAP[param_info["dtype"]],
            pin_memory=True
        )

        # æ‹·è´æœ‰æ•ˆå­—èŠ‚
        param_tensor.view(-1).view(torch.uint8)[:nbytes].copy_(
            staging_buffer[:nbytes]
        )

        layer_weights[param_name] = param_tensor

    elapsed = time.time() - start

    print(f"âœ… Loaded {len(layer_weights)} parameters in {elapsed*1000:.2f} ms")
    print()

    # å…³é—­æ–‡ä»¶
    dio.close()

    return layer_weights


def compare_performance(manifest_path: str, layer_id: int = 5, iterations: int = 3):
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("=" * 80)
    print(f"TEST 4: Performance Comparison (Layer {layer_id}, {iterations} iterations)")
    print("=" * 80)

    with open(manifest_path, 'r') as f:
        manifest = json.load(f)

    from llama3.weights_io_ssd_dram import DTYPE_MAP

    raw_device = manifest["raw_device"]
    block_size = manifest["block_size"]

    # æ„å»ºå—è¡¨
    layer_table = build_layer_block_table(manifest, layer_id)

    # æå–å‚æ•°ï¼ˆç”¨äº baselineï¼‰
    layer_params = [p for p in manifest["params"]
                   if p.get("layer") == layer_id and p.get("policy") == "stream"]

    # åˆ†é… staging buffers
    max_span = max(layer_table.attn_block.total_span, layer_table.ffn_block.total_span)
    staging_size = ((max_span + block_size - 1) // block_size) * block_size
    staging_buffer = alloc_pinned_aligned(staging_size, block_size)

    # Baseline æµ‹è¯•
    print(f"\nğŸŒ Baseline (per-param) - {iterations} iterations:")
    baseline_times = []

    for i in range(iterations):
        dio = DirectIOFile(raw_device, mode="r", block_size=block_size)

        start = time.time()
        layer_weights = {}
        for param_info in layer_params:
            dio.pread_into_tensor(staging_buffer, param_info["stride"], param_info["offset"])
            param_tensor = torch.empty(param_info["shape"], dtype=DTYPE_MAP[param_info["dtype"]], pin_memory=True)
            param_tensor.view(-1).view(torch.uint8)[:param_info["nbytes"]].copy_(staging_buffer[:param_info["nbytes"]])
            layer_weights[param_info["name"]] = param_tensor

        elapsed = time.time() - start
        baseline_times.append(elapsed * 1000)
        dio.close()

        print(f"   Iteration {i+1}: {elapsed*1000:.2f} ms")

    baseline_avg = sum(baseline_times) / len(baseline_times)
    print(f"   Average: {baseline_avg:.2f} ms")

    # LBT ä¼˜åŒ–æµ‹è¯•
    print(f"\nâš¡ LBT Optimized (block mode) - {iterations} iterations:")
    lbt_times = []

    for i in range(iterations):
        dio = DirectIOFile(raw_device, mode="r", block_size=block_size)

        start = time.time()
        layer_weights = load_layer_from_raw_block(layer_table, dio, staging_buffer, block_size)
        elapsed = time.time() - start
        lbt_times.append(elapsed * 1000)
        dio.close()

        print(f"   Iteration {i+1}: {elapsed*1000:.2f} ms")

    lbt_avg = sum(lbt_times) / len(lbt_times)
    print(f"   Average: {lbt_avg:.2f} ms")

    # æ€§èƒ½æå‡
    speedup = (baseline_avg - lbt_avg) / baseline_avg * 100
    print()
    print(f"ğŸ“Š Performance Improvement:")
    print(f"   Baseline:    {baseline_avg:.2f} ms")
    print(f"   LBT:         {lbt_avg:.2f} ms")
    print(f"   Improvement: {speedup:.1f}% faster")
    print()


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Test Layer Block Table functionality")
    parser.add_argument("--manifest", default="/data1/70b-fixed.runtime_manifest.json",
                       help="Path to runtime manifest")
    parser.add_argument("--layer", type=int, default=5,
                       help="Layer to test (default: 5)")
    parser.add_argument("--iterations", type=int, default=3,
                       help="Number of iterations for performance test")

    args = parser.parse_args()

    try:
        # Test 1: Build block tables
        tables = test_build_block_table(args.manifest)

        # Test 2: Load layer with LBT
        lbt_weights = test_load_layer_optimized(args.manifest, args.layer)

        # Test 3: Load layer with baseline
        baseline_weights = test_load_layer_baseline(args.manifest, args.layer)

        # Verify they loaded the same parameters
        print("=" * 80)
        print("VERIFICATION")
        print("=" * 80)
        print()

        if set(lbt_weights.keys()) == set(baseline_weights.keys()):
            print("âœ… Both methods loaded the same parameters")
        else:
            print("âŒ Parameter mismatch!")
            print(f"   LBT:      {set(lbt_weights.keys())}")
            print(f"   Baseline: {set(baseline_weights.keys())}")

        print()

        # Test 4: Performance comparison
        compare_performance(args.manifest, args.layer, args.iterations)

    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())
