#!/usr/bin/env python3
"""
7B æ¨¡å‹å®Œæ•´æµ‹è¯•è„šæœ¬
æµ‹è¯•æ‰€æœ‰æ”¹è¿›æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š
1. äº‹ä»¶åŒ–ç­‰å¾…ï¼ˆstream_mntï¼‰
2. DRAM é…é¢ä¼°ç®—ä¼˜åŒ–ï¼ˆkv_offloadï¼‰
3. å†™é˜Ÿåˆ—é˜»å¡å¼å¤„ç†
"""
import torch
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from llama3.generator import LLaMA
from llama3.config import KVCacheArgs

def test_7b_model():
    """
    è¿è¡Œ 7B æ¨¡å‹çš„åŸºæœ¬æ¨ç†æµ‹è¯•
    """
    print("="*80)
    print("7B æ¨¡å‹æµ‹è¯• - éªŒè¯æ‰€æœ‰æ”¹è¿›")
    print("="*80)

    # é…ç½®å‚æ•°
    checkpoint_dir = "/path/to/llama3-7b"  # è¯·æ›¿æ¢ä¸ºå®é™…è·¯å¾„
    device = "cuda:0" if torch.cuda.is_available() else "cpu"

    if not torch.cuda.is_available():
        print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")

    # æ›´æ–° KVCacheArgs é…ç½®
    print("\nğŸ“‹ é…ç½® KV Cache å‚æ•°...")
    KVCacheArgs.dram_limit_gb = 16.0  # DRAM é™åˆ¶
    KVCacheArgs.dram_sizing_batch = 8  # ä½¿ç”¨æ”¹è¿›çš„é…é¢ä¼°ç®—
    KVCacheArgs.ssd_device_path = "/dev/nvme0n1p4"  # SSD è®¾å¤‡è·¯å¾„
    KVCacheArgs.max_concurrent_io = 4

    print(f"  - DRAM limit: {KVCacheArgs.dram_limit_gb} GB")
    print(f"  - DRAM sizing batch: {KVCacheArgs.dram_sizing_batch}")

    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    ckpt_path = Path(checkpoint_dir)
    if not ckpt_path.exists():
        print(f"\nâŒ é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {checkpoint_dir}")
        print(f"\nè¯·è®¾ç½®æ­£ç¡®çš„æ¨¡å‹è·¯å¾„ã€‚ä¾‹å¦‚ï¼š")
        print(f"  checkpoint_dir = '/home/roger/models/Meta-Llama-3-8B'")
        return False

    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    params_file = ckpt_path / "params.json"
    if not params_file.exists():
        print(f"\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° params.json æ–‡ä»¶")
        return False

    pth_files = list(ckpt_path.glob("*.pth"))
    if not pth_files:
        print(f"\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° .pth æƒé‡æ–‡ä»¶")
        return False

    print(f"\nâœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    print(f"  - params.json: {params_file}")
    print(f"  - æƒé‡æ–‡ä»¶: {pth_files[0]}")

    # æ„å»ºæ¨¡å‹
    print(f"\nğŸ”¨ æ„å»ºæ¨¡å‹...")
    try:
        llama = LLaMA.build(
            checkpoints_dir=str(ckpt_path),
            load_model=True,
            device=device,
            max_seq_len=512,  # è¾ƒçŸ­åºåˆ—ç”¨äºå¿«é€Ÿæµ‹è¯•
            max_batch_size=4,
            topk_blk=8,
        )
        print(f"âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    # å‡†å¤‡æµ‹è¯•æç¤ºè¯
    prompts = [
        "Once upon a time",
        "The quick brown fox",
        "In the beginning",
    ]

    print(f"\nğŸ“ æµ‹è¯•æç¤ºè¯:")
    for i, p in enumerate(prompts, 1):
        print(f"  {i}. '{p}'")

    # è¿è¡Œæ¨ç†
    print(f"\nğŸš€ å¼€å§‹æ¨ç†...")
    try:
        tokens, texts = llama.text_completion(
            prompts=prompts,
            temperature=0.6,
            top_p=0.9,
            max_gen_len=50,  # ç”Ÿæˆ 50 ä¸ª token
            batch_size=2,
            enable_batching=True,
        )

        print(f"\nâœ… æ¨ç†å®Œæˆï¼")
        print(f"\nğŸ“„ ç”Ÿæˆç»“æœ:")
        print("="*80)
        for i, (prompt, text) in enumerate(zip(prompts, texts), 1):
            print(f"\n[{i}] æç¤º: {prompt}")
            print(f"    ç”Ÿæˆ: {text}")
        print("="*80)

        return True

    except torch.cuda.OutOfMemoryError as e:
        print(f"\nâŒ GPU å†…å­˜ä¸è¶³: {e}")
        print(f"\nğŸ’¡ å»ºè®®ï¼š")
        print(f"  1. å‡å°‘ max_seq_len (å½“å‰: 512)")
        print(f"  2. å‡å°‘ batch_size (å½“å‰: 2)")
        print(f"  3. ä½¿ç”¨æƒé‡æµå¼ä¼ è¾“æ¨¡å¼")
        return False
    except Exception as e:
        print(f"\nâŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_streaming_mode():
    """
    æµ‹è¯•æƒé‡æµå¼ä¼ è¾“æ¨¡å¼ï¼ˆé€‚ç”¨äº GPU å†…å­˜ä¸è¶³çš„æƒ…å†µï¼‰
    """
    print("\n"+"="*80)
    print("7B æ¨¡å‹æµ‹è¯• - æƒé‡æµå¼ä¼ è¾“æ¨¡å¼")
    print("="*80)

    checkpoint_dir = "/path/to/llama3-7b"  # è¯·æ›¿æ¢ä¸ºå®é™…è·¯å¾„
    device = "cuda:0"

    if not torch.cuda.is_available():
        print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡æµå¼ä¼ è¾“æµ‹è¯•")
        return False

    ckpt_path = Path(checkpoint_dir)
    if not ckpt_path.exists():
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡æµå¼ä¼ è¾“æµ‹è¯•")
        return False

    # æ›´æ–°é…ç½®
    KVCacheArgs.dram_limit_gb = 16.0
    KVCacheArgs.dram_sizing_batch = 4  # æµå¼æ¨¡å¼ä½¿ç”¨æ›´å°çš„ batch

    print(f"\nğŸ”¨ æ„å»ºæ¨¡å‹ï¼ˆæµå¼æ¨¡å¼ï¼‰...")
    try:
        llama = LLaMA.build(
            checkpoints_dir=str(ckpt_path),
            load_model=True,
            device=device,
            mode="stream",  # å¯ç”¨æµå¼ä¼ è¾“
            mode_config={
                'prefetch_distance': 2,
                'max_cached_layers': 4,
                'warmup_layers': 2,
                'verbose': True,
            },
            max_seq_len=256,
            max_batch_size=2,
            topk_blk=4,
        )
        print(f"âœ… æµå¼æ¨¡å¼æ¨¡å‹æ„å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æµå¼æ¨¡å¼æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    # ç®€å•æ¨ç†æµ‹è¯•
    prompts = ["Hello, how are you?"]

    print(f"\nğŸš€ å¼€å§‹æµå¼æ¨ç†...")
    try:
        tokens, texts = llama.text_completion(
            prompts=prompts,
            temperature=0.6,
            max_gen_len=30,
            batch_size=1,
        )

        print(f"\nâœ… æµå¼æ¨ç†å®Œæˆï¼")
        print(f"\nç”Ÿæˆ: {texts[0]}")
        return True

    except Exception as e:
        print(f"\nâŒ æµå¼æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print(f"\nğŸ”¬ å¼€å§‹ 7B æ¨¡å‹æµ‹è¯•")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPU è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

    # æµ‹è¯• 1: åŸºæœ¬æ¨¡å¼
    print(f"\n{'='*80}")
    print(f"æµ‹è¯• 1: åŸºæœ¬æ¨ç†æ¨¡å¼")
    print(f"{'='*80}")
    success_basic = test_7b_model()

    # æµ‹è¯• 2: æµå¼æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
    if torch.cuda.is_available():
        print(f"\n{'='*80}")
        print(f"æµ‹è¯• 2: æƒé‡æµå¼ä¼ è¾“æ¨¡å¼ï¼ˆå¯é€‰ï¼‰")
        print(f"{'='*80}")
        success_streaming = test_streaming_mode()
    else:
        success_streaming = None

    # æ€»ç»“
    print(f"\n{'='*80}")
    print(f"æµ‹è¯•æ€»ç»“")
    print(f"{'='*80}")
    print(f"åŸºæœ¬æ¨¡å¼: {'âœ… é€šè¿‡' if success_basic else 'âŒ å¤±è´¥'}")
    if success_streaming is not None:
        print(f"æµå¼æ¨¡å¼: {'âœ… é€šè¿‡' if success_streaming else 'âŒ å¤±è´¥'}")

    if success_basic:
        print(f"\nğŸ‰ æ­å–œï¼æ‰€æœ‰æ”¹è¿›å·²æˆåŠŸåº”ç”¨å¹¶é€šè¿‡æµ‹è¯•")
        print(f"\næ”¹è¿›éªŒè¯:")
        print(f"  âœ… äº‹ä»¶åŒ–ç­‰å¾…ï¼ˆstream_mnt.pyï¼‰")
        print(f"  âœ… DRAM é…é¢ä¼˜åŒ–ï¼ˆkv_offload.pyï¼‰")
        print(f"  âœ… å†™é˜Ÿåˆ—é˜»å¡å¼å¤„ç†")
    else:
        print(f"\nâš ï¸  æµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        print(f"\nå¸¸è§é—®é¢˜:")
        print(f"  1. æ¨¡å‹è·¯å¾„è®¾ç½®: ä¿®æ”¹ checkpoint_dir å˜é‡")
        print(f"  2. GPU å†…å­˜ä¸è¶³: å°è¯•æµå¼æ¨¡å¼æˆ–å‡å° batch_size")
        print(f"  3. SSD è·¯å¾„é”™è¯¯: æ£€æŸ¥ KVCacheArgs.ssd_device_path")
