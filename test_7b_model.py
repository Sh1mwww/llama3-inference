#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
7B/8B æ¨¡å‹å®Œæ•´æµ‹è¯•è„šæœ¬ï¼ˆä¿®æ­£ç‰ˆï¼‰
ä¿®å¤è¦ç‚¹ï¼š
1) è‡ªåŠ¨é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼š16GB çº§æ˜¾å­˜é»˜è®¤ä½¿ç”¨â€œæƒé‡æµå¼â€
2) æ„å»ºé˜¶æ®µè‹¥ CUDA OOMï¼Œå¼ºåˆ¶æŠŠâ€œåŠä¸Šå¡â€çš„æ¨¡å‹å›è¿ CPU å¹¶é‡å»º
3) æ„å»ºå®Œæˆåï¼Œç»Ÿä¸€æŠŠå°æ¨¡å—ï¼ˆembedding/norm/output ç­‰ï¼‰è¿åˆ°ç›®æ ‡è®¾å¤‡ï¼Œé˜²æ­¢è®¾å¤‡ä¸ä¸€è‡´
4) å¯é€‰ä¿é™©é˜€ï¼šåœ¨ forward å…¥å£æŠŠ tokens è¿åˆ° embed_tokens çš„è®¾å¤‡ï¼ˆçŒ´è¡¥ä¸ï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
    python test_7b_model.py
"""
import sys
from pathlib import Path
import torch

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ sys.pathï¼ˆæ ¹æ®ä½ çš„å·¥ç¨‹ç»“æ„è°ƒèŠ‚ï¼‰
sys.path.insert(0, str(Path(__file__).parent))

from llama3.generator import LLaMA
from llama3.config import KVCacheArgs


# ============== å·¥å…·å‡½æ•° ==============

def _force_model_small_modules_to(model, target_device: torch.device):
    """
    æŠŠå°æ¨¡å—ï¼ˆembedding/norm/output/é¢‘ç‡è¡¨/å„å±‚normï¼‰ç»Ÿä¸€è¿åˆ°ç›®æ ‡è®¾å¤‡ï¼Œé˜²æ­¢â€œåŠä¸Šå¡â€æ®‹ç•™ã€‚
    """
    
    def _move_safely(mod):
        # åªæ£€æµ‹â€œå½“å‰æ¨¡å—è‡ªèº«â€æ˜¯å¦å«æœ‰ meta å‚æ•°/ç¼“å†²
        has_meta = False
        for p in mod.parameters(recurse=False):
            if getattr(p, "is_meta", False):
                has_meta = True; break
        if not has_meta:
            for b in mod.buffers(recurse=False):
                if getattr(b, "is_meta", False):
                    has_meta = True; break
        return mod.to_empty(device=target_device) if has_meta else mod.to(target_device)
    

    if hasattr(model, "embed_tokens"):
        # model.embed_tokens = model.embed_tokens.to(target_device)
        model.embed_tokens = _move_safely(model.embed_tokens)
    if hasattr(model, "norm"):
        # model.norm = model.norm.to(target_device)
        model.norm = _move_safely(model.norm)
    if hasattr(model, "output"):
        # model.output = model.output.to(target_device)
        model.output = _move_safely(model.output)
    if hasattr(model, "freqs_complex") and hasattr(model.freqs_complex, "device"):
        try:
            model.freqs_complex = model.freqs_complex.to(target_device)
        except Exception:
            pass

    if hasattr(model, "layers"):
        for lyr in model.layers:
            for name in ("attn_norm", "ffn_norm"):
                if hasattr(lyr, name):
                    # setattr(lyr, name, getattr(lyr, name).to(target_device))
                    setattr(lyr, name, _move_safely(getattr(lyr, name)))


def _patch_safe_forward(llama_model):
    """
    å¯é€‰â€œä¿é™©é˜€â€ï¼šç¡®ä¿ tokens å’Œ embed_tokens.weight åœ¨åŒä¸€è®¾å¤‡ã€‚
    ä¸æ”¹åº“æºç æ—¶ï¼Œå¯ç”¨çŒ´è¡¥ä¸æ–¹å¼æ›¿æ¢ forwardã€‚
    """
    if not hasattr(llama_model, "forward"):
        return

    orig_forward = llama_model.forward

    def _safe_forward(tokens, start_pos: int):
        dev = llama_model.embed_tokens.weight.device
        if tokens.device != dev:
            tokens = tokens.to(dev, non_blocking=True)
        return orig_forward(tokens, start_pos)

    llama_model.forward = _safe_forward


# ============== 7B/8B åŸºæœ¬æ¨¡å¼ ==============

def test_7b_model():
    """
    è¿è¡Œ 7B/8B æ¨¡å‹çš„åŸºæœ¬æ¨ç†æµ‹è¯•ï¼ˆè‡ªåŠ¨æ¨¡å¼é€‰æ‹© + OOM å›é€€ï¼‰
    """
    print("=" * 80)
    print("7B æ¨¡å‹æµ‹è¯• - éªŒè¯æ‰€æœ‰æ”¹è¿›")
    print("=" * 80)

    # ---- æ¨¡å‹ä¸è®¾å¤‡é…ç½® ----
    # æ”¹æˆä½ çš„ 7B/8B è·¯å¾„ï¼ˆç¤ºä¾‹ä¸º 8Bï¼‰
    checkpoint_dir = "/home/roger/.llama/checkpoints/Llama3.1-8B/"
    ckpt_path = Path(checkpoint_dir)

    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    if not torch.cuda.is_available():
        print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPUï¼ˆä»…éªŒè¯æµç¨‹ï¼Œé€Ÿåº¦è¾ƒæ…¢ï¼‰")

    # ---- KV Cache å‚æ•°ï¼ˆæ ¹æ®ä½ çš„ç¯å¢ƒå¯è°ƒæ•´ï¼‰----
    print("\nğŸ“‹ é…ç½® KV Cache å‚æ•°...")
    KVCacheArgs.dram_limit_gb = 16.0             # DRAM é™åˆ¶
    KVCacheArgs.dram_sizing_batch = 8            # DRAM é…é¢ä¼°ç®—æ‰¹é‡ï¼ˆæ›´è´´è¿‘å®é™…ï¼‰
    KVCacheArgs.ssd_device_path = "/dev/nvme0n1p4"  # å¯é€‰ï¼šè‹¥æ—  SSD è·¯å¾„å¯ç½®ç©º ""
    KVCacheArgs.max_concurrent_io = 4
    print(f"  - DRAM limit: {KVCacheArgs.dram_limit_gb} GB")
    print(f"  - DRAM sizing batch: {KVCacheArgs.dram_sizing_batch}")

    # ---- åŸºç¡€æ£€æŸ¥ ----
    if not ckpt_path.exists():
        print(f"\nâŒ é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {checkpoint_dir}")
        print("è¯·è®¾ç½®æ­£ç¡®çš„æ¨¡å‹è·¯å¾„ï¼Œä¾‹å¦‚ï¼š/home/roger/models/Meta-Llama-3-8B")
        return False

    params_file = ckpt_path / "params.json"
    if not params_file.exists():
        print("\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° params.json æ–‡ä»¶")
        return False

    pth_files = list(ckpt_path.glob("*.pth"))
    if not pth_files:
        print("\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° .pth æƒé‡æ–‡ä»¶")
        return False

    print("\nâœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    print(f"  - params.json: {params_file}")
    print(f"  - æƒé‡æ–‡ä»¶: {pth_files[0]}")

    # ---- æ ¹æ®æ˜¾å­˜è‡ªåŠ¨é€‰æ‹©æ¨¡å¼ ----
    use_stream_by_default = False
    if torch.cuda.is_available():
        total_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        # 16~20GB çº§æ˜¾å­˜é»˜è®¤æµå¼ï¼›>=24GB å¯å°è¯• full
        use_stream_by_default = total_gb < 20

    build_kwargs = dict(
        checkpoints_dir=str(ckpt_path),
        load_model=True,
        device=device,
        max_seq_len=512,      # å¿«é€Ÿå†’çƒŸ
        max_batch_size=4,
        topk_blk=8,
    )
    if use_stream_by_default:
        build_kwargs.update({
            "mode": "stream",
            "mode_config": {
                "prefetch_distance": 1,
                "max_cached_layers": 3,
                "warmup_layers": 1,
                "verbose": True,
            }
        })

    # ---- æ„å»ºæ¨¡å‹ï¼ˆå« OOM å›é€€ï¼‰----
    print("\nğŸ”¨ æ„å»ºæ¨¡å‹...")
    recovered_to_cpu = False
    try:
        llama = LLaMA.build(**build_kwargs)
        print("âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")
    except torch.cuda.OutOfMemoryError as e:
        print("âŒ CUDA OOM when building; falling back to CPU:", e)
        torch.cuda.empty_cache()
        # å¼ºåˆ¶æŠŠï¼ˆå¯èƒ½åŠä¸Šå¡çš„ï¼‰æ¨¡å‹å›åˆ° CPUï¼šé‡æ–° build åˆ° CPU
        cpu_build_kwargs = {**build_kwargs, "device": "cpu"}
        cpu_build_kwargs.pop("mode", None)         # CPU ä¸‹å¯å¿½ç•¥æµå¼æ¨¡å¼
        cpu_build_kwargs.pop("mode_config", None)
        llama = LLaMA.build(**cpu_build_kwargs)
        recovered_to_cpu = True
        print("âœ… å·²å›é€€åˆ° CPU æ„å»ºï¼ˆå¯é…åˆæµå¼ç»§ç»­è·‘ï¼‰")
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    # ---- æ„å»ºåè®¾å¤‡ä¸€è‡´æ€§ï¼ˆåŒå‘ï¼‰----
    # ç›®æ ‡è®¾å¤‡ï¼šä»¥ LLaMA.args.device ä¸ºå‡†ï¼›è‹¥ OOM å›é€€ï¼Œåˆ™æ˜¯ "cpu"
    target_device = torch.device(getattr(llama.args, "device", "cpu"))
    _force_model_small_modules_to(llama.model, target_device)

    # å¯é€‰ï¼šä¸º forward æ‰“ä¿é™©é˜€ï¼Œé˜²æ­¢ä¸Šå±‚è¯¯ä¼  device ä¸ä¸€è‡´çš„ tokens
    SAFE_FORWARD_PATCH = True
    if SAFE_FORWARD_PATCH:
        _patch_safe_forward(llama.model)

    # ---- æç¤ºè¯ä¸æ¨ç†å‚æ•° ----
    prompts = [
        "Once upon a time",
        "The quick brown fox",
        "In the beginning",
    ]
    print("\nğŸ“ æµ‹è¯•æç¤ºè¯:")
    for i, p in enumerate(prompts, 1):
        print(f"  {i}. '{p}'")

    print("\nğŸš€ å¼€å§‹æ¨ç†...")
    try:
        tokens, texts = llama.text_completion(
            prompts=prompts,
            temperature=0.6,
            top_p=0.9,
            max_gen_len=50,
            batch_size=2,
            enable_batching=True,
        )

        print("\nâœ… æ¨ç†å®Œæˆï¼")
        print("\nğŸ“„ ç”Ÿæˆç»“æœ:")
        print("=" * 80)
        for i, (prompt, text) in enumerate(zip(prompts, texts), 1):
            print(f"\n[{i}] æç¤º: {prompt}")
            print(f"    ç”Ÿæˆ: {text}")
        print("=" * 80)
        return True

    except torch.cuda.OutOfMemoryError as e:
        print("\nâŒ GPU å†…å­˜ä¸è¶³:", e)
        print("\nğŸ’¡ å»ºè®®ï¼š")
        print("  1) å‡å°‘ max_seq_lenï¼ˆå½“å‰ 512ï¼‰")
        print("  2) å‡å°‘ batch_sizeï¼ˆå½“å‰ 2ï¼‰")
        print("  3) ä½¿ç”¨æƒé‡æµå¼æ¨¡å¼ï¼ˆæœ¬è„šæœ¬åœ¨ 16GB æ˜¾å­˜å·²é»˜è®¤å¯ç”¨ï¼‰")
        return False
    except Exception as e:
        print("\nâŒ æ¨ç†å¤±è´¥:", e)
        import traceback
        traceback.print_exc()
        return False


# ============== æµå¼æ¨¡å¼ä¸“æµ‹ï¼ˆå¯é€‰ï¼‰ ==============

def test_streaming_mode():
    """
    æµ‹è¯•æƒé‡æµå¼ä¼ è¾“æ¨¡å¼ï¼ˆé€‚ç”¨äº GPU æ˜¾å­˜ç´§å¼ çš„æƒ…å†µï¼‰
    """
    print("\n" + "=" * 80)
    print("7B æ¨¡å‹æµ‹è¯• - æƒé‡æµå¼ä¼ è¾“æ¨¡å¼")
    print("=" * 80)

    # æ”¹æˆä½ çš„ 7B/8B è·¯å¾„
    checkpoint_dir = "/home/roger/.llama/checkpoints/Llama3.1-8B/"
    device = "cuda:0"

    if not torch.cuda.is_available():
        print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡æµå¼ä¼ è¾“æµ‹è¯•")
        return False

    ckpt_path = Path(checkpoint_dir)
    if not ckpt_path.exists():
        print("âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡æµå¼ä¼ è¾“æµ‹è¯•")
        return False

    # KV é…ç½®
    KVCacheArgs.dram_limit_gb = 16.0
    KVCacheArgs.dram_sizing_batch = 4

    print("\nğŸ”¨ æ„å»ºæ¨¡å‹ï¼ˆæµå¼æ¨¡å¼ï¼‰...")
    try:
        llama = LLaMA.build(
            checkpoints_dir=str(ckpt_path),
            load_model=True,
            device=device,
            mode="stream",
            mode_config={
                "prefetch_distance": 2,
                "max_cached_layers": 4,
                "warmup_layers": 2,
                "verbose": True,
            },
            max_seq_len=256,
            max_batch_size=2,
            topk_blk=4,
        )
        # æ„å»ºåè®¾å¤‡ä¸€è‡´æ€§ï¼ˆåŒå‘ï¼‰
        _force_model_small_modules_to(llama.model, torch.device(llama.args.device))
        _patch_safe_forward(llama.model)

        print("âœ… æµå¼æ¨¡å¼æ¨¡å‹æ„å»ºæˆåŠŸ")
    except Exception as e:
        print("âŒ æµå¼æ¨¡å¼æ„å»ºå¤±è´¥:", e)
        import traceback
        traceback.print_exc()
        return False

    prompts = ["Hello, how are you?"]
    print("\nğŸš€ å¼€å§‹æµå¼æ¨ç†...")
    try:
        tokens, texts = llama.text_completion(
            prompts=prompts,
            temperature=0.6,
            max_gen_len=30,
            batch_size=1,
        )
        print("\nâœ… æµå¼æ¨ç†å®Œæˆï¼")
        print("\nç”Ÿæˆ:", texts[0])
        return True

    except Exception as e:
        print("\nâŒ æµå¼æ¨ç†å¤±è´¥:", e)
        import traceback
        traceback.print_exc()
        return False


# ============== ä¸»å…¥å£ ==============

if __name__ == "__main__":
    print("\nğŸ”¬ å¼€å§‹ 7B æ¨¡å‹æµ‹è¯•")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPU è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

    # æµ‹è¯• 1: åŸºæœ¬æ¨¡å¼ï¼ˆå«è‡ªåŠ¨é€‰æ‹© + OOM å›é€€ï¼‰
    print(f"\n{'='*80}\næµ‹è¯• 1: åŸºæœ¬æ¨ç†æ¨¡å¼\n{'='*80}")
    success_basic = test_7b_model()

    # æµå¼æ¨¡å¼ä¸“æµ‹ï¼ˆå¯é€‰ï¼‰
    success_streaming = None
    if torch.cuda.is_available():
        print(f"\n{'='*80}\næµ‹è¯• 2: æƒé‡æµå¼ä¼ è¾“æ¨¡å¼ï¼ˆå¯é€‰ï¼‰\n{'='*80}")
        success_streaming = test_streaming_mode()

    # æ€»ç»“
    print(f"\n{'='*80}\næµ‹è¯•æ€»ç»“\n{'='*80}")
    print(f"åŸºæœ¬æ¨¡å¼: {'âœ… é€šè¿‡' if success_basic else 'âŒ å¤±è´¥'}")
    if success_streaming is not None:
        print(f"æµå¼æ¨¡å¼: {'âœ… é€šè¿‡' if success_streaming else 'âŒ å¤±è´¥'}")

    if success_basic:
        print("\nğŸ‰ æ­å–œï¼å…³é”®æ”¹è¿›å·²æˆåŠŸåº”ç”¨å¹¶é€šè¿‡æµ‹è¯•")
        print("\næ”¹è¿›éªŒè¯ï¼š")
        print("  âœ… äº‹ä»¶åŒ–ç­‰å¾…ï¼ˆstream_mntï¼‰")
        print("  âœ… DRAM é…é¢ä¼˜åŒ–ï¼ˆkv_offloadï¼‰")
        print("  âœ… å†™é˜Ÿåˆ—é˜»å¡å¼å¤„ç†ï¼ˆé¿å… dropï¼‰")
    else:
        print("\nâš ï¸  æµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        print("\nå¸¸è§é—®é¢˜ï¼š")
        print("  1) æ¨¡å‹è·¯å¾„è®¾ç½®æ˜¯å¦æ­£ç¡®")
        print("  2) 16GB æ˜¾å­˜è¯·ä¼˜å…ˆä½¿ç”¨æµå¼æ¨¡å¼ï¼›æˆ–å‡å°‘ batch/max_seq_len")
        print("  3) SSD è·¯å¾„ï¼ˆKVCacheArgs.ssd_device_pathï¼‰æ˜¯å¦å­˜åœ¨ï¼ˆå¯ç½®ç©ºåœç”¨ï¼‰")
