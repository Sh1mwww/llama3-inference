#!/usr/bin/env python3
"""
å®Œæ•´å‹åŠ›æµ‹è¯•è„šæœ¬ï¼šä½¿ç”¨ raw param store å¯¹ llama3.1-8b è¿›è¡Œå…¨å±‚å¤šæ¬¡éªŒè¯
- éå†æ‰€æœ‰å±‚ï¼ˆ32 å±‚ï¼‰
- å¤šæ¬¡åŠ è½½éªŒè¯
- å¸¦ evict æœºåˆ¶é˜²æ­¢ OOM
- æµ‹é‡åŠ è½½é€Ÿåº¦å’Œååé‡
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

import time
import gc
import psutil
import torch
from llama3.raw_param_store import ParamStore


def get_memory_info():
    """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    process = psutil.Process()
    rss_mb = process.memory_info().rss / (1024**2)

    # ç³»ç»Ÿå†…å­˜
    vm = psutil.virtual_memory()
    total_mb = vm.total / (1024**2)
    available_mb = vm.available / (1024**2)
    used_percent = vm.percent

    return {
        "process_rss_mb": rss_mb,
        "system_total_mb": total_mb,
        "system_available_mb": available_mb,
        "system_used_percent": used_percent
    }


def evict_layer(tensors_dict):
    """
    é‡Šæ”¾å±‚æƒé‡ï¼Œé˜²æ­¢ OOM
    """
    if tensors_dict:
        for tensor in tensors_dict.values():
            del tensor
        tensors_dict.clear()

    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()

    # å¦‚æœæœ‰ CUDAï¼Œä¹Ÿæ¸…ç† GPU ç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def test_single_layer(store, layer_id, check_integrity=True, verbose=False):
    """
    æµ‹è¯•å•ä¸ªå±‚çš„åŠ è½½

    Returns:
        dict: åŒ…å« success, load_time_ms, size_mb, num_params, tensors
    """
    mem_before = get_memory_info()

    start_time = time.perf_counter()

    try:
        # åŠ è½½å±‚
        tensors = store.fetch_layer(layer_id=layer_id, only_stream=True)

        load_time = (time.perf_counter() - start_time) * 1000  # ms

        if not tensors:
            return {
                "success": False,
                "error": "No stream tensors found",
                "load_time_ms": load_time,
            }

        # è®¡ç®—å¤§å°
        total_bytes = sum(t.numel() * t.element_size() for t in tensors.values())
        size_mb = total_bytes / (1024**2)
        num_params = len(tensors)

        # å¯é€‰ï¼šå®Œæ•´æ€§æ ¡éªŒ
        integrity_ok = True
        if check_integrity:
            matched, total = store.sanity_check_layer(
                layer_id=layer_id,
                tensors=tensors,
                check_bytes=64,
                verbose=False
            )
            integrity_ok = (matched == total)

        mem_after = get_memory_info()
        mem_delta_mb = mem_after["process_rss_mb"] - mem_before["process_rss_mb"]

        result = {
            "success": True,
            "load_time_ms": load_time,
            "size_mb": size_mb,
            "num_params": num_params,
            "throughput_mbps": size_mb / (load_time / 1000) if load_time > 0 else 0,
            "integrity_ok": integrity_ok,
            "mem_delta_mb": mem_delta_mb,
            "mem_before_mb": mem_before["process_rss_mb"],
            "mem_after_mb": mem_after["process_rss_mb"],
            "tensors": tensors,
        }

        if verbose:
            print(f"   Layer {layer_id}: {size_mb:.2f} MB, "
                  f"{load_time:.2f} ms, {result['throughput_mbps']:.2f} MB/s, "
                  f"integrity={'âœ…' if integrity_ok else 'âŒ'}")

        return result

    except Exception as e:
        load_time = (time.perf_counter() - start_time) * 1000
        return {
            "success": False,
            "error": str(e),
            "load_time_ms": load_time,
        }


def test_all_layers_sequential(store, num_layers, check_integrity=True, evict_after_load=True):
    """
    é¡ºåºæµ‹è¯•æ‰€æœ‰å±‚ï¼ˆå¸¦ evict é˜²æ­¢ OOMï¼‰

    Args:
        store: ParamStore å®ä¾‹
        num_layers: æ€»å±‚æ•°
        check_integrity: æ˜¯å¦è¿›è¡Œå®Œæ•´æ€§æ ¡éªŒ
        evict_after_load: æ˜¯å¦åœ¨åŠ è½½åç«‹å³é‡Šæ”¾ï¼ˆé˜²æ­¢ OOMï¼‰

    Returns:
        dict: æµ‹è¯•ç»“æœç»Ÿè®¡
    """
    print(f"\n{'='*70}")
    print(f"ğŸ“Š é¡ºåºåŠ è½½æ‰€æœ‰ {num_layers} å±‚ï¼ˆevict={evict_after_load}ï¼‰")
    print(f"{'='*70}")

    results = []
    total_size_mb = 0
    total_time_ms = 0
    failed_layers = []

    mem_start = get_memory_info()
    print(f"åˆå§‹å†…å­˜: è¿›ç¨‹ {mem_start['process_rss_mb']:.2f} MB, "
          f"ç³»ç»Ÿå¯ç”¨ {mem_start['system_available_mb']:.2f} MB "
          f"({mem_start['system_used_percent']:.1f}% å·²ä½¿ç”¨)")

    for layer_id in range(num_layers):
        result = test_single_layer(store, layer_id, check_integrity=check_integrity, verbose=True)
        results.append(result)

        if result["success"]:
            total_size_mb += result["size_mb"]
            total_time_ms += result["load_time_ms"]

            # é‡Šæ”¾å†…å­˜é˜²æ­¢ OOM
            if evict_after_load:
                evict_layer(result.get("tensors", {}))
                result["tensors"] = None  # æ¸…é™¤å¼•ç”¨
        else:
            failed_layers.append(layer_id)
            print(f"   âŒ Layer {layer_id} å¤±è´¥: {result.get('error', 'Unknown')}")

    mem_end = get_memory_info()
    mem_growth_mb = mem_end["process_rss_mb"] - mem_start["process_rss_mb"]

    # ç»Ÿè®¡
    success_count = sum(1 for r in results if r["success"])
    avg_load_time_ms = total_time_ms / success_count if success_count > 0 else 0
    avg_throughput_mbps = sum(r.get("throughput_mbps", 0) for r in results if r["success"]) / success_count if success_count > 0 else 0

    integrity_failures = sum(1 for r in results if r["success"] and not r.get("integrity_ok", True))

    print(f"\n{'='*70}")
    print(f"ğŸ“ˆ é¡ºåºåŠ è½½ç»Ÿè®¡:")
    print(f"   æˆåŠŸåŠ è½½: {success_count}/{num_layers} å±‚")
    print(f"   æ€»æ•°æ®é‡: {total_size_mb:.2f} MB")
    print(f"   æ€»è€—æ—¶: {total_time_ms:.2f} ms ({total_time_ms/1000:.2f} s)")
    print(f"   å¹³å‡æ¯å±‚: {avg_load_time_ms:.2f} ms")
    print(f"   å¹³å‡åå: {avg_throughput_mbps:.2f} MB/s")
    print(f"   å®Œæ•´æ€§æ ¡éªŒ: {success_count - integrity_failures}/{success_count} é€šè¿‡")
    print(f"   å†…å­˜å¢é•¿: {mem_growth_mb:+.2f} MB")
    print(f"   æœ€ç»ˆå†…å­˜: è¿›ç¨‹ {mem_end['process_rss_mb']:.2f} MB, "
          f"ç³»ç»Ÿå¯ç”¨ {mem_end['system_available_mb']:.2f} MB")

    if failed_layers:
        print(f"   âŒ å¤±è´¥çš„å±‚: {failed_layers}")

    print(f"{'='*70}")

    return {
        "success_count": success_count,
        "total_layers": num_layers,
        "failed_layers": failed_layers,
        "total_size_mb": total_size_mb,
        "total_time_ms": total_time_ms,
        "avg_load_time_ms": avg_load_time_ms,
        "avg_throughput_mbps": avg_throughput_mbps,
        "integrity_failures": integrity_failures,
        "mem_growth_mb": mem_growth_mb,
        "results": results,
    }


def test_random_access_pattern(store, num_layers, num_iterations=3):
    """
    æµ‹è¯•éšæœºè®¿é—®æ¨¡å¼ï¼ˆæ¨¡æ‹Ÿå®é™…æ¨ç†åœºæ™¯ï¼‰
    æ¯æ¬¡åªä¿ç•™å½“å‰å±‚ï¼Œç«‹å³é‡Šæ”¾

    Args:
        store: ParamStore å®ä¾‹
        num_layers: æ€»å±‚æ•°
        num_iterations: å®Œæ•´éå†æ¬¡æ•°
    """
    print(f"\n{'='*70}")
    print(f"ğŸ”€ éšæœºè®¿é—®æµ‹è¯•ï¼ˆ{num_iterations} æ¬¡å®Œæ•´éå†ï¼‰")
    print(f"{'='*70}")

    import random

    all_load_times = []
    all_throughputs = []
    iteration_stats = []

    for iteration in range(num_iterations):
        print(f"\n--- ç¬¬ {iteration+1}/{num_iterations} æ¬¡éå† ---")

        # ç”Ÿæˆéšæœºè®¿é—®é¡ºåº
        layer_order = list(range(num_layers))
        random.shuffle(layer_order)

        iteration_start = time.perf_counter()
        iteration_size = 0
        success_in_iteration = 0

        for layer_id in layer_order:
            result = test_single_layer(store, layer_id, check_integrity=False, verbose=False)

            if result["success"]:
                all_load_times.append(result["load_time_ms"])
                all_throughputs.append(result["throughput_mbps"])
                iteration_size += result["size_mb"]
                success_in_iteration += 1

                # ç«‹å³é‡Šæ”¾
                evict_layer(result.get("tensors", {}))

            # æ¯ 8 å±‚æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (layer_order.index(layer_id) + 1) % 8 == 0:
                progress = (layer_order.index(layer_id) + 1) / num_layers * 100
                print(f"   è¿›åº¦: {progress:.1f}% ({layer_order.index(layer_id)+1}/{num_layers} å±‚)")

        iteration_time = (time.perf_counter() - iteration_start) * 1000
        iteration_throughput = iteration_size / (iteration_time / 1000) if iteration_time > 0 else 0

        iteration_stats.append({
            "iteration": iteration + 1,
            "success_count": success_in_iteration,
            "total_size_mb": iteration_size,
            "total_time_ms": iteration_time,
            "throughput_mbps": iteration_throughput,
        })

        print(f"   âœ… å®Œæˆ: {success_in_iteration}/{num_layers} å±‚, "
              f"{iteration_size:.2f} MB, {iteration_time:.2f} ms, "
              f"{iteration_throughput:.2f} MB/s")

    # æ€»ç»“ç»Ÿè®¡
    print(f"\n{'='*70}")
    print(f"ğŸ“Š éšæœºè®¿é—®ç»Ÿè®¡:")
    print(f"   æ€»åŠ è½½æ¬¡æ•°: {len(all_load_times)}")
    print(f"   å¹³å‡åŠ è½½æ—¶é—´: {sum(all_load_times)/len(all_load_times):.2f} ms")
    print(f"   æœ€å°/æœ€å¤§åŠ è½½æ—¶é—´: {min(all_load_times):.2f} / {max(all_load_times):.2f} ms")
    print(f"   å¹³å‡ååé‡: {sum(all_throughputs)/len(all_throughputs):.2f} MB/s")
    print(f"   æœ€å°/æœ€å¤§ååé‡: {min(all_throughputs):.2f} / {max(all_throughputs):.2f} MB/s")

    print(f"\n   å„æ¬¡éå†å¯¹æ¯”:")
    for stat in iteration_stats:
        print(f"      ç¬¬ {stat['iteration']} æ¬¡: {stat['total_time_ms']:.2f} ms, "
              f"{stat['throughput_mbps']:.2f} MB/s")

    print(f"{'='*70}")

    return {
        "num_iterations": num_iterations,
        "total_loads": len(all_load_times),
        "avg_load_time_ms": sum(all_load_times) / len(all_load_times) if all_load_times else 0,
        "avg_throughput_mbps": sum(all_throughputs) / len(all_throughputs) if all_throughputs else 0,
        "iteration_stats": iteration_stats,
    }


def test_concurrent_loading(store, num_layers, batch_size=4):
    """
    æµ‹è¯•å¹¶å‘æ‰¹é‡åŠ è½½ï¼ˆä½¿ç”¨ fetch_layer_batchï¼‰
    """
    print(f"\n{'='*70}")
    print(f"âš¡ å¹¶å‘æ‰¹é‡åŠ è½½æµ‹è¯•ï¼ˆbatch_size={batch_size}ï¼‰")
    print(f"{'='*70}")

    batches = [list(range(i, min(i + batch_size, num_layers)))
               for i in range(0, num_layers, batch_size)]

    total_size_mb = 0
    total_time_ms = 0
    successful_batches = 0

    mem_start = get_memory_info()

    for batch_idx, batch_layers in enumerate(batches):
        print(f"\næ‰¹æ¬¡ {batch_idx+1}/{len(batches)}: å±‚ {batch_layers}")

        start_time = time.perf_counter()

        try:
            # å¹¶å‘åŠ è½½
            batch_tensors = store.fetch_layer_batch(batch_layers, only_stream=True)

            load_time = (time.perf_counter() - start_time) * 1000

            # è®¡ç®—å¤§å°
            batch_size_mb = 0
            for layer_id, tensors in batch_tensors.items():
                layer_size = sum(t.numel() * t.element_size() for t in tensors.values())
                batch_size_mb += layer_size / (1024**2)

            throughput = batch_size_mb / (load_time / 1000) if load_time > 0 else 0

            print(f"   âœ… åŠ è½½æˆåŠŸ: {len(batch_tensors)} å±‚, "
                  f"{batch_size_mb:.2f} MB, {load_time:.2f} ms, "
                  f"{throughput:.2f} MB/s")

            total_size_mb += batch_size_mb
            total_time_ms += load_time
            successful_batches += 1

            # é‡Šæ”¾
            for tensors in batch_tensors.values():
                evict_layer(tensors)

        except Exception as e:
            print(f"   âŒ æ‰¹æ¬¡åŠ è½½å¤±è´¥: {e}")

    mem_end = get_memory_info()

    print(f"\n{'='*70}")
    print(f"ğŸ“Š å¹¶å‘åŠ è½½ç»Ÿè®¡:")
    print(f"   æˆåŠŸæ‰¹æ¬¡: {successful_batches}/{len(batches)}")
    print(f"   æ€»æ•°æ®é‡: {total_size_mb:.2f} MB")
    print(f"   æ€»è€—æ—¶: {total_time_ms:.2f} ms")
    print(f"   å¹³å‡åå: {total_size_mb / (total_time_ms/1000) if total_time_ms > 0 else 0:.2f} MB/s")
    print(f"   å†…å­˜å˜åŒ–: {mem_end['process_rss_mb'] - mem_start['process_rss_mb']:+.2f} MB")
    print(f"{'='*70}")


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""

    print("=" * 70)
    print("ğŸš€ LLaMA3.1-8B Raw Param Store å®Œæ•´å‹åŠ›æµ‹è¯•")
    print("=" * 70)

    manifest_path = "/data1/llama3.1-8B.runtime_manifest.json"

    try:
        # åˆ›å»º ParamStore
        print("\n[1/6] åˆå§‹åŒ– ParamStore...")
        store = ParamStore(
            manifest_or_path=manifest_path,
            method="bytecopy",
            staging_mb=32,
            rw=False,
            max_concurrent_io=4
        )
        print("âœ… ParamStore åˆ›å»ºæˆåŠŸ")

        # è·å–æ¨¡å‹ä¿¡æ¯
        stats = store.get_storage_stats()
        print(f"\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        print(f"   æ€»å‚æ•°: {stats['total_params']}")
        print(f"   æ€»å¤§å°: {stats['total_gb']:.2f} GB")
        print(f"   Stream: {stats['stream_gb']:.2f} GB ({stats['stream_bytes']/stats['total_bytes']*100:.1f}%)")
        print(f"   Raw è®¾å¤‡: {stats['raw_device']}")

        # ç¡®å®šå±‚æ•°
        num_layers = 32  # llama3.1-8b
        print(f"   æ¨¡å‹å±‚æ•°: {num_layers}")

        # æµ‹è¯• 1: é¡ºåºåŠ è½½æ‰€æœ‰å±‚ï¼ˆå¸¦ evictï¼‰
        print("\n[2/6] æµ‹è¯• 1: é¡ºåºåŠ è½½æ‰€æœ‰å±‚ï¼ˆå¸¦ evictï¼‰")
        seq_result = test_all_layers_sequential(
            store, num_layers,
            check_integrity=True,
            evict_after_load=True
        )

        # å¼ºåˆ¶æ¸…ç†
        gc.collect()
        time.sleep(1)

        # æµ‹è¯• 2: éšæœºè®¿é—®æ¨¡å¼ï¼ˆå¤šæ¬¡éå†ï¼‰
        print("\n[3/6] æµ‹è¯• 2: éšæœºè®¿é—®æ¨¡å¼")
        random_result = test_random_access_pattern(store, num_layers, num_iterations=3)

        gc.collect()
        time.sleep(1)

        # æµ‹è¯• 3: å¹¶å‘æ‰¹é‡åŠ è½½
        print("\n[4/6] æµ‹è¯• 3: å¹¶å‘æ‰¹é‡åŠ è½½")
        test_concurrent_loading(store, num_layers, batch_size=4)

        gc.collect()
        time.sleep(1)

        # æµ‹è¯• 4: ä¸å¸¦ evict çš„é¡ºåºåŠ è½½ï¼ˆæµ‹è¯•å†…å­˜å‹åŠ›ï¼‰
        print("\n[5/6] æµ‹è¯• 4: ä¸å¸¦ evict çš„é¡ºåºåŠ è½½ï¼ˆæµ‹è¯•å†…å­˜å‹åŠ›ï¼‰")
        print("âš ï¸  è­¦å‘Š: æ­¤æµ‹è¯•å¯èƒ½æ¶ˆè€—å¤§é‡å†…å­˜")
        mem_before = get_memory_info()
        if mem_before["system_available_mb"] < 15000:  # å°‘äº 15GB å¯ç”¨å†…å­˜
            print(f"âŒ è·³è¿‡ï¼šç³»ç»Ÿå¯ç”¨å†…å­˜ä¸è¶³ ({mem_before['system_available_mb']:.2f} MB)")
        else:
            no_evict_result = test_all_layers_sequential(
                store, num_layers,
                check_integrity=False,
                evict_after_load=False
            )
            print(f"   ä¿ç•™æ‰€æœ‰å±‚åçš„å†…å­˜: {get_memory_info()['process_rss_mb']:.2f} MB")

            # æ¸…ç†
            gc.collect()

        # æµ‹è¯• 5: å¼‚æ­¥åŠ è½½å‹åŠ›æµ‹è¯•
        print("\n[6/6] æµ‹è¯• 5: å¼‚æ­¥åŠ è½½å‹åŠ›æµ‹è¯•")
        print("æäº¤å¤šä¸ªå¼‚æ­¥åŠ è½½ä»»åŠ¡...")

        futures = []
        for layer_id in range(min(8, num_layers)):
            future = store.fetch_layer_async(layer_id, only_stream=True)
            futures.append((layer_id, future))

        print(f"   æäº¤äº† {len(futures)} ä¸ªå¼‚æ­¥ä»»åŠ¡")

        for layer_id, future in futures:
            result = future.result()
            size_mb = sum(t.numel() * t.element_size() for t in result.values()) / (1024**2)
            print(f"   âœ… Layer {layer_id}: {size_mb:.2f} MB")
            evict_layer(result)

        # æœ€ç»ˆæ€»ç»“
        print(f"\n{'='*70}")
        print(f"ğŸ‰ æ‰€æœ‰å‹åŠ›æµ‹è¯•å®Œæˆï¼")
        print(f"{'='*70}")
        print(f"é¡ºåºåŠ è½½: {seq_result['success_count']}/{seq_result['total_layers']} å±‚æˆåŠŸ")
        print(f"   å¹³å‡è€—æ—¶: {seq_result['avg_load_time_ms']:.2f} ms/å±‚")
        print(f"   å¹³å‡åå: {seq_result['avg_throughput_mbps']:.2f} MB/s")
        print(f"   å®Œæ•´æ€§: {seq_result['success_count'] - seq_result['integrity_failures']}/{seq_result['success_count']} é€šè¿‡")
        print(f"\néšæœºè®¿é—®: {random_result['num_iterations']} æ¬¡éå†")
        print(f"   å¹³å‡è€—æ—¶: {random_result['avg_load_time_ms']:.2f} ms/å±‚")
        print(f"   å¹³å‡åå: {random_result['avg_throughput_mbps']:.2f} MB/s")
        print(f"{'='*70}")

        # å…³é—­
        store.close()
        print("\nâœ… ParamStore å·²å…³é—­")

        return True

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
