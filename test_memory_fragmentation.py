#!/usr/bin/env python3
"""
GPUå†…å­˜ç¢ç‰‡åŒ–æ£€æµ‹å·¥å…·
ç›‘æ§CUDAå†…å­˜åˆ†é…å’Œç¢ç‰‡åŒ–æƒ…å†µ
"""

import torch
import gc
import time
import json
from typing import Dict, List
import matplotlib.pyplot as plt
import numpy as np

def get_gpu_memory_info():
    """è·å–è¯¦ç»†çš„GPUå†…å­˜ä¿¡æ¯"""
    if not torch.cuda.is_available():
        return None
    
    # åŸºç¡€å†…å­˜ä¿¡æ¯
    allocated = torch.cuda.memory_allocated()
    reserved = torch.cuda.memory_reserved()
    max_allocated = torch.cuda.max_memory_allocated()
    max_reserved = torch.cuda.max_memory_reserved()
    
    # å†…å­˜ç»Ÿè®¡ä¿¡æ¯
    memory_stats = torch.cuda.memory_stats()
    
    return {
        'allocated_mb': allocated / 1024**2,
        'reserved_mb': reserved / 1024**2,
        'max_allocated_mb': max_allocated / 1024**2,
        'max_reserved_mb': max_reserved / 1024**2,
        'fragmentation_ratio': (reserved - allocated) / reserved if reserved > 0 else 0,
        'allocation_count': memory_stats.get('allocation.all.current', 0),
        'free_count': memory_stats.get('free_retries.all.current', 0),
        'segment_count': memory_stats.get('segment.all.current', 0),
        'large_pool_allocated': memory_stats.get('allocated_bytes.large_pool.current', 0) / 1024**2,
        'small_pool_allocated': memory_stats.get('allocated_bytes.small_pool.current', 0) / 1024**2,
    }

def test_allocation_pattern(sizes: List[int], pattern: str = "sequential") -> List[Dict]:
    """æµ‹è¯•ä¸åŒåˆ†é…æ¨¡å¼çš„ç¢ç‰‡åŒ–æƒ…å†µ"""
    print(f"ğŸ§ª æµ‹è¯•åˆ†é…æ¨¡å¼: {pattern}")
    
    results = []
    tensors = []
    
    # æ¸…ç†åˆå§‹çŠ¶æ€
    torch.cuda.empty_cache()
    initial_info = get_gpu_memory_info()
    results.append({
        'step': 'initial',
        'pattern': pattern,
        **initial_info
    })
    
    if pattern == "sequential":
        # é¡ºåºåˆ†é…
        for i, size in enumerate(sizes):
            try:
                tensor = torch.randn(size, device='cuda')
                tensors.append(tensor)
                
                info = get_gpu_memory_info()
                results.append({
                    'step': f'alloc_{i}',
                    'size_mb': size * 4 / 1024**2,  # float32 = 4 bytes
                    'pattern': pattern,
                    **info
                })
                
            except torch.cuda.OutOfMemoryError:
                print(f"âŒ OOM at allocation {i}, size: {size}")
                break
    
    elif pattern == "random_free":
        # éšæœºåˆ†é…å’Œé‡Šæ”¾
        for i, size in enumerate(sizes):
            try:
                tensor = torch.randn(size, device='cuda')
                tensors.append(tensor)
                
                # éšæœºé‡Šæ”¾ä¸€äº›tensor
                if len(tensors) > 3 and i % 3 == 0:
                    idx_to_free = np.random.randint(0, len(tensors))
                    del tensors[idx_to_free]
                    tensors = [t for j, t in enumerate(tensors) if j != idx_to_free]
                    gc.collect()
                    torch.cuda.empty_cache()
                
                info = get_gpu_memory_info()
                results.append({
                    'step': f'alloc_free_{i}',
                    'size_mb': size * 4 / 1024**2,
                    'pattern': pattern,
                    **info
                })
                
            except torch.cuda.OutOfMemoryError:
                print(f"âŒ OOM at allocation {i}, size: {size}")
                break
    
    elif pattern == "alternating_sizes":
        # äº¤æ›¿å¤§å°åˆ†é…
        large_size = max(sizes)
        small_size = min(sizes)
        
        for i in range(len(sizes)):
            try:
                size = large_size if i % 2 == 0 else small_size
                tensor = torch.randn(size, device='cuda')
                tensors.append(tensor)
                
                info = get_gpu_memory_info()
                results.append({
                    'step': f'alternating_{i}',
                    'size_mb': size * 4 / 1024**2,
                    'pattern': pattern,
                    **info
                })
                
            except torch.cuda.OutOfMemoryError:
                print(f"âŒ OOM at allocation {i}")
                break
    
    # æ¸…ç†
    del tensors
    gc.collect()
    torch.cuda.empty_cache()
    
    final_info = get_gpu_memory_info()
    results.append({
        'step': 'final_cleanup',
        'pattern': pattern,
        **final_info
    })
    
    return results

def analyze_fragmentation(results: List[Dict]) -> Dict:
    """åˆ†æç¢ç‰‡åŒ–ç¨‹åº¦"""
    
    max_fragmentation = max(r['fragmentation_ratio'] for r in results if 'fragmentation_ratio' in r)
    avg_fragmentation = np.mean([r['fragmentation_ratio'] for r in results if 'fragmentation_ratio' in r])
    
    max_segments = max(r['segment_count'] for r in results if 'segment_count' in r)
    
    # å†…å­˜æ•ˆç‡ï¼šåˆ†é…çš„å†…å­˜ / ä¿ç•™çš„å†…å­˜
    efficiency_ratios = [r['allocated_mb'] / r['reserved_mb'] if r['reserved_mb'] > 0 else 1.0 
                        for r in results if 'allocated_mb' in r and 'reserved_mb' in r]
    min_efficiency = min(efficiency_ratios) if efficiency_ratios else 1.0
    
    return {
        'max_fragmentation_ratio': max_fragmentation,
        'avg_fragmentation_ratio': avg_fragmentation,
        'max_segments': max_segments,
        'min_efficiency': min_efficiency,
        'severity': 'HIGH' if max_fragmentation > 0.3 else 'MEDIUM' if max_fragmentation > 0.15 else 'LOW'
    }

def plot_fragmentation_timeline(results_by_pattern: Dict[str, List[Dict]]):
    """ç»˜åˆ¶ç¢ç‰‡åŒ–æ—¶é—´çº¿å›¾"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('GPU Memory Fragmentation Analysis', fontsize=16)
    
    for pattern, results in results_by_pattern.items():
        steps = list(range(len(results)))
        fragmentation_ratios = [r.get('fragmentation_ratio', 0) for r in results]
        allocated_mb = [r.get('allocated_mb', 0) for r in results]
        reserved_mb = [r.get('reserved_mb', 0) for r in results]
        segment_counts = [r.get('segment_count', 0) for r in results]
        
        # ç¢ç‰‡åŒ–æ¯”ä¾‹
        axes[0, 0].plot(steps, fragmentation_ratios, label=pattern, marker='o')
        axes[0, 0].set_title('Fragmentation Ratio Over Time')
        axes[0, 0].set_ylabel('Fragmentation Ratio')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # å†…å­˜ä½¿ç”¨
        axes[0, 1].plot(steps, allocated_mb, label=f'{pattern} (allocated)', linestyle='-')
        axes[0, 1].plot(steps, reserved_mb, label=f'{pattern} (reserved)', linestyle='--')
        axes[0, 1].set_title('Memory Usage Over Time')
        axes[0, 1].set_ylabel('Memory (MB)')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # æ®µæ•°é‡
        axes[1, 0].plot(steps, segment_counts, label=pattern, marker='s')
        axes[1, 0].set_title('Memory Segments Over Time')
        axes[1, 0].set_ylabel('Segment Count')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # æ•ˆç‡æ¯”
        efficiency = [a/r if r > 0 else 1.0 for a, r in zip(allocated_mb, reserved_mb)]
        axes[1, 1].plot(steps, efficiency, label=pattern, marker='^')
        axes[1, 1].set_title('Memory Efficiency Over Time')
        axes[1, 1].set_ylabel('Efficiency (allocated/reserved)')
        axes[1, 1].legend()
        axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig('memory_fragmentation_analysis.png', dpi=300, bbox_inches='tight')
    print("ğŸ“Š å›¾è¡¨å·²ä¿å­˜ä¸º memory_fragmentation_analysis.png")

def simulate_weight_streaming_memory_pattern():
    """æ¨¡æ‹Ÿæƒé‡æµå¼ä¼ è¾“çš„å†…å­˜åˆ†é…æ¨¡å¼"""
    print("ğŸ”„ æ¨¡æ‹Ÿæƒé‡æµå¼ä¼ è¾“å†…å­˜æ¨¡å¼")
    
    # æ¨¡æ‹Ÿä¸åŒå¤§å°çš„æƒé‡çŸ©é˜µ
    weight_sizes = [
        1024 * 1024,      # 1M elements ~4MB
        2048 * 2048,      # 4M elements ~16MB  
        1024 * 4096,      # 4M elements ~16MB
        4096 * 1024,      # 4M elements ~16MB
        512 * 512,        # 256K elements ~1MB
        8192 * 512,       # 4M elements ~16MB
    ]
    
    results = []
    weight_cache = {}
    max_cached_layers = 4
    
    torch.cuda.empty_cache()
    initial_info = get_gpu_memory_info()
    results.append({
        'step': 'initial',
        'operation': 'start',
        **initial_info
    })
    
    # æ¨¡æ‹Ÿå¤šå±‚æ¨ç†è¿‡ç¨‹
    for layer_id in range(8):
        print(f"  å¤„ç†Layer {layer_id}")
        
        # 1. æ£€æŸ¥ç¼“å­˜ï¼Œå¦‚æœè¶…å‡ºé™åˆ¶åˆ™é€å‡º
        if len(weight_cache) >= max_cached_layers:
            # LRUé€å‡º
            oldest_layer = min(weight_cache.keys())
            del weight_cache[oldest_layer]
            gc.collect()
            
            info = get_gpu_memory_info()
            results.append({
                'step': f'layer_{layer_id}_evict',
                'operation': f'evict_layer_{oldest_layer}',
                **info
            })
        
        # 2. åŠ è½½å½“å‰å±‚æƒé‡
        try:
            layer_weights = []
            for i, size in enumerate(weight_sizes):
                weight = torch.randn(size, device='cuda')
                layer_weights.append(weight)
            
            weight_cache[layer_id] = layer_weights
            
            info = get_gpu_memory_info()
            results.append({
                'step': f'layer_{layer_id}_load',
                'operation': f'load_layer_{layer_id}',
                **info
            })
            
        except torch.cuda.OutOfMemoryError:
            print(f"âŒ OOM loading layer {layer_id}")
            break
        
        # 3. æ¨¡æ‹Ÿè®¡ç®—ï¼ˆå°é‡æ¿€æ´»å†…å­˜ï¼‰
        try:
            activations = torch.randn(2048 * 1024, device='cuda')  # ~8MB
            
            info = get_gpu_memory_info()
            results.append({
                'step': f'layer_{layer_id}_compute',
                'operation': f'compute_layer_{layer_id}',
                **info
            })
            
            del activations
            
        except torch.cuda.OutOfMemoryError:
            print(f"âŒ OOM during computation layer {layer_id}")
    
    # æ¸…ç†
    weight_cache.clear()
    gc.collect()
    torch.cuda.empty_cache()
    
    final_info = get_gpu_memory_info()
    results.append({
        'step': 'final_cleanup',
        'operation': 'cleanup',
        **final_info
    })
    
    return results

def main():
    print("ğŸ” GPUå†…å­˜ç¢ç‰‡åŒ–æ£€æµ‹å¼€å§‹")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return
    
    # è·å–GPUä¿¡æ¯
    gpu_name = torch.cuda.get_device_name()
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"ğŸ”§ GPU: {gpu_name}")
    print(f"ğŸ’¾ æ€»å†…å­˜: {total_memory:.1f} GB")
    
    # æµ‹è¯•å¤§å°ï¼ˆå…ƒç´ æ•°é‡ï¼‰
    test_sizes = [
        512 * 512,        # ~1MB
        1024 * 1024,      # ~4MB
        2048 * 1024,      # ~8MB
        2048 * 2048,      # ~16MB
        1024 * 4096,      # ~16MB
        4096 * 1024,      # ~16MB
    ]
    
    results_by_pattern = {}
    
    # æµ‹è¯•ä¸åŒåˆ†é…æ¨¡å¼
    patterns = ["sequential", "random_free", "alternating_sizes"]
    
    for pattern in patterns:
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å¼: {pattern}")
        results = test_allocation_pattern(test_sizes, pattern)
        results_by_pattern[pattern] = results
        
        # åˆ†æè¿™ä¸ªæ¨¡å¼çš„ç¢ç‰‡åŒ–
        analysis = analyze_fragmentation(results)
        print(f"  æœ€å¤§ç¢ç‰‡åŒ–æ¯”ä¾‹: {analysis['max_fragmentation_ratio']:.3f}")
        print(f"  å¹³å‡ç¢ç‰‡åŒ–æ¯”ä¾‹: {analysis['avg_fragmentation_ratio']:.3f}")
        print(f"  æœ€å¤§æ®µæ•°: {analysis['max_segments']}")
        print(f"  æœ€ä½æ•ˆç‡: {analysis['min_efficiency']:.3f}")
        print(f"  ä¸¥é‡ç¨‹åº¦: {analysis['severity']}")
    
    # æµ‹è¯•æƒé‡æµå¼ä¼ è¾“æ¨¡å¼
    print(f"\nğŸ”„ æµ‹è¯•æƒé‡æµå¼ä¼ è¾“æ¨¡å¼")
    streaming_results = simulate_weight_streaming_memory_pattern()
    results_by_pattern["weight_streaming"] = streaming_results
    
    streaming_analysis = analyze_fragmentation(streaming_results)
    print(f"  æƒé‡æµå¼ä¼ è¾“ç¢ç‰‡åŒ–: {streaming_analysis['max_fragmentation_ratio']:.3f}")
    print(f"  æƒé‡æµå¼ä¼ è¾“ä¸¥é‡ç¨‹åº¦: {streaming_analysis['severity']}")
    
    # ä¿å­˜ç»“æœ
    with open('memory_fragmentation_results.json', 'w') as f:
        json.dump(results_by_pattern, f, indent=2)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° memory_fragmentation_results.json")
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    print(f"\nğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    try:
        plot_fragmentation_timeline(results_by_pattern)
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")
    
    # æ€»ç»“å»ºè®®
    print(f"\nğŸ“‹ ç¢ç‰‡åŒ–æ£€æµ‹æ€»ç»“")
    print("=" * 30)
    
    all_analyses = {}
    for pattern, results in results_by_pattern.items():
        all_analyses[pattern] = analyze_fragmentation(results)
    
    worst_pattern = max(all_analyses.keys(), 
                       key=lambda k: all_analyses[k]['max_fragmentation_ratio'])
    best_pattern = min(all_analyses.keys(), 
                      key=lambda k: all_analyses[k]['max_fragmentation_ratio'])
    
    print(f"ğŸ”´ æœ€ä¸¥é‡ç¢ç‰‡åŒ–æ¨¡å¼: {worst_pattern} ({all_analyses[worst_pattern]['max_fragmentation_ratio']:.3f})")
    print(f"ğŸŸ¢ æœ€è½»å¾®ç¢ç‰‡åŒ–æ¨¡å¼: {best_pattern} ({all_analyses[best_pattern]['max_fragmentation_ratio']:.3f})")
    
    # å»ºè®®
    if all_analyses[worst_pattern]['max_fragmentation_ratio'] > 0.3:
        print("\nâš ï¸  é«˜ç¢ç‰‡åŒ–é£é™©æ£€æµ‹åˆ°!")
        print("å»ºè®®:")
        print("1. å¢åŠ æƒé‡ç¼“å­˜å¤§å° (max_cached_layers)")
        print("2. ä½¿ç”¨å†…å­˜æ± é¢„åˆ†é…")
        print("3. å®šæœŸè°ƒç”¨ torch.cuda.empty_cache()")
        print("4. ä¼˜åŒ–æƒé‡åŠ è½½é¡ºåº")

if __name__ == "__main__":
    main()