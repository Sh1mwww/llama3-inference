#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„GSPé”™è¯¯ç›‘æ§å™¨ - é€‚åˆé•¿æœŸè¿è¡Œ
è§£å†³å†…å­˜æ³„æ¼ã€åŠŸè€—å’Œæ€§èƒ½é—®é¢˜
"""

import torch
import time
import threading
import logging
import logging.handlers
import subprocess
import sys
import gc
import os
from typing import Optional, Dict
from contextlib import contextmanager

# å¯é€‰å¯¼å…¥psutil
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# é…ç½®è½»é‡çº§æ—¥å¿—ï¼Œé¿å…æ—¥å¿—æ–‡ä»¶è¿‡å¤§
log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# åˆ›å»ºæ—‹è½¬æ–‡ä»¶å¤„ç†å™¨
rotating_handler = logging.handlers.RotatingFileHandler(
    'gsp_monitor.log', 
    maxBytes=10*1024*1024,  # 10MB
    backupCount=2
)
rotating_handler.setFormatter(log_formatter)

# åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(log_formatter)

# é…ç½®æ ¹æ—¥å¿—å™¨
logging.basicConfig(
    level=logging.INFO,
    handlers=[console_handler, rotating_handler]
)
logger = logging.getLogger(__name__)

class OptimizedGSPMonitor:
    """ä¼˜åŒ–çš„GSPé”™è¯¯ç›‘æ§å™¨ - é€‚åˆé•¿æœŸè¿è¡Œ"""
    
    def __init__(self, device: str = "cuda:0", base_interval: float = 60.0):
        self.device = device
        self.base_interval = base_interval  # åŸºç¡€æ£€æŸ¥é—´éš”
        self.current_interval = base_interval
        self.monitoring = False
        self.monitor_thread = None
        self.last_error_time = 0
        self.error_count = 0
        self.health_streak = 0  # è¿ç»­å¥åº·æ£€æŸ¥æ¬¡æ•°
        
        # æ™ºèƒ½æ¨¡å¼
        self.smart_mode = True  # å¯ç”¨æ™ºèƒ½ç›‘æ§
        self.min_interval = 30.0   # æœ€å°é—´éš”
        self.max_interval = 300.0  # æœ€å¤§é—´éš”ï¼ˆ5åˆ†é’Ÿï¼‰
        
        # èµ„æºç®¡ç†
        self.keepalive_tensor = None
        self.last_cleanup = time.time()
        self.cleanup_interval = 3600  # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡
        
        # æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA not available")
            
        self.device_id = int(device.split(":")[1]) if ":" in device else 0
        
        # å»¶è¿Ÿåˆå§‹åŒ–keepaliveå¼ é‡
        self._keepalive_active = False
        
    def _init_keepalive(self):
        """æŒ‰éœ€åˆå§‹åŒ–ä¿æ´»å¼ é‡"""
        if self._keepalive_active and self.keepalive_tensor is not None:
            return True
            
        try:
            # åˆ›å»ºæœ€å°çš„å¼ é‡ä¿æŒGPUæ´»è·ƒ
            self.keepalive_tensor = torch.ones(10, 10, device=self.device, dtype=torch.float16)
            self._keepalive_active = True
            logger.info(f"Minimal keepalive tensor created (400 bytes)")
            return True
        except Exception as e:
            logger.error(f"Failed to create keepalive tensor: {e}")
            self._keepalive_active = False
            return False
            
    def _cleanup_keepalive(self):
        """æ¸…ç†ä¿æ´»å¼ é‡"""
        if self.keepalive_tensor is not None:
            del self.keepalive_tensor
            self.keepalive_tensor = None
            self._keepalive_active = False
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            logger.debug("Keepalive tensor cleaned up")
    
    @contextmanager
    def _temporary_keepalive(self):
        """ä¸´æ—¶keepaliveä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        was_active = self._keepalive_active
        if not was_active:
            self._init_keepalive()
        try:
            yield
        finally:
            if not was_active:
                self._cleanup_keepalive()
    
    def check_gpu_health(self, lightweight: bool = False) -> Dict:
        """æ£€æŸ¥GPUå¥åº·çŠ¶æ€
        
        Args:
            lightweight: æ˜¯å¦ä½¿ç”¨è½»é‡çº§æ£€æŸ¥ï¼ˆä¸åˆ›å»ºå¼ é‡ï¼‰
        """
        try:
            # åŸºæœ¬CUDAæ£€æŸ¥
            if not torch.cuda.is_available():
                return {"status": "cuda_unavailable", "error": "CUDA not available"}
            
            # æ£€æŸ¥å†…å­˜çŠ¶æ€ï¼ˆè½»é‡çº§ï¼‰
            memory_info = torch.cuda.memory_stats(self.device_id)
            allocated = memory_info.get("allocated_bytes.all.current", 0)
            
            health_data = {
                "status": "healthy",
                "allocated_mb": allocated / (1024**2),
                "timestamp": time.time()
            }
            
            # éè½»é‡çº§æ£€æŸ¥ï¼šè¿›è¡Œç®€å•è®¡ç®—æµ‹è¯•
            if not lightweight:
                with self._temporary_keepalive():
                    if self.keepalive_tensor is not None:
                        # æœ€å°çš„è®¡ç®—æµ‹è¯•
                        result = self.keepalive_tensor.sum()
                        del result
            
            # è·å–nvidia-smiä¿¡æ¯ï¼ˆé™ä½é¢‘ç‡ï¼‰
            if not lightweight or self.health_streak % 5 == 0:
                gpu_info = self._check_nvidia_smi()
                health_data["gpu_info"] = gpu_info
                
                # æ£€æŸ¥æ¸©åº¦å’ŒåŠŸè€—
                if gpu_info:
                    temp = gpu_info.get("temperature", 0)
                    power = gpu_info.get("power_draw", 0)
                    if temp > 85:  # é«˜æ¸©è­¦å‘Š
                        health_data["warning"] = f"High temperature: {temp}Â°C"
                    if power > 300:  # é«˜åŠŸè€—è­¦å‘Š
                        health_data["warning"] = f"High power draw: {power}W"
            
            return health_data
            
        except RuntimeError as e:
            error_msg = str(e).lower()
            if "gsp" in error_msg or "gpu system processor" in error_msg:
                logger.error(f"GSP error detected: {e}")
                self._handle_gsp_error()
                return {"status": "gsp_error", "error": str(e), "timestamp": time.time()}
            else:
                logger.warning(f"GPU health check failed: {e}")
                return {"status": "gpu_error", "error": str(e), "timestamp": time.time()}
                
        except Exception as e:
            logger.error(f"Unexpected error in GPU health check: {e}")
            return {"status": "unknown_error", "error": str(e), "timestamp": time.time()}
    
    def _check_nvidia_smi(self) -> Optional[Dict]:
        """æ£€æŸ¥nvidia-smiçŠ¶æ€ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        try:
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=temperature.gpu,power.draw,utilization.gpu,memory.used', 
                 '--format=csv,noheader,nounits'], 
                capture_output=True, text=True, timeout=3
            )
            if result.returncode == 0:
                parts = result.stdout.strip().split(', ')
                return {
                    "temperature": float(parts[0]),
                    "power_draw": float(parts[1]),
                    "utilization": float(parts[2]),
                    "memory_used_mb": float(parts[3])
                }
        except Exception as e:
            logger.debug(f"nvidia-smi check failed: {e}")
        return None
    
    def _handle_gsp_error(self):
        """å¤„ç†GSPé”™è¯¯"""
        current_time = time.time()
        
        # è®°å½•é”™è¯¯é¢‘ç‡
        if current_time - self.last_error_time < 300:  # 5åˆ†é’Ÿå†…é‡å¤é”™è¯¯
            self.error_count += 1
        else:
            self.error_count = 1
            
        self.last_error_time = current_time
        self.health_streak = 0  # é‡ç½®å¥åº·streak
        
        logger.warning(f"GSP error #{self.error_count} detected")
        
        # å°è¯•æ¢å¤
        self._attempt_recovery()
        
        # è°ƒæ•´ç›‘æ§ç­–ç•¥
        if self.smart_mode:
            # GSPé”™è¯¯åæé«˜ç›‘æ§é¢‘ç‡
            self.current_interval = max(self.min_interval, self.current_interval * 0.5)
            logger.info(f"Increased monitoring frequency to {self.current_interval}s")
        
        # å¦‚æœé”™è¯¯è¿‡äºé¢‘ç¹ï¼Œå»ºè®®é‡å¯
        if self.error_count >= 3:
            logger.error("Multiple GSP errors detected! Consider rebooting or disabling GSP firmware.")
            
    def _attempt_recovery(self):
        """è½»é‡çº§GSPé”™è¯¯æ¢å¤"""
        try:
            # 1. æ¸…ç†æ‰€æœ‰GPUèµ„æº
            self._cleanup_keepalive()
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                # é¿å…åŒæ­¥ï¼Œé™¤éå¿…è¦
                # torch.cuda.synchronize(self.device_id)
                
            # 2. å¼ºåˆ¶Pythonåƒåœ¾å›æ”¶
            gc.collect()
            
            # 3. çŸ­æš‚ç­‰å¾…è®©GPUç¨³å®š
            time.sleep(2)
            
            logger.info("Lightweight GSP recovery completed")
            
        except Exception as e:
            logger.error(f"GSP recovery failed: {e}")
    
    def _adaptive_interval(self):
        """æ™ºèƒ½è°ƒæ•´ç›‘æ§é—´éš”"""
        if not self.smart_mode:
            return
            
        # è¿ç»­å¥åº·æ—¶é™ä½é¢‘ç‡
        if self.health_streak > 10:
            self.current_interval = min(self.max_interval, self.current_interval * 1.2)
        elif self.health_streak > 5:
            self.current_interval = min(self.max_interval, self.current_interval * 1.1)
        elif self.error_count > 0:
            # æœ‰é”™è¯¯å†å²æ—¶ä¿æŒè¾ƒé«˜é¢‘ç‡
            self.current_interval = max(self.min_interval, self.current_interval * 0.9)
    
    def _periodic_cleanup(self):
        """å®šæœŸæ¸…ç†èµ„æº"""
        current_time = time.time()
        if current_time - self.last_cleanup > self.cleanup_interval:
            logger.info("Performing periodic cleanup...")
            
            # æ¸…ç†GPUèµ„æº
            was_active = self._keepalive_active
            self._cleanup_keepalive()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # è®°å½•å†…å­˜ä½¿ç”¨ï¼ˆå¦‚æœpsutilå¯ç”¨ï¼‰
            if PSUTIL_AVAILABLE:
                try:
                    process = psutil.Process(os.getpid())
                    memory_mb = process.memory_info().rss / 1024 / 1024
                    logger.info(f"Process memory usage: {memory_mb:.1f}MB")
                except Exception as e:
                    logger.debug(f"Memory check failed: {e}")
            else:
                logger.debug("psutil not available for memory monitoring")
            
            # å¦‚æœkeepaliveä¹‹å‰æ˜¯æ´»è·ƒçš„ï¼Œé‡æ–°åˆ›å»º
            if was_active:
                self._init_keepalive()
                
            self.last_cleanup = current_time
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        if self.monitoring:
            return
            
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        logger.info(f"Optimized GSP monitoring started (base interval: {self.base_interval}s, smart mode: {self.smart_mode})")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=10)
        self._cleanup_keepalive()
        logger.info("GSP monitoring stopped and resources cleaned")
    
    def _monitor_loop(self):
        """ä¼˜åŒ–çš„ç›‘æ§å¾ªç¯"""
        consecutive_errors = 0
        
        while self.monitoring:
            try:
                # æ ¹æ®å¥åº·çŠ¶æ€é€‰æ‹©æ£€æŸ¥ç±»å‹
                lightweight = self.health_streak > 5 and self.error_count == 0
                health = self.check_gpu_health(lightweight=lightweight)
                
                if health["status"] == "healthy":
                    self.health_streak += 1
                    consecutive_errors = 0
                    
                    # è®°å½•å¥åº·çŠ¶æ€ï¼ˆé™ä½é¢‘ç‡ï¼‰
                    if self.health_streak % 10 == 0:
                        logger.info(f"GPU healthy (streak: {self.health_streak})")
                        
                elif health["status"] == "gsp_error":
                    consecutive_errors += 1
                    self.health_streak = 0
                    
                    # è¿ç»­é”™è¯¯æ—¶å¢åŠ ç­‰å¾…æ—¶é—´
                    if consecutive_errors > 2:
                        logger.warning("Multiple consecutive GSP errors, extended wait...")
                        time.sleep(30)
                        
                else:
                    logger.warning(f"GPU health issue: {health['status']}")
                    consecutive_errors += 1
                    self.health_streak = 0
                
                # æ™ºèƒ½è°ƒæ•´é—´éš”
                self._adaptive_interval()
                
                # å®šæœŸæ¸…ç†
                self._periodic_cleanup()
                
                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                time.sleep(self.current_interval)
                
            except Exception as e:
                logger.error(f"Monitor loop error: {e}")
                consecutive_errors += 1
                time.sleep(min(self.current_interval, 60))  # é”™è¯¯æ—¶æœ€å¤šç­‰å¾…1åˆ†é’Ÿ
    
    def get_status(self) -> Dict:
        """è·å–ç›‘æ§å™¨çŠ¶æ€"""
        return {
            "monitoring": self.monitoring,
            "current_interval": self.current_interval,
            "health_streak": self.health_streak,
            "error_count": self.error_count,
            "keepalive_active": self._keepalive_active,
            "uptime": time.time() - (self.last_cleanup - self.cleanup_interval) if hasattr(self, 'start_time') else 0
        }
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        self.stop_monitoring()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›¡ï¸ Optimized GSP Monitor for Long-term Use")
    print("==========================================")
    
    try:
        # åˆ›å»ºä¼˜åŒ–çš„ç›‘æ§å™¨
        monitor = OptimizedGSPMonitor(device="cuda:0", base_interval=60.0)
        
        # è¿è¡Œåˆå§‹å¥åº·æ£€æŸ¥
        print("\nğŸ“Š Initial GPU Health Check:")
        health = monitor.check_gpu_health(lightweight=False)
        print(f"Status: {health['status']}")
        if health.get('gpu_info'):
            info = health['gpu_info']
            print(f"Temperature: {info.get('temperature', 'N/A')}Â°C")
            print(f"Power: {info.get('power_draw', 'N/A')}W")
            print(f"Memory Used: {info.get('memory_used_mb', 'N/A')}MB")
        
        # å¼€å§‹ç›‘æ§
        monitor.start_monitoring()
        
        print(f"\nğŸ” Optimized monitoring started")
        print("Features:")
        print("- Smart interval adjustment (60s - 300s)")
        print("- Lightweight health checks")
        print("- Automatic resource cleanup")
        print("- Memory leak prevention")
        print("\nPress Ctrl+C to stop...")
        
        try:
            while True:
                time.sleep(10)
                # æ¯10ç§’æ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€
                status = monitor.get_status()
                print(f"Interval: {status['current_interval']:.0f}s, Health streak: {status['health_streak']}, Errors: {status['error_count']}")
        except KeyboardInterrupt:
            print("\nğŸ›‘ Stopping monitor...")
            monitor.stop_monitoring()
            print("âœ… Monitor stopped and cleaned up")
            
    except Exception as e:
        print(f"âŒ Error: {e}")
        return 1
        
    return 0

if __name__ == "__main__":
    sys.exit(main())