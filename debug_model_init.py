#!/usr/bin/env python3
"""
è°ƒè¯•æ¨¡å‹åˆå§‹åŒ–é—®é¢˜
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

import psutil
import gc
import torch

def monitor_memory():
    """ç›‘æ§å†…å­˜ä½¿ç”¨"""
    process = psutil.Process()
    memory_info = process.memory_info()
    virtual_memory = psutil.virtual_memory()

    print(f"è¿›ç¨‹å†…å­˜: {memory_info.rss / (1024**3):.2f} GB")
    print(f"ç³»ç»Ÿå¯ç”¨å†…å­˜: {virtual_memory.available / (1024**3):.2f} GB")

    if torch.cuda.is_available():
        gpu_memory = torch.cuda.memory_allocated() / (1024**3)
        print(f"GPUå†…å­˜: {gpu_memory:.2f} GB")

def test_model_creation_steps():
    """é€æ­¥æµ‹è¯•æ¨¡å‹åˆ›å»ºè¿‡ç¨‹"""

    checkpoints_dir = "/data1/.llama/checkpoints/Llama3.1-70B"

    print("ğŸ” Debug Model Initialization Steps")
    print("=" * 50)

    print("1. Initial memory state:")
    monitor_memory()

    try:
        print("\n2. Loading tokenizer...")
        from transformers import LlamaTokenizerFast
        tokenizer = LlamaTokenizerFast.from_pretrained(checkpoints_dir, legacy=True)
        print("âœ… Tokenizer loaded")
        monitor_memory()

        print("\n3. Loading model config...")
        from llama3.config import ModelArgs
        params_path = Path(checkpoints_dir) / "params.json"

        # åˆ›å»ºä¸€ä¸ªå°çš„æµ‹è¯•é…ç½®æ¥éªŒè¯æµç¨‹
        args = ModelArgs.from_json(
            str(params_path),
            max_seq_len=512,  # å‡å°åºåˆ—é•¿åº¦
            max_batch_size=1, # å‡å°batch size
            device="cpu"      # å…ˆåœ¨CPUä¸Šåˆ›å»º
        )
        print(f"âœ… Config loaded: {args.n_layers} layers, {args.dim} dim")
        monitor_memory()

        print("\n4. Creating model on CPU...")
        from llama3.model import Transformer

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

        print("Memory before model creation:")
        monitor_memory()

        # å°è¯•åˆ›å»ºæ¨¡å‹
        model = Transformer(args)

        print("âœ… Model created on CPU")
        monitor_memory()

        print("\n5. Model structure:")
        total_params = sum(p.numel() for p in model.parameters())
        print(f"Total parameters: {total_params:,} ({total_params * 4 / (1024**3):.2f} GB in fp32)")

        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ç§»åŠ¨åˆ°GPUï¼ˆå°æ‰¹é‡æµ‹è¯•ï¼‰
        if torch.cuda.is_available():
            print("\n6. Testing GPU memory...")
            try:
                # åªç§»åŠ¨ä¸€å°éƒ¨åˆ†æ¥æµ‹è¯•
                model.embed_tokens = model.embed_tokens.cuda()
                print("âœ… Successfully moved embed_tokens to GPU")
                monitor_memory()
            except Exception as e:
                print(f"âŒ GPU test failed: {e}")

        return True

    except Exception as e:
        print(f"âŒ Error at step: {e}")
        import traceback
        traceback.print_exc()
        monitor_memory()
        return False

def test_memory_limits():
    """æµ‹è¯•ä¸åŒçš„å†…å­˜é™åˆ¶"""

    print(f"\nğŸ§ª Testing Memory Limits")
    print("=" * 30)

    # è·å–ç³»ç»Ÿä¿¡æ¯
    virtual_memory = psutil.virtual_memory()
    print(f"System memory: {virtual_memory.total / (1024**3):.1f} GB total")
    print(f"Available memory: {virtual_memory.available / (1024**3):.1f} GB")
    print(f"Memory usage: {virtual_memory.percent}%")

    if torch.cuda.is_available():
        print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")

    # è®¡ç®—LLaMA3-70Bçš„ç†è®ºå†…å­˜éœ€æ±‚
    # 70Bå‚æ•° * 2å­—èŠ‚(fp16) = 140GB
    print(f"\nLLaMA3-70B theoretical memory:")
    print(f"70B params Ã— 2 bytes (fp16) = 140 GB")
    print(f"70B params Ã— 4 bytes (fp32) = 280 GB")

    available_gb = virtual_memory.available / (1024**3)
    if available_gb < 150:
        print(f"âŒ Insufficient memory for full model loading")
        print(f"ğŸ’¡ SSD streaming is essential for this model size")
        return False
    else:
        print(f"âœ… Sufficient memory available")
        return True

if __name__ == "__main__":
    print("ğŸ”§ Model Initialization Debug")
    print("=" * 40)

    # æµ‹è¯•å†…å­˜é™åˆ¶
    memory_ok = test_memory_limits()

    if not memory_ok:
        print(f"\nğŸ’¡ Recommendation:")
        print(f"   - Use SSD streaming with smaller cpu_cache_layers")
        print(f"   - Consider using a smaller model variant")
        print(f"   - Add more system RAM")
    else:
        # æµ‹è¯•æ¨¡å‹åˆ›å»ºæ­¥éª¤
        success = test_model_creation_steps()

        if success:
            print(f"\nâœ… Model initialization debug complete")
        else:
            print(f"\nâŒ Model initialization failed")