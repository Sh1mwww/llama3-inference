#!/usr/bin/env python3
"""
Nsight Systems æ€§èƒ½åˆ†ææµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•IOå’Œcomputeé‡å æ•ˆæœ

ä½¿ç”¨æ–¹æ³•:
nsys profile --trace=cuda,nvtx,osrt --cuda-memory-usage=true \
    --output=weight_streaming_analysis python test_nsight_profiling.py
"""

import torch
import torch.cuda.nvtx as nvtx
import time
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/roger/llama3_project')

from llama3.generator import LlamaGenerator

def test_io_compute_overlap():
    """æµ‹è¯•æƒé‡æµå¼ä¼ è¾“çš„IOå’Œè®¡ç®—é‡å æ•ˆæœ"""
    
    nvtx.range_push("model_initialization")
    print("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ¨¡å‹...")
    
    # æ£€æŸ¥checkpointè·¯å¾„
    ckpt_dir = "/home/roger/llama3_project/checkpoints"  # è¯·æ ¹æ®å®é™…è·¯å¾„ä¿®æ”¹
    if not os.path.exists(ckpt_dir):
        print(f"âŒ Checkpointç›®å½•ä¸å­˜åœ¨: {ckpt_dir}")
        print("è¯·ä¿®æ”¹ckpt_dirä¸ºå®é™…çš„checkpointè·¯å¾„")
        return
    
    try:
        generator = LlamaGenerator.build(
            ckpt_dir=ckpt_dir,
            tokenizer_path=f"{ckpt_dir}/tokenizer.model",
            max_seq_len=1024,
            max_batch_size=1,
            device="cuda:0",
            enable_weight_streaming=True,
            streaming_config={
                "prefetch_distance": 2,
                "max_cached_layers": 4,
                "verbose": True
            }
        )
        print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥checkpointè·¯å¾„å’Œæ–‡ä»¶")
        nvtx.range_pop()  # model_initialization
        return
    
    nvtx.range_pop()  # model_initialization
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "name": "çŸ­æ–‡æœ¬ç”Ÿæˆ",
            "prompt": "The future of artificial intelligence is",
            "max_gen_len": 50
        },
        {
            "name": "æŠ€æœ¯è§£é‡Š",
            "prompt": "Machine learning is a subset of artificial intelligence that",
            "max_gen_len": 80
        },
        {
            "name": "åˆ›æ„å†™ä½œ",
            "prompt": "In a world where robots and humans coexist,",
            "max_gen_len": 100
        }
    ]
    
    print(f"\nğŸ§ª å¼€å§‹æ‰§è¡Œ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹...")
    
    for i, test_case in enumerate(test_cases):
        nvtx.range_push(f"test_case_{i}_{test_case['name']}")
        print(f"\nğŸ“ æµ‹è¯• {i+1}: {test_case['name']}")
        print(f"   æç¤ºè¯: {test_case['prompt']}")
        
        try:
            nvtx.range_push(f"generation_{i}")
            start_time = time.time()
            
            results = generator.text_completion(
                prompts=[test_case['prompt']],
                max_gen_len=test_case['max_gen_len'],
                temperature=0.7,
                top_p=0.9
            )
            
            end_time = time.time()
            nvtx.range_pop()  # generation
            
            # è¾“å‡ºç»“æœ
            generation_time = end_time - start_time
            result_text = results[0] if results else "ç”Ÿæˆå¤±è´¥"
            
            print(f"   â±ï¸  ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’")
            print(f"   ğŸ“„ ç”Ÿæˆç»“æœ: {result_text[:100]}...")
            
            # æ·»åŠ å»¶è¿Ÿç¡®ä¿æµç¨‹æ¸…æ™°å¯è§
            nvtx.range_push(f"cooldown_{i}")
            time.sleep(0.5)
            nvtx.range_pop()  # cooldown
            
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        nvtx.range_pop()  # test_case
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")

def test_baseline_comparison():
    """å¯¹æ¯”æµ‹è¯•ï¼šå¸¦æƒé‡æµå¼ä¼ è¾“ vs ä¸å¸¦æƒé‡æµå¼ä¼ è¾“"""
    
    print("\nğŸ”¬ å¼€å§‹åŸºå‡†å¯¹æ¯”æµ‹è¯•...")
    test_prompt = "Artificial intelligence will transform"
    
    # æµ‹è¯•1: ä¸å¯ç”¨æƒé‡æµå¼ä¼ è¾“
    nvtx.range_push("baseline_test")
    print("ğŸ“Š åŸºå‡†æµ‹è¯• (æ— æƒé‡æµå¼ä¼ è¾“)")
    
    try:
        generator_baseline = LlamaGenerator.build(
            ckpt_dir="/home/roger/llama3_project/checkpoints",
            tokenizer_path="/home/roger/llama3_project/checkpoints/tokenizer.model",
            max_seq_len=512,
            max_batch_size=1,
            device="cuda:0",
            enable_weight_streaming=False  # å…³é—­æƒé‡æµå¼ä¼ è¾“
        )
        
        start_time = time.time()
        results_baseline = generator_baseline.text_completion(
            prompts=[test_prompt],
            max_gen_len=50,
            temperature=0.7
        )
        baseline_time = time.time() - start_time
        
        print(f"   â±ï¸  åŸºå‡†æµ‹è¯•æ—¶é—´: {baseline_time:.2f}ç§’")
        
    except Exception as e:
        print(f"   âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        baseline_time = 0
    
    nvtx.range_pop()  # baseline_test
    
    # æµ‹è¯•2: å¯ç”¨æƒé‡æµå¼ä¼ è¾“
    nvtx.range_push("streaming_test")
    print("ğŸš€ æµå¼ä¼ è¾“æµ‹è¯• (å¯ç”¨æƒé‡æµå¼ä¼ è¾“)")
    
    try:
        generator_streaming = LlamaGenerator.build(
            ckpt_dir="/home/roger/llama3_project/checkpoints",
            tokenizer_path="/home/roger/llama3_project/checkpoints/tokenizer.model",
            max_seq_len=512,
            max_batch_size=1,
            device="cuda:0",
            enable_weight_streaming=True,
            streaming_config={
                "prefetch_distance": 2,
                "max_cached_layers": 4,
                "verbose": True
            }
        )
        
        start_time = time.time()
        results_streaming = generator_streaming.text_completion(
            prompts=[test_prompt],
            max_gen_len=50,
            temperature=0.7
        )
        streaming_time = time.time() - start_time
        
        print(f"   â±ï¸  æµå¼ä¼ è¾“æ—¶é—´: {streaming_time:.2f}ç§’")
        
        # è®¡ç®—æ€§èƒ½æ”¹è¿›
        if baseline_time > 0:
            improvement = ((baseline_time - streaming_time) / baseline_time) * 100
            print(f"   ğŸ“ˆ æ€§èƒ½æå‡: {improvement:.1f}%")
        
    except Exception as e:
        print(f"   âŒ æµå¼ä¼ è¾“æµ‹è¯•å¤±è´¥: {e}")
    
    nvtx.range_pop()  # streaming_test

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    nvtx.range_push("main_test_suite")
    
    print("ğŸ¯ Nsight Systems æ€§èƒ½åˆ†ææµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œæµ‹è¯•")
        return
    
    print(f"ğŸ”§ CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
    print(f"ğŸ”§ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # æ‰§è¡Œä¸»è¦æµ‹è¯•
    test_io_compute_overlap()
    
    # æ‰§è¡Œå¯¹æ¯”æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
    # test_baseline_comparison()
    
    nvtx.range_pop()  # main_test_suite
    print("\nğŸ‰ æµ‹è¯•å¥—ä»¶å®Œæˆï¼")
    print("\nğŸ“Š Nsightåˆ†æå»ºè®®:")
    print("1. æŸ¥çœ‹Timelineä¸­çš„NVTX ranges")
    print("2. æ£€æŸ¥weight_h2d streamä¸default streamçš„é‡å ")
    print("3. å…³æ³¨prefetchæ“ä½œçš„æ—¶æœº")
    print("4. åˆ†æGPUåˆ©ç”¨ç‡å’Œå†…å­˜å¸¦å®½")

if __name__ == "__main__":
    main()