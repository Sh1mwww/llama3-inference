#!/usr/bin/env python3
"""
çœŸå®inferenceçš„NVTXæ€§èƒ½åˆ†æè„šæœ¬
ä½¿ç”¨å®é™…çš„LLaMAæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œå¹¶åˆ†ææƒé‡æµå¼ä¼ è¾“çš„æ€§èƒ½

ä½¿ç”¨æ–¹æ³•:
nsys profile --trace=cuda,nvtx --output=real_inference python test_real_inference_nvtx.py
"""

import torch
import time
import os
import sys
from pathlib import Path
import argparse

# NVTX profiling support
try:
    import torch.cuda.nvtx as nvtx
    NVTX_AVAILABLE = True
    print("âœ… NVTXå¯ç”¨")
except ImportError:
    print("âŒ NVTXä¸å¯ç”¨ï¼Œä½¿ç”¨fallback")
    # Fallback no-op functions
    class nvtx:
        @staticmethod
        def range_push(name): pass
        @staticmethod
        def range_pop(): pass
    NVTX_AVAILABLE = False

def get_model_path():
    """è·å–æ¨¡å‹è·¯å¾„"""
    # å¯èƒ½çš„æ¨¡å‹è·¯å¾„
    possible_paths = [
        "/home/roger/llama3_project/checkpoints",
        "./checkpoints",
        "../checkpoints",
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ–‡ä»¶
            path_obj = Path(path)
            if (path_obj / "params.json").exists():
                return str(path_obj)
    
    return None

def run_real_inference_with_nvtx(model_path, prompts, enable_streaming=True):
    """è¿è¡Œå¸¦NVTXæ ‡è®°çš„çœŸå®inference"""
    
    nvtx.range_push("real_inference_session")
    
    print(f"ğŸ¯ å¼€å§‹çœŸå®inferenceåˆ†æ")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸ”§ æƒé‡æµå¼ä¼ è¾“: {'å¯ç”¨' if enable_streaming else 'ç¦ç”¨'}")
    print(f"ğŸ“ Promptæ•°é‡: {len(prompts)}")
    
    # 1. æ¨¡å‹åŠ è½½é˜¶æ®µ
    nvtx.range_push("model_loading")
    print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    try:
        from llama3.generator import LLaMA
        
        nvtx.range_push("llama_build")
        llama = LLaMA.build(
            model_path,
            load_model=True,
            device="cuda" if torch.cuda.is_available() else "cpu",
            enable_weight_streaming=enable_streaming,
            streaming_config={
                'prefetch_distance': 2,
                'max_cached_layers': 4,
                'warmup_layers': 1,
                'verbose': True
            } if enable_streaming else None,
            max_seq_len=1024,  # è¾ƒå°çš„åºåˆ—é•¿åº¦ç”¨äºæµ‹è¯•
            max_batch_size=8
        )
        nvtx.range_pop()  # llama_build
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        nvtx.range_pop()  # model_loading
        nvtx.range_pop()  # real_inference_session
        return None
    
    nvtx.range_pop()  # model_loading
    
    # 2. æ¨ç†é˜¶æ®µ
    nvtx.range_push("inference_execution")
    
    results = []
    
    for i, prompt in enumerate(prompts):
        nvtx.range_push(f"prompt_{i}_inference")
        
        print(f"ğŸ”„ å¤„ç†Prompt {i+1}/{len(prompts)}: {prompt[:50]}...")
        
        try:
            nvtx.range_push(f"prompt_{i}_text_completion")
            
            start_time = time.time()
            _, completions = llama.text_completion(
                [prompt],
                max_gen_len=32,  # è¾ƒçŸ­çš„ç”Ÿæˆé•¿åº¦ç”¨äºå¿«é€Ÿæµ‹è¯•
                temperature=0.6,
                top_p=0.9
            )
            inference_time = time.time() - start_time
            
            nvtx.range_pop()  # text_completion
            
            completion = completions[0] if completions else ""
            results.append({
                'prompt': prompt,
                'completion': completion,
                'time': inference_time
            })
            
            print(f"âœ… Prompt {i+1} å®Œæˆ ({inference_time:.2f}s)")
            print(f"   ç»“æœ: {completion[:100]}...")
            
        except Exception as e:
            print(f"âŒ Prompt {i+1} æ¨ç†å¤±è´¥: {e}")
            results.append({
                'prompt': prompt,
                'completion': f"ERROR: {e}",
                'time': 0.0
            })
        
        nvtx.range_pop()  # prompt_i_inference
    
    nvtx.range_pop()  # inference_execution
    nvtx.range_pop()  # real_inference_session
    
    return results

def analyze_streaming_performance():
    """åˆ†ææƒé‡æµå¼ä¼ è¾“æ€§èƒ½"""
    
    nvtx.range_push("streaming_performance_analysis")
    
    print("\nğŸ” æƒé‡æµå¼ä¼ è¾“æ€§èƒ½åˆ†æ")
    print("=" * 50)
    
    # æµ‹è¯•Promptåˆ—è¡¨
    test_prompts = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "è¯·è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†ã€‚",
        "å¦‚ä½•ä¼˜åŒ–ç¥ç»ç½‘ç»œçš„æ€§èƒ½ï¼Ÿ"
    ]
    
    model_path = get_model_path()
    if not model_path:
        print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®")
        nvtx.range_pop()  # streaming_performance_analysis
        return
    
    # è¿è¡Œå¸¦æƒé‡æµå¼ä¼ è¾“çš„inference
    nvtx.range_push("streaming_enabled_test")
    print("\nğŸ”§ æµ‹è¯•1: å¯ç”¨æƒé‡æµå¼ä¼ è¾“")
    streaming_results = run_real_inference_with_nvtx(
        model_path, test_prompts, enable_streaming=True
    )
    nvtx.range_pop()  # streaming_enabled_test
    
    if streaming_results:
        total_time = sum(r['time'] for r in streaming_results)
        print(f"âœ… æƒé‡æµå¼ä¼ è¾“æµ‹è¯•å®Œæˆï¼Œæ€»æ—¶é—´: {total_time:.2f}s")
        
        # æ˜¾ç¤ºç»“æœ
        for i, result in enumerate(streaming_results):
            print(f"\nğŸ“ Prompt {i+1}: {result['prompt'][:50]}...")
            print(f"ğŸ¤– å›ç­”: {result['completion'][:100]}...")
            print(f"â±ï¸  æ—¶é—´: {result['time']:.2f}s")
    
    nvtx.range_pop()  # streaming_performance_analysis

def simulate_layer_processing():
    """æ¨¡æ‹Ÿå®é™…çš„å±‚å¤„ç†è¿‡ç¨‹ï¼ˆç”¨äºæ— æ¨¡å‹çš„æµ‹è¯•ï¼‰"""
    
    nvtx.range_push("layer_processing_simulation")
    
    print("\nğŸ­ æ¨¡æ‹Ÿå±‚å¤„ç†è¿‡ç¨‹")
    print("=" * 30)
    
    num_layers = 8
    seq_len = 64
    
    for token_pos in range(seq_len):
        nvtx.range_push(f"token_{token_pos}_processing")
        
        for layer_id in range(num_layers):
            nvtx.range_push(f"layer_{layer_id}_token_{token_pos}")
            
            # æ¨¡æ‹Ÿæƒé‡ç¡®ä¿åœ¨GPU
            nvtx.range_push(f"ensure_layer_{layer_id}_weights")
            time.sleep(0.001)  # æ¨¡æ‹Ÿæƒé‡åŠ è½½æ—¶é—´
            nvtx.range_pop()  # ensure_weights
            
            # æ¨¡æ‹Ÿå®é™…è®¡ç®—
            nvtx.range_push(f"layer_{layer_id}_computation")
            if torch.cuda.is_available():
                # å®é™…çš„CUDAæ“ä½œ
                a = torch.randn(512, 512, device="cuda")
                b = torch.randn(512, 512, device="cuda") 
                c = torch.matmul(a, b)
                torch.cuda.synchronize()
            else:
                time.sleep(0.002)
            nvtx.range_pop()  # computation
            
            nvtx.range_pop()  # layer_processing
        
        nvtx.range_pop()  # token_processing
        
        # æ¯éš”å‡ ä¸ªtokenæ‰“å°è¿›åº¦
        if token_pos % 10 == 0:
            print(f"ğŸ”„ å¤„ç†token {token_pos}/{seq_len}")
    
    nvtx.range_pop()  # layer_processing_simulation

def main():
    """ä¸»å‡½æ•°"""
    
    parser = argparse.ArgumentParser(description="çœŸå®inferenceçš„NVTXæ€§èƒ½åˆ†æ")
    parser.add_argument("--mode", choices=["full", "simulation"], default="simulation",
                       help="è¿è¡Œæ¨¡å¼: full(å®Œæ•´æ¨¡å‹) æˆ– simulation(æ¨¡æ‹Ÿ)")
    parser.add_argument("--model-path", help="æ¨¡å‹è·¯å¾„ï¼ˆè¦†ç›–è‡ªåŠ¨æ£€æµ‹ï¼‰")
    
    args = parser.parse_args()
    
    print("ğŸ¯ çœŸå®inferenceçš„NVTXæ€§èƒ½åˆ†æ")
    print("=" * 50)
    
    if torch.cuda.is_available():
        print(f"ğŸ”§ ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
    else:
        print("ğŸ”§ ä½¿ç”¨CPUæ¨¡æ‹Ÿ")
    
    if args.mode == "full":
        # å®Œæ•´æ¨¡å‹æµ‹è¯•
        if args.model_path:
            model_path = args.model_path
        else:
            model_path = get_model_path()
        
        if model_path:
            analyze_streaming_performance()
        else:
            print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹ï¼Œåˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
            simulate_layer_processing()
    else:
        # æ¨¡æ‹Ÿæ¨¡å¼
        simulate_layer_processing()
    
    print("\nâœ… åˆ†æå®Œæˆ!")
    print("\nğŸ“Š ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ç»“æœ:")
    print("nsys profile --trace=cuda,nvtx --output=real_inference python test_real_inference_nvtx.py")
    print("nsys stats --report nvtxsum real_inference.nsys-rep")
    print("python analyze_nsys_report.py real_inference.nsys-rep")

if __name__ == "__main__":
    main()