#!/usr/bin/env python3
"""
ä»raw block deviceåŠ è½½å•å±‚æƒé‡åˆ°pinned memoryçš„ç¤ºä¾‹
"""

import sys
import json
import torch
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

from llama3.weights_io_ssd_dram import DirectIOFile, alloc_pinned_aligned, DTYPE_MAP


def load_single_layer_from_ssd(manifest_path: str, layer_id: int, verbose: bool = True):
    """
    ä»SSDåŠ è½½æŒ‡å®šå±‚çš„æ‰€æœ‰æƒé‡åˆ°pinned memory

    Args:
        manifest_path: runtime manifestæ–‡ä»¶è·¯å¾„
        layer_id: è¦åŠ è½½çš„å±‚ID (0-79 for LLaMA3-70B)
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        dict: {param_name: torch.Tensor} åŒ…å«è¯¥å±‚æ‰€æœ‰æƒé‡çš„å­—å…¸
    """

    if verbose:
        print(f"ğŸ”„ Loading layer {layer_id} from SSD...")

    # 1. åŠ è½½manifest
    if not Path(manifest_path).exists():
        raise FileNotFoundError(f"Manifest not found: {manifest_path}")

    with open(manifest_path, 'r') as f:
        manifest = json.load(f)

    if verbose:
        print(f"âœ… Manifest loaded: {manifest['raw_device']}")

    # 2. æ‰“å¼€SSDè®¾å¤‡
    dio = DirectIOFile(
        manifest["raw_device"],
        mode="r",
        block_size=manifest["block_size"]
    )

    # 3. æ‰¾åˆ°æŒ‡å®šå±‚çš„æ‰€æœ‰å‚æ•°
    layer_params = []
    for param in manifest["params"]:
        if param["layer"] == layer_id and param["policy"] == "stream":
            layer_params.append(param)

    if not layer_params:
        dio.close()
        raise ValueError(f"No stream parameters found for layer {layer_id}")

    if verbose:
        print(f"ğŸ“‹ Found {len(layer_params)} parameters for layer {layer_id}")

    # 4. è®¡ç®—æœ€å¤§staging bufferéœ€æ±‚
    max_stride = max(p["stride"] for p in layer_params)
    staging_buffer = alloc_pinned_aligned(max_stride, manifest["block_size"])

    if verbose:
        print(f"ğŸ’¾ Allocated {max_stride} byte staging buffer")

    # 5. é€ä¸ªåŠ è½½å‚æ•°
    layer_weights = {}

    for param_info in layer_params:
        param_name = param_info["name"]
        stride = param_info["stride"]
        offset = param_info["offset"]
        nbytes = param_info["nbytes"]
        shape = param_info["shape"]
        dtype_str = param_info["dtype"]

        if verbose:
            print(f"   Loading {param_name}: {shape} ({nbytes} bytes)")

        # æ‰©å±•staging bufferå¦‚æœéœ€è¦
        if stride > len(staging_buffer):
            staging_buffer = alloc_pinned_aligned(stride, manifest["block_size"])
            if verbose:
                print(f"   Expanded staging buffer to {stride} bytes")

        # ä»SSDè¯»å–åˆ°staging buffer
        dio.pread_into_tensor(staging_buffer, stride, offset)

        # åˆ›å»ºç›®æ ‡tensor (pinned)
        param_tensor = torch.empty(
            shape,
            dtype=DTYPE_MAP[dtype_str],
            pin_memory=True
        )

        # å¤åˆ¶æ•°æ®
        param_tensor.view(-1).view(torch.uint8)[:nbytes].copy_(
            staging_buffer[:nbytes]
        )

        layer_weights[param_name] = param_tensor

        if verbose:
            print(f"   âœ… {param_name} loaded to pinned memory")

    # 6. æ¸…ç†
    dio.close()

    if verbose:
        total_mb = sum(t.numel() * t.element_size() for t in layer_weights.values()) / (1024**2)
        print(f"âœ… Layer {layer_id} loaded: {len(layer_weights)} params, {total_mb:.1f} MB")

    return layer_weights


def load_specific_parameter(manifest_path: str, param_name: str, verbose: bool = True):
    """
    ä»SSDåŠ è½½æŒ‡å®šçš„å•ä¸ªå‚æ•°

    Args:
        manifest_path: runtime manifestæ–‡ä»¶è·¯å¾„
        param_name: å‚æ•°åç§° (å¦‚ "layers.0.attention.wq.weight")
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        torch.Tensor: åŠ è½½çš„å‚æ•°tensor (pinned memory)
    """

    if verbose:
        print(f"ğŸ”„ Loading parameter: {param_name}")

    # 1. åŠ è½½manifest
    with open(manifest_path, 'r') as f:
        manifest = json.load(f)

    # 2. æ‰¾åˆ°æŒ‡å®šå‚æ•°
    param_info = None
    for param in manifest["params"]:
        if param["name"] == param_name:
            param_info = param
            break

    if param_info is None:
        raise ValueError(f"Parameter {param_name} not found in manifest")

    if verbose:
        print(f"ğŸ“‹ Found parameter: {param_info['shape']} ({param_info['nbytes']} bytes)")

    # 3. æ‰“å¼€SSDè®¾å¤‡
    dio = DirectIOFile(
        manifest["raw_device"],
        mode="r",
        block_size=manifest["block_size"]
    )

    # 4. åˆ†é…staging buffer
    stride = param_info["stride"]
    staging_buffer = alloc_pinned_aligned(stride, manifest["block_size"])

    # 5. è¯»å–å‚æ•°
    dio.pread_into_tensor(staging_buffer, stride, param_info["offset"])

    # 6. åˆ›å»ºç›®æ ‡tensor
    param_tensor = torch.empty(
        param_info["shape"],
        dtype=DTYPE_MAP[param_info["dtype"]],
        pin_memory=True
    )

    # 7. å¤åˆ¶æ•°æ®
    param_tensor.view(-1).view(torch.uint8)[:param_info["nbytes"]].copy_(
        staging_buffer[:param_info["nbytes"]]
    )

    # 8. æ¸…ç†
    dio.close()

    if verbose:
        print(f"âœ… Parameter loaded to pinned memory: {param_tensor.shape}")

    return param_tensor


def batch_load_layers(manifest_path: str, layer_ids: list, verbose: bool = True):
    """
    æ‰¹é‡åŠ è½½å¤šä¸ªå±‚çš„æƒé‡

    Args:
        manifest_path: runtime manifestæ–‡ä»¶è·¯å¾„
        layer_ids: å±‚IDåˆ—è¡¨
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        dict: {layer_id: {param_name: torch.Tensor}}
    """

    if verbose:
        print(f"ğŸ”„ Batch loading layers: {layer_ids}")

    all_layers = {}

    for layer_id in layer_ids:
        try:
            layer_weights = load_single_layer_from_ssd(manifest_path, layer_id, verbose=False)
            all_layers[layer_id] = layer_weights

            if verbose:
                total_mb = sum(t.numel() * t.element_size() for t in layer_weights.values()) / (1024**2)
                print(f"âœ… Layer {layer_id}: {len(layer_weights)} params, {total_mb:.1f} MB")

        except Exception as e:
            if verbose:
                print(f"âŒ Failed to load layer {layer_id}: {e}")
            continue

    if verbose:
        total_layers = len(all_layers)
        total_mb = sum(
            sum(t.numel() * t.element_size() for t in layer_weights.values())
            for layer_weights in all_layers.values()
        ) / (1024**2)
        print(f"ğŸ‰ Batch load complete: {total_layers} layers, {total_mb:.1f} MB total")

    return all_layers


def demonstrate_usage():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è¿™äº›å‡½æ•°"""

    manifest_path = "/data1/llama-70b.runtime_manifest.json"

    print("ğŸš€ SSD Layer Loading Demonstration")
    print("=" * 50)

    try:
        # ç¤ºä¾‹1: åŠ è½½å•ä¸ªå±‚
        print("\n1. åŠ è½½å•ä¸ªå±‚ (Layer 0)")
        print("-" * 30)
        layer_0_weights = load_single_layer_from_ssd(manifest_path, 0)

        print(f"\nLayer 0 å‚æ•°åˆ—è¡¨:")
        for name, tensor in layer_0_weights.items():
            print(f"  {name}: {tensor.shape} {tensor.dtype} (pinned: {tensor.is_pinned()})")

        # ç¤ºä¾‹2: åŠ è½½ç‰¹å®šå‚æ•°
        print("\n\n2. åŠ è½½ç‰¹å®šå‚æ•°")
        print("-" * 30)
        if layer_0_weights:
            first_param_name = list(layer_0_weights.keys())[0]
            param_tensor = load_specific_parameter(manifest_path, first_param_name)
            print(f"å‚æ•°è¯¦æƒ…: {param_tensor.shape}, pinned: {param_tensor.is_pinned()}")

        # ç¤ºä¾‹3: æ‰¹é‡åŠ è½½å¤šå±‚
        print("\n\n3. æ‰¹é‡åŠ è½½å¤šå±‚ (Layer 0-2)")
        print("-" * 30)
        multi_layers = batch_load_layers(manifest_path, [0, 1, 2])

        # ç¤ºä¾‹4: è½¬ç§»åˆ°GPU
        if torch.cuda.is_available():
            print("\n\n4. è½¬ç§»åˆ°GPU")
            print("-" * 30)
            first_layer = list(multi_layers.values())[0]
            first_param = list(first_layer.values())[0]

            print(f"åŸå§‹: {first_param.device}")
            gpu_param = first_param.cuda(non_blocking=True)
            print(f"GPU: {gpu_param.device}")
            print("âœ… éé˜»å¡ä¼ è¾“æˆåŠŸ")

        print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼æ‰€æœ‰æƒé‡éƒ½åœ¨pinned memoryä¸­ï¼Œå¯ä»¥é«˜æ•ˆä¼ è¾“åˆ°GPU")

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("ğŸ”§ Raw Block Device Layer Loading")
    print("=" * 40)

    # è¿è¡Œæ¼”ç¤º
    demonstrate_usage()

    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•æ€»ç»“:")
    print(f"1. load_single_layer_from_ssd(manifest_path, layer_id)")
    print(f"2. load_specific_parameter(manifest_path, param_name)")
    print(f"3. batch_load_layers(manifest_path, layer_ids)")
    print(f"4. æ‰€æœ‰è¿”å›çš„tensoréƒ½æ˜¯pinned memoryï¼Œå¯ä»¥non_blockingä¼ è¾“åˆ°GPU")