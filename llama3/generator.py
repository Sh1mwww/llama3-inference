import json
import os
import time
from pathlib import Path
from typing import List, Optional

import torch
from tqdm import tqdm
from transformers import LlamaTokenizerFast,AutoTokenizer

from .config import ModelArgs
from .model import Transformer

# NVTX profiling support
try:
    import torch.cuda.nvtx as nvtx
    NVTX_AVAILABLE = True
except ImportError:
    NVTX_AVAILABLE = False
    # Fallback no-op functions
    class nvtx:
        @staticmethod
        def range_push(name): pass
        @staticmethod
        def range_pop(): pass

# tokenizer, checkpoint, args
class LLaMA:
    def __init__(self, tokenizer, checkpoint, args: ModelArgs):
        self.tokenizer = tokenizer
        self.args = args
        
        # åˆå§‹åŒ–å…¨å±€çŠ¶æ€è·Ÿè¸ªå™¨
        from .global_state_tracker import init_global_tracker, get_global_tracker
        from .kv_offload import BLOCK
        if get_global_tracker() is None:
            print(f"[INFO] Initializing global state tracker...")
            n_blocks = (args.max_seq_len + BLOCK - 1) // BLOCK  # è®¡ç®—éœ€è¦çš„blockæ•°é‡
            tracker = init_global_tracker(
                max_batch=args.max_batch_size,
                layers=args.n_layers,
                n_blocks=n_blocks
            )
            # ä¸è®¾ç½®é»˜è®¤çš„future batchesï¼Œç­‰å¾…å®é™…ä½¿ç”¨æ—¶å†è®¾ç½®
            print(f"[INFO] Global state tracker initialized, waiting for actual batch registration")
        
        print(f"[INFO] Initializing model on device: {args.device}")
        self.model = Transformer(args)
        
        print(f"[INFO] Moving model to {args.device}...")
        self.model = self.model.to(args.device)
        
        print(f"[INFO] Converting to half precision...")
        self.model = self.model.half()
        
        if checkpoint is not None:
            print(f"[INFO] Loading state dict...")
            missing_keys, unexpected_keys = self.model.load_state_dict(checkpoint, strict=False)
            if missing_keys:
                print(f"[WARNING] Missing keys: {len(missing_keys)} keys")
            if unexpected_keys:
                print(f"[WARNING] Unexpected keys: {len(unexpected_keys)} keys")
            print(f"[INFO] Model weights loaded successfully")
    
    def _configure_weight_streaming(self, streaming_config: dict):
        """é…ç½®æƒé‡æµå¼ä¼ è¾“"""
        print("ğŸ”§ Configuring Weight Streaming...")
        
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from .weight_streaming_manager import WeightStreamingManager
        from . import layers
        from . import stream_mnt
        
        # è®¾ç½®é»˜è®¤é…ç½®
        config = {
            'prefetch_distance': 1,
            'max_cached_layers': 4,
            'warmup_layers': 1,
            'verbose': False
        }
        config.update(streaming_config)
        
        # ç¡®ä¿æ¨¡å‹çš„ layers å±æ€§å¯è®¿é—®ï¼ˆä¾› WSM ä½¿ç”¨ï¼‰
        if hasattr(self.model, "layer_infos"):
            try:
                blocks = [info.block for info in self.model.layer_infos if info.block is not None]
                if blocks and not hasattr(self.model, "layers"):
                    self.model.layers = blocks
            except Exception:
                pass
        
        # é…ç½®æ ¸å¿ƒç»„ä»¶åˆ°ç›®æ ‡è®¾å¤‡ï¼ˆå°æ¨¡å—å¸¸é©» HBMï¼‰
        self._configure_core_components()
        
        # åˆ›å»ºå’Œé…ç½® WSM
        wsm = WeightStreamingManager(
            self.model, 
            device=self.args.device,
            prefetch_distance=config['prefetch_distance'],
            max_cached_layers=config['max_cached_layers'],
            warmup_layers=config['warmup_layers'],
            verbose=True  # å¼ºåˆ¶å¯ç”¨è¯¦ç»†æ—¥å¿—ä»¥ä¾¿éªŒè¯
        )
        
        # é›†æˆ WSM åˆ°æ¨¡å‹å±‚
        self._integrate_wsm_to_layers(wsm)
        
        # é…ç½® KV streams
        self._configure_kv_streams()
        
        # éªŒè¯å¹¶ä¿®å¤è®¾å¤‡æ”¾ç½®
        self._verify_and_fix_device_placement()
        
        # è¾“å‡ºå…³é”®çš„è¯Šæ–­ä¿¡æ¯
        try:
            first_blk = getattr(self.model, "layers", [None])[0]
            if first_blk is not None:
                print("[CHECK] first block param device:", next(first_blk.parameters()).device)
        except Exception:
            pass
        
        print("âœ… Weight streaming enabled (activations on GPU, weights streamed per-layer).")
        print("âš™ï¸  Running on GPU")
        
        return wsm
    
    def _configure_core_components(self):
        """é…ç½®æ ¸å¿ƒç»„ä»¶åˆ°ç›®æ ‡è®¾å¤‡"""
        device = self.args.device
        model = self.model
        
        # å°æ¨¡å—å¸¸é©» HBM
        model.embed_tokens = model.embed_tokens.to(device)
        model.norm = model.norm.to(device)
        model.output = model.output.to(device)
        
        # å¤„ç† freqs_complex
        self._handle_freqs_complex(device)
    
    def _handle_freqs_complex(self, device: str):
        """å¤„ç† freqs_complex çš„è®¾å¤‡æ”¾ç½®ä¸é‡å»º"""
        model = self.model
        
        if hasattr(model, "freqs_complex"):
            try:
                model.freqs_complex = model.freqs_complex.to(device)
            except Exception as e:
                print(f"âš ï¸ Warning: Failed to move freqs_complex to {device}: {e}")
                # é‡æ–°åˆ›å»º freqs_complex åœ¨ç›®æ ‡è®¾å¤‡ä¸Š
                self._recreate_freqs_complex(device)
    
    def _recreate_freqs_complex(self, device: str):
        """é‡æ–°åˆ›å»º freqs_complex åœ¨ç›®æ ‡è®¾å¤‡ä¸Š"""
        try:
            from .layers import precompute_theta_pos_frequencies
            print(f"   Attempting to recreate freqs_complex on {device}...")
            
            # ä½¿ç”¨ ModelArgs ä¸­çš„é…ç½®
            dim = self.args.dim
            n_heads = self.args.n_heads
            max_seq_len = self.args.max_seq_len
            rope_theta = self.args.rope_theta
            
            self.model.freqs_complex = precompute_theta_pos_frequencies(
                dim // n_heads,
                max_seq_len * 2,
                device=device,
                theta=rope_theta,
            )
            print(f"   Successfully recreated freqs_complex on {device}")
        except Exception as e:
            print(f"   Failed to recreate freqs_complex: {e}")
            raise RuntimeError(f"Cannot ensure freqs_complex is on {device}") from e
    
    def _integrate_wsm_to_layers(self, wsm):
        """å°† WSM é›†æˆåˆ°æ¨¡å‹å±‚"""
        try:
            from . import layers
            layers.set_weight_manager(wsm)  # è®¾ç½®å…¨å±€å¼•ç”¨
            
            # ä¸ºç°æœ‰å±‚æ‰‹åŠ¨æ³¨å…¥
            if hasattr(self.model, "layers"):
                for i, layer in enumerate(self.model.layers):
                    if hasattr(layer, "attention"):
                        layer.attention.weight_manager = wsm
                        layer.attention.layer_id = getattr(layer, "layer_id", i)
                    if hasattr(layer, "feed_forward"):
                        layer.feed_forward.weight_manager = wsm
                        layer.feed_forward.layer_id = getattr(layer, "layer_id", i)
        except Exception as e:
            print(f"[WARN] failed to set_weight_manager on blocks: {e}")
    
    def _configure_kv_streams(self):
        """é…ç½® KV streams"""
        try:
            from . import stream_mnt
            streams = stream_mnt.get_streams(self.args.device)
            
            if hasattr(self.model, "layers"):
                for layer in self.model.layers:
                    if hasattr(layer, "attention"):
                        off = getattr(layer.attention, "offloader", None)
                        if off is not None:
                            off.h2d_stream = streams.kv_h2d
                            off.d2h_stream = streams.kv_d2h
        except Exception as e:
            print(f"[WARN] failed to configure KV streams: {e}")
    
    def _verify_and_fix_device_placement(self):
        """éªŒè¯å¹¶ä¿®å¤è®¾å¤‡æ”¾ç½®"""
        device = self.args.device
        model = self.model
        
        if not device.startswith("cuda"):
            return
            
        print("ğŸ” Verifying device placement before inference...")
        
        try:
            # å¼ºåˆ¶åŒæ­¥æ‰€æœ‰æ ¸å¿ƒç»„ä»¶åˆ°ç›®æ ‡è®¾å¤‡
            print("ğŸ”§ Synchronizing all components to target device...")
            model.embed_tokens = model.embed_tokens.to(device)
            model.norm = model.norm.to(device)
            model.output = model.output.to(device)
            if hasattr(model, 'freqs_complex'):
                model.freqs_complex = model.freqs_complex.to(device)
            
            # åŒæ­¥æ‰€æœ‰å±‚çš„ norm æƒé‡åˆ° GPU
            print("ğŸ”§ Synchronizing layer norms to GPU...")
            if hasattr(model, "layers"):
                for layer in model.layers:
                    if hasattr(layer, 'attn_norm'):
                        layer.attn_norm = layer.attn_norm.to(device)
                    if hasattr(layer, 'ffn_norm'):
                        layer.ffn_norm = layer.ffn_norm.to(device)
            
            # GPU åŒæ­¥ç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆ
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            
            print("âœ… All layer components synchronized to target device")
                
        except Exception as e:
            print(f"âš ï¸ Error during device synchronization: {e}")

    # ---------- æ„å»º ----------
    @staticmethod
    def build(
        checkpoints_dir: str,
        load_model: bool = True,
        device: str = "cuda",
        enable_weight_streaming: bool = False,
        streaming_config: Optional[dict] = None,
        topk_blk: Optional[int] = None,
        max_seq_len: int = 2048,
        max_batch_size: int = 512,
    ) -> "LLaMA":
        ckpt_dir = Path(checkpoints_dir)
        tokenizer = LlamaTokenizerFast.from_pretrained(pretrained_model_name_or_path=ckpt_dir/tokenizer.model, legacy=True)
        params_path = ckpt_dir / "params.json"
        args = ModelArgs.from_json(
            str(params_path), max_seq_len=max_seq_len, max_batch_size=max_batch_size, device=device
        )
        
        # å¦‚æœæŒ‡å®šäº† topk_blkï¼Œæ›´æ–°åˆ° ModelArgs ä¸­
        if topk_blk is not None:
            args.topk_blk = topk_blk
        
        args.checkpoints_dir = str(ckpt_dir)

        checkpoint = None
        if load_model:
            ckpt_file = sorted(ckpt_dir.glob("*.pth"))[0]
            print(f"[INFO] Loading checkpoint: {ckpt_file}")
            t0 = time.time()
            checkpoint = torch.load(ckpt_file, map_location="cpu")
            print(f"[INFO] Done ({time.time() - t0:.1f}s)")

        # å…ˆåœ¨ CPU ä¸Šåˆ›å»º LLaMA å®ä¾‹ï¼ˆé¿å… OOMï¼‰
        cpu_args = ModelArgs.from_json(
            str(params_path), max_seq_len=max_seq_len, max_batch_size=max_batch_size, device="cpu"
        )
        if topk_blk is not None:
            cpu_args.topk_blk = topk_blk
        cpu_args.checkpoints_dir = str(ckpt_dir)
        
        llama = LLaMA(tokenizer, checkpoint, cpu_args)
        
        # å¦‚æœå¯ç”¨æƒé‡æµå¼ä¼ è¾“ä¸”è®¾å¤‡æ˜¯ CUDA
        if enable_weight_streaming and device.startswith("cuda"):
            llama._configure_weight_streaming(streaming_config or {})
        elif device.startswith("cuda"):
            # éæµå¼ä¼ è¾“æ¨¡å¼ï¼šç›´æ¥ç§»åŠ¨åˆ° GPU
            try:
                llama.model = llama.model.to(device)
                llama.args.device = device
            except torch.cuda.OutOfMemoryError:
                print("âŒ CUDA OOM when moving model. Keeping on CPU...")
                device = "cpu"
                llama.args.device = "cpu"
        
        return llama

    # ---------- æ¨ç† ----------
    def text_completion(
        self,
        prompts: List[str],
        temperature: float = 0.6,
        top_p: float = 0.9,
        max_gen_len: Optional[int] = None,
        profile_output_dir: Optional[str] = None,
        batch_size: int = 4,  
    ):
        nvtx.range_push("text_completion")
        
        num_batches = (len(prompts) + batch_size - 1) // batch_size
        
        # register future batches in the global state tracker
        from .global_state_tracker import get_global_tracker
        tracker = get_global_tracker()
        if tracker:
            actual_batches = list(range(num_batches))
            # åªåœ¨future_batchesç‚ºç©ºæ™‚è¨»å†Šï¼Œé¿å…è¦†è“‹å·²å­˜åœ¨çš„batchåºåˆ—
            if not tracker.future_batches:
                tracker.register_future_batch(actual_batches)
                print(f"[INFO] Registered {num_batches} batches for {len(prompts)} prompts (batch_size={batch_size}): {actual_batches}")
        else:
            print("[WARNING] Global tracker not found during batch registration")
        
        self.args.max_batch_size = max(self.args.max_batch_size, len(prompts))

        if max_gen_len is None:
            max_gen_len = self.args.max_seq_len - 1

        nvtx.range_push("tokenization")
        prompts_tok = [
            self.tokenizer.encode(p, add_special_tokens=False) for p in prompts
        ]
        nvtx.range_pop()  # tokenization
        
        '''
        all_out_tokens: æœ€ç»ˆæ¯ä¸ª prompt ç”Ÿæˆçš„ token ID åºåˆ—
        all_out_text: å¯¹ä¸Šé¢ token çš„ decode ç»“æœ
        kv_profile: æ¯ä¸ª token çš„ KV è®¿é—® profile è®°å½•ï¼ˆå¸¦æ—¶é—´å’Œå†…å­˜ï¼‰
        '''
        all_out_tokens, all_out_text = [], []
        kv_profile = []
        
        for batch_idx in range(num_batches):
            try:
                # ç¡®å®šå½“å‰æ‰¹æ¬¡çš„promptsèŒƒå›´
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(prompts_tok))
                batch_prompts = prompts_tok[start_idx:end_idx]
                
                # é¡¯ç¤ºå…¨å±€batché€²åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                try:
                    if tracker and tracker.future_batches:
                        global_batch_idx = tracker.current_batch
                        total_global_batches = len(tracker.future_batches)
                        print(f"[INFO] Processing batch {global_batch_idx + 1}/{total_global_batches} with {len(batch_prompts)} prompts")
                    else:
                        print(f"[INFO] Processing batch {batch_idx + 1}/{num_batches} with {len(batch_prompts)} prompts")
                except:
                    print(f"[INFO] Processing batch {batch_idx + 1}/{num_batches} with {len(batch_prompts)} prompts")
                
                '''
                bsz: å½“å‰ batch çš„æ ·æœ¬æ•°
                max_prompt: å½“å‰ batch ä¸­æœ€é•¿çš„ prompt token æ•°
                total_len: å½“å‰ batch éœ€è¦åˆ†é…çš„æœ€å¤§åºåˆ—é•¿åº¦ (æœ€é•¿ prompt + å¯ç”Ÿæˆçš„ token)
                '''
                bsz = len(batch_prompts)
                max_prompt = max(len(x) for x in batch_prompts)
                total_len = min(self.args.max_seq_len, max_gen_len + max_prompt)
                
            except Exception as e:
                print(f"âŒ Error during batch {batch_idx + 1} initialization: {e}")
                continue

            try:
                '''
                è·å– tokenizer é‡Œç”¨äº padding çš„ token ID;
                å¦‚æœ tokenizer æ²¡å®šä¹‰ pad_token_id(ä¾‹å¦‚åŸç”Ÿ LLaMA å°±æ²¡æœ‰ï¼‰ï¼Œåˆ™ fallback ä½¿ç”¨ eos_token_id æ¥å¡«å……ï¼›
                è¿™ä¸ª pad_id å°†ç”¨äºå¡«æ»¡æ¯æ¡ prompt åé¢çš„ç©ºç™½ä½ç½®ã€‚
                '''
                pad_id = (
                    self.tokenizer.pad_token_id
                    if self.tokenizer.pad_token_id is not None
                    else self.tokenizer.eos_token_id
                )
                '''
                tokens æ˜¯è¾“å…¥æ¨¡å‹çš„ token ID äºŒç»´å¼ é‡,shape ä¸º (bsz, total_len);
                åˆå§‹åŒ–æ—¶å…¨éƒ¨å¡«å……ä¸º pad_id,å³â€œç©ºâ€çš„æ ‡è®°;
                '''
                tokens = torch.full(
                    (bsz, total_len),
                    pad_id,
                    dtype=torch.long,
                    device=self.args.device,
                )

                for i, tok in enumerate(batch_prompts):
                    tokens[i, : len(tok)] = torch.tensor(tok, device=self.args.device)

                eos_mask = torch.zeros(bsz, dtype=torch.bool, device=self.args.device)
                prompt_mask = tokens != pad_id
                
            except torch.cuda.OutOfMemoryError as e:
                print(f"âŒ CUDA OOM during batch {batch_idx + 1} tensor allocation: {e}")
                torch.cuda.empty_cache()
                continue
            except RuntimeError as e:
                if "CUDA" in str(e):
                    print(f"âŒ CUDA error during batch {batch_idx + 1} tensor allocation: {e}")
                    torch.cuda.empty_cache()
                    continue
                else:
                    raise
            '''
            æ¯ä¸ª step(cur_pos)åšï¼š
            1.è°ƒç”¨æ¨¡å‹è¿›è¡Œä¸€æ¬¡ forwardè¾“å…¥å½“å‰åºåˆ—çš„æœ€åä¸€ä¸ª token;
            2.å¾—åˆ° logits â†’ æ ¹æ®æ¸©åº¦é‡‡æ ·æˆ– argmax,å¾—åˆ°ä¸‹ä¸€ä¸ª token;
            3.å†™å…¥ tokens;
            4.å¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½ç”Ÿæˆäº† <eos>ï¼Œæå‰é€€å‡ºï¼›
            5.åŒæ—¶æ”¶é›† KV cache profiling ä¿¡æ¯ï¼ˆæ—¶é—´ã€ç©ºé—´ï¼‰ï¼›
            '''
            # è·å–å…¨å±€æ‰¹æ¬¡ä¿¡æ¯ä»¥æ˜¾ç¤ºæ›´å‡†ç¡®çš„è¿›åº¦
            try:
                tracker = get_global_tracker()
                if tracker and hasattr(tracker, 'current_batch') and tracker.current_batch is not None:
                    global_batch_num = tracker.current_batch + 1
                    total_global_batches = len(tracker.future_batches) if hasattr(tracker, 'future_batches') else 'Unknown'
                    desc = f"Generating tokens for batch {global_batch_num}/{total_global_batches} (local {batch_idx + 1}/{num_batches})"
                else:
                    desc = f"Generating tokens for batch {batch_idx + 1}/{num_batches}"
            except:
                desc = f"Generating tokens for batch {batch_idx + 1}/{num_batches}"
            
            for cur_pos in tqdm(range(1, total_len), desc=desc):
                nvtx.range_push(f"token_{cur_pos}_generation")
                try:
                    # ---- forward ----
                    nvtx.range_push(f"token_{cur_pos}_forward")
                    with torch.no_grad():
                        logits = self.model(tokens[:, cur_pos - 1 : cur_pos], cur_pos)
                    nvtx.range_pop()  # forward

                    nvtx.range_push(f"token_{cur_pos}_sampling")
                    if temperature > 0:
                        probs = torch.softmax(logits[:, -1] / temperature, dim=-1)
                        next_tok = self._sample_top_p(probs, top_p)
                    else:
                        next_tok = torch.argmax(logits[:, -1], dim=-1)

                    next_tok = next_tok.reshape(-1)
                    next_tok = torch.where(prompt_mask[:, cur_pos], tokens[:, cur_pos], next_tok)
                    tokens[:, cur_pos] = next_tok
                    eos_mask |= (~prompt_mask[:, cur_pos]) & (next_tok == self.tokenizer.eos_token_id)
                    nvtx.range_pop()  # sampling
                    
                    if eos_mask.all():
                        nvtx.range_pop()  # token_generation
                        break
                        
                except torch.cuda.OutOfMemoryError as e:
                    print(f"âŒ CUDA OOM during inference at position {cur_pos}: {e}")
                    torch.cuda.empty_cache()
                    nvtx.range_pop()  # token_generation (error case)
                    raise RuntimeError(f"GPU out of memory during inference") from e
                except RuntimeError as e:
                    if "CUDA" in str(e):
                        print(f"âŒ CUDA error during inference at position {cur_pos}: {e}")
                        torch.cuda.empty_cache()
                        nvtx.range_pop()  # token_generation (error case)
                        raise RuntimeError(f"CUDA error during inference") from e
                    else:
                        nvtx.range_pop()  # token_generation (error case)
                        raise
                        
                nvtx.range_pop()  # token_generation

                kv_re_time = sum(self.model.kv_times)
                bytes_per_token = (                
                    2 * self.model.args.n_kv_heads
                    * self.model.args.dim // self.model.args.n_heads
                    * self.model.embed_tokens.weight.element_size()
                )
                kv_bytes = bytes_per_token * cur_pos * self.model.args.n_layers
                kv_profile.append(
                    {
                        "batch_idx": batch_idx,
                        "token_idx": int(cur_pos),
                        "phase": "prefill" if cur_pos < max_prompt else "decode",
                        "kv_re_ms": float(kv_re_time),
                        "kv_kb": float(kv_bytes / 1024),
                    }
                )
                
            # ---- å¤„ç†å½“å‰æ‰¹æ¬¡è¾“å‡º ----
            for row in tokens.tolist():
                if self.tokenizer.eos_token_id in row:
                    row = row[: row.index(self.tokenizer.eos_token_id)]
                all_out_tokens.append(row)
                all_out_text.append(self.tokenizer.decode(row))
        
        # ä½¿ç”¨å¤„ç†åçš„ç»“æœ
        out_tokens, out_text = all_out_tokens, all_out_text

        # ä¿å­˜ profiling
        if profile_output_dir:
            os.makedirs(profile_output_dir, exist_ok=True)
            save_name = os.path.join(profile_output_dir, f"{Path(self.args.checkpoints_dir).name}_kv_profile.json")
            with open(save_name, "w", encoding="utf-8") as f:
                json.dump(kv_profile, f, indent=2)
            print(f"[INFO] KV profile saved â†’ {save_name}")

        return out_tokens, out_text

    # ---------- utils ----------
    @staticmethod
    def _sample_top_p(probs, p):
        sort_probs, sort_idx = torch.sort(probs, dim=-1, descending=True)
        cumsum = torch.cumsum(sort_probs, dim=-1)
        sort_probs[cumsum - sort_probs > p] = 0.0
        sort_probs.div_(sort_probs.sum(dim=-1, keepdim=True))
        next_tok = torch.multinomial(sort_probs, 1)
        return torch.gather(sort_idx, -1, next_tok)

    def encode(self, text: str):
        return self.tokenizer.encode(text)

    def decode(self, ids):
        return self.tokenizer.decode(ids)
