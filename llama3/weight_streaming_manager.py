import os
import queue
import time
import json
import threading
import psutil
from collections import OrderedDict
from pathlib import Path
from typing import Dict, List, Optional, Any

import torch
import torch.nn as nn

from .stream_mnt import get_streams
from .config import load_runtime_config

# NVTX profiling support (no-op fallback if unavailable)
try:
    import torch.cuda.nvtx as nvtx
    NVTX_AVAILABLE = True
except ImportError:
    NVTX_AVAILABLE = False

    class nvtx:
        @staticmethod
        def range_push(name): ...
        @staticmethod
        def range_pop(): ...
        
ATTN_GROUP = (
    "attention.wq.weight", "attention.wk.weight",
    "attention.wv.weight", "attention.wo.weight",
)
FFN_GROUP  = (
    "feed_forward.w1.weight", "feed_forward.w2.weight",
    "feed_forward.w3.weight",
)
GROUPS = {"attn": ATTN_GROUP, "ffn": FFN_GROUP}

def _pinned_clone_cpu(t: torch.Tensor) -> torch.Tensor:
    """Return a contiguous, (best-effort) pinned CPU copy of tensor t."""
    x = t.detach().cpu().contiguous()
    try:
        x = x.pin_memory()
    except Exception:
        pass
    return x


def _collect_block_modules(block: nn.Module) -> Dict[str, nn.Module]:
    """
    Collect submodules within a transformer block that should be streamed:
      - Prefer fine-grained modules exposed by attention/feed_forward via `_get_modules_dict`.
      - Collect common norm layers (small, suitable for GPU residency).
      - Fallback to the entire block if nothing else is found.
    The returned dict maps a descriptive name -> submodule.
    """
    mods: Dict[str, nn.Module] = {}

    # Attention submodules
    if hasattr(block, "attention") and hasattr(block.attention, "_get_modules_dict"):
        try:
            mods.update(block.attention._get_modules_dict())
        except Exception:
            pass

    # FFN submodules
    if hasattr(block, "feed_forward") and hasattr(block.feed_forward, "_get_modules_dict"):
        try:
            mods.update(block.feed_forward._get_modules_dict())
        except Exception:
            pass

    # Norm layers (small; good GPU residents)
    norm_modules: Dict[str, nn.Module] = {}
    for norm_name in ["attn_norm", "ffn_norm", "attention_norm", "ffn_norm_pre", "ffn_norm_post"]:
        if hasattr(block, norm_name):
            norm_module = getattr(block, norm_name)
            if isinstance(norm_module, nn.Module):
                norm_modules[f"norm_{norm_name}"] = norm_module

    if norm_modules:
        mods.update(norm_modules)

    # Fallback: stream the whole block
    if not mods:
        mods["__block__"] = block

    return mods


class WeightStreamingManager:
    """
    Weight Streaming Manager (minimally invasive):

    - Keep a pinned-CPU master copy of parameters; stage to GPU just-in-time.
    - Before entering layer i: ensure_on_gpu(i) (optionally wait for readiness).
    - Prefetch layers i+1..i+prefetch_distance asynchronously on a dedicated H2D stream.
    - Simple LRU keeps up to `max_cached_layers` layers resident on GPU.
      Eviction does not do D2H for parametersâ€”switches pointers back to CPU master copy.
      (Buffers may copy back to CPU if needed.)
    - Norm modules are moved to GPU once and kept resident.

    Typical usage:
        wsm = WeightStreamingManager(model, device="cuda",
                                     prefetch_distance=1, max_cached_layers=4)
        # Hooks are installed automatically on each layer.
    """

    def __init__(self,
                 model: nn.Module,
                 device: str = "cuda",
                 prefetch_distance: int = 4,
                 max_cached_layers: int = 4,
                 layers_attr: str = "layers",
                 warmup_layers: int = 3,
                 verbose: bool = False,
                 monitor_fragmentation: bool = False,
                 ssd_manifest_path: Optional[str] = None,
                 cpu_cache_layers: int = 50,
                 staging_mb: int = 64):
        assert torch.cuda.is_available(), "CUDA not available"
        self.model = model
        self.device = device
        self.verbose = verbose

        # è¯Šæ–­ï¼šæ‰“å°ä¼ å…¥çš„ device å‚æ•°
        print(f"[WSM] Initialized with device={device} (type={type(device)})")
        
        self.streams = get_streams(device)
        self.prefetch_distance = int(prefetch_distance)
        # self.max_cached_layers = int(max_cached_layers)
        # self.cpu_cache_layers = int(cpu_cache_layers)
        self.gpu_cache_max = max(1, int(max_cached_layers))
        self.cpu_cache_max = max(1, int(cpu_cache_layers))
        self.max_cached_layers = self.gpu_cache_max
        self.cpu_cache_layers  = self.cpu_cache_max
        self.warmup_layers = max(0, int(warmup_layers))

        # â€”â€” è¿è¡ŒçŠ¶æ€ â€”â€”
        self.n_layers = len(getattr(model, "layers", []))
        self.grouped_mode = True  # ä¾› SelfAttention/FFN èµ°ç»„çº§ API
        self._anchor = 0          # è®¡ç®—é”šç‚¹ï¼ˆEncoderBlock.forward ä¼šæŒç»­æ›´æ–°ï¼‰
        self._anchor_lock = threading.Lock()

        
        # retain policy to avoid evicting "now" or "soon" layers
        self._retain_set = set()               # layers to avoid evicting
        self._last_touch = {}                  # layer_id -> monotonic seconds
        self._min_retain_ms = 50               # default 50ms; tune via runtime cfg if needed


        # SSD backend configuration
        self.ssd_enabled = ssd_manifest_path is not None
        self.ssd_manifest = None
        self.ssd_dio = None
        self.staging_buffer = None
        self.layers_params = {}

        # CPU cache for SSD->CPU->GPU pipeline
        self.cpu_cache: OrderedDict[int, Dict[str, torch.Tensor]] = OrderedDict()
        self.cpu_cache_lock = threading.Lock()

        # # CPU LRU tracking: ç»´æŠ¤å·²ç¼“å­˜å±‚é›†åˆ + LRU é˜Ÿåˆ—
        # self._cpu_cached_layers = set()
        # self._cpu_lru = []  # å­˜ layer_idxï¼Œæœ€è¿‘ä½¿ç”¨ç§»åˆ°æœ«å°¾
        # GPU/CPU ç¼“å­˜ç»“æ„ï¼ˆLRUï¼‰
        self._gpu_layers_lru = OrderedDict()   # {layer_idx: timestamp}
        self._cpu_layers_lru = OrderedDict()   # {layer_idx: timestamp}
        self._cpu_cached_layers = set()        # membership å¿«é€Ÿåˆ¤æ–­
        self._cpu_lru: list[int] = []  # LRU: oldest at front, newest at end

        # ==== add to class WeightStreamingManager (init: set knobs) ====
        self.evict_after_use: bool = (os.getenv("WSM_EVICT_AFTER_USE", "1") == "1")
        self.evict_delay_ms: int = int(os.getenv("WSM_EVICT_DELAY_MS", "0"))
        self.cpu_evict_after_use: bool = (os.getenv("WSM_CPU_EVICT_AFTER_USE", "0") == "1")

        # Background prefetch management
        self.prefetch_thread: Optional[threading.Thread] = None
        self.prefetch_queue: List[int] = []
        self.stop_prefetch = threading.Event()
        self.prefetch_lock = threading.Lock()

        # Preload completion tracking
        self.gpu_preload_complete = threading.Event()
        self.cpu_preload_complete = threading.Event()
        self.target_gpu_layers = warmup_layers
        self.target_cpu_layers = cpu_cache_layers

        # Optional fragmentation monitoring
        self.monitor_fragmentation = monitor_fragmentation
        self.memory_stats: List[Dict] = []
        self.fragmentation_threshold = 0.3
        # PD è‡ªé€‚åº”å‚æ•°ï¼ˆæ»å› + EMAï¼‰
        
        
        
        
        # --- thread & queue state ---
        self._stopped: bool = False
        self._stop_event: threading.Event = threading.Event()
        self._threads: list[threading.Thread] = []
                
        self._cpu_lock = threading.RLock()
        self._epoch = 0
        self._cpu_pf_q: "queue.Queue[tuple[int, int|None]]" = queue.Queue(maxsize=2048)
        self._inflight_cpu_layers = set()
        
        # ------------- GPU group tracking -------------
        self._gpu_group_inflight: dict[tuple[int, str], threading.Event] = {}
        self._gpu_group_in_use: set[tuple[int,str]] = set()   # å¯é€‰ï¼Œå·²æœ‰åˆ™å¤ç”¨
        self._group_ready_events: dict[tuple[int,str], torch.cuda.Event] = {}
        
        # ------------- H2D å¹¶å‘é—¸é—¨ï¼ˆç»„çº§ï¼‰-------------
        self._h2d_groups_max: int = int(os.getenv("WSM_H2D_GROUP_BACKLOG_MAX", "1"))  # å¼ºçƒˆå»ºè®®=1
        self._h2d_sem = threading.Semaphore(self._h2d_groups_max)

        # ------------- GPU å†…å­˜ä½™é‡å®ˆå« -------------
        self._gpu_free_guard_mb: int = int(os.getenv("WSM_GPU_FREE_GUARD_MB", "1024"))  # 1GB ä¿æŠ¤
        # self._gpu_max_groups: int = int(os.getenv("WSM_GPU_MAX_GROUPS", "3"))  # [å·²åºŸå¼ƒ] ä½¿ç”¨ä¸‹æ–¹çš„ self.gpu_max_groups


        t = threading.Thread(target=self._cpu_prefetch_worker, name="wsm_cpu_pf", daemon=True)
        self._threads.append(t)
        t.start()

        
        _rcfg = {}
        try:
            _rcfg = load_runtime_config()
            _rcfg = getattr(_rcfg, "io", {}) if hasattr(_rcfg, "io") else {}
        except Exception:
            _rcfg = {}
        self.pd_cap = int(getattr(_rcfg, "PD_CAP", 3) or 3)
        self.pcie_hi = float(getattr(_rcfg, "PCIE_BUSY_UTIL_TH_HI", 0.70))
        self.pcie_lo = float(getattr(_rcfg, "PCIE_BUSY_UTIL_TH_LO", 0.60))
        self.pin_lo  = float(getattr(_rcfg, "PINNED_LOW_WATERMARK", 0.20))
        self.pin_hi  = float(getattr(_rcfg, "PINNED_HIGH_WATERMARK", 0.30))
        self.throttle_ms = int(getattr(_rcfg, "IO_RAW_THROTTLE_MS", 30))
        self._pd_current = max(1, self.prefetch_distance)
        self._pcie_ema = 0.0
        self._ema_alpha = 0.2
        self._last_h2d_ms = 0.0
        # å¯é€‰ï¼šå¤–éƒ¨ KV Offloaderï¼ˆè‹¥ä¸»ç¨‹åºä¼ å…¥ï¼Œå¯ç”¨äºè§¦å‘â€œæš‚åœå†™â€ï¼‰
        self.kv_offloader = None
        
        self.grouped_mode = True  # å¼€å¯"ç»„çº§"æ¨¡å¼
        # ç»„é¢„ç®—è®¡ç®—ï¼šå½“å‰å±‚(attn=1) + å½“å‰å±‚é¢„å–(ffn=1) + ä¸‹ä¸€å±‚é¢„å–(attn=1) + å®‰å…¨ä½™é‡(1) + å¼¹æ€§ç¼“å†²(1~2) = 5~7
        # å¯¹äº SSD æ¨¡å¼æˆ–æ›´æ¿€è¿›çš„å¹¶å‘é¢„å–ï¼Œå»ºè®® 6~9ï¼›æ˜¾å­˜å……è¶³æ—¶å¯ä»¥è®¾æ›´é«˜
        self.gpu_max_groups = int(os.getenv("WSM_GPU_MAX_GROUPS", "6"))  # é»˜è®¤ 6ï¼Œå»ºè®®èŒƒå›´ 6~9
        
        self.group_prefetch_depth = int(os.getenv("WSM_GROUP_PREFETCH_DEPTH", "4"))
        
        self.cpu_prefetch_distance = int(os.getenv("WSM_CPU_PREFETCH_DISTANCE", "50"))  # CPU ç«¯é¢„å–çª—å£
        self.cpu_cache_cap_layers  = int(os.getenv("WSM_CPU_CACHE_CAP_LAYERS",  "50"))  # ç¡¬ä¸Šé™
        self.cpu_cache_hwm_layers  = int(os.getenv("WSM_CPU_CACHE_HWM_LAYERS",  "55"))  # é«˜æ°´ä½
        self.gpu_free_guard_mb = int(os.getenv("WSM_GPU_FREE_GUARD_MB", "1024"))

        # æ»‘åŠ¨çª—å£ + å›æ»å‚æ•°
        # â˜… å…³é”®ä¿®å¤: cpu_cache_cap åº”è¯¥ä½¿ç”¨ cpu_cache_layers å‚æ•°
        env_cap = os.getenv("WSM_CPU_CACHE_CAP_LAYERS")
        if env_cap is not None:
            self.cpu_cache_cap = int(env_cap)
            print(f"[WSM] Using WSM_CPU_CACHE_CAP_LAYERS from env: {self.cpu_cache_cap}")
        else:
            self.cpu_cache_cap = cpu_cache_layers
            print(f"[WSM] Using cpu_cache_layers parameter: {self.cpu_cache_cap}")

        self.cpu_hwm       = int(os.getenv("WSM_CPU_CACHE_HWM_LAYERS", str(self.cpu_cache_cap + 5)))
        self.cpu_lwm       = int(os.getenv("WSM_CPU_CACHE_LWM_LAYERS", str(max(2, self.cpu_cache_cap - 5))))
        self.cpu_back_margin = int(os.getenv("WSM_CPU_BACK_MARGIN", "4"))  # ç•™ä¸€ç‚¹å†å²
        self.cpu_win_base  = 0  # æ»‘åŠ¨çª—å£èµ·ç‚¹ï¼ˆå±‚å·ï¼‰
        self._warm_done = False  # é¢„çƒ­å¹‚ç­‰æ ‡å¿—

        print(f"[WSM] CPU cache config: cap={self.cpu_cache_cap}, hwm={self.cpu_hwm}, lwm={self.cpu_lwm}")

        # æ¿€è¿›é¢„å–æ¨¡å¼ï¼šåœ¨ hook ä¸­è‡ªåŠ¨é¢„å–ä¸‹ N å±‚ï¼ˆä¸è®¡ç®—é‡å ï¼Œæå‡ GPU åˆ©ç”¨ç‡ï¼‰
        self.aggressive_gpu_prefetch = int(os.getenv("WSM_AGGRESSIVE_GPU_PREFETCH", "2"))  # é»˜è®¤é¢„å–å 2 å±‚

        # ä¿å­˜åŸå§‹çš„prefetch_distanceï¼Œæ ¹æ®æ¨¡å¼é€‰æ‹©ä½¿ç”¨
        self._original_prefetch_distance = self.prefetch_distance
        if self.grouped_mode:
            # ç»„çº§æ¨¡å¼ï¼šå…³é—­æ•´å±‚é¢„å–ï¼Œæ”¹ç”¨ç»„çº§é¢„å–
            self._layer_prefetch_distance = 0
            self._group_prefetch_distance = 2  # é¢„å–2ç»„ï¼ˆå½“å‰å±‚ffn + ä¸‹ä¸€å±‚attnï¼‰
        else:
            # å±‚çº§æ¨¡å¼ï¼šä¿æŒåŸæœ‰é€»è¾‘
            self._layer_prefetch_distance = self.prefetch_distance
            self._group_prefetch_distance = 0

        self._gpu_group_lru = []    # [(layer_idx, 'attn'/'ffn'), ...] ç»´æŠ¤åœ¨å¡ä¸Šçš„ç»„
        self._gpu_groups_in_use = set()  # æ­£åœ¨è®¡ç®—çš„ç»„ï¼Œé˜²æ­¢è¢«è¸¢æ‰
        self._group_lock = threading.RLock()

        # â˜… ä¿®å¤ 5: å»é‡ - é˜²æ­¢é‡å¤åŠ è½½åŒä¸€å±‚/ç»„
        self._inflight_cpu_layers = set()       # æ­£åœ¨åŠ è½½åˆ° CPU çš„å±‚
        self._inflight_gpu_groups = set()       # æ­£åœ¨åŠ è½½åˆ° GPU çš„ç»„ (layer, kind)
        self._inflight_lock = threading.Lock()  # ä¿æŠ¤ inflight é›†åˆ

        # â˜… ä¿®å¤ 6: çª—å£é©±åŠ¨çš„ Prefetch Cursorï¼ˆæœ‰åºåŠ è½½ï¼‰
        self._cpu_pf_cursor = 0                 # CPU é¢„å–æ¸¸æ ‡
        self._gpu_pf_cursor_attn = 0            # GPU attn ç»„é¢„å–æ¸¸æ ‡
        self._gpu_pf_cursor_ffn = 0             # GPU ffn ç»„é¢„å–æ¸¸æ ‡

        # â˜… ä¿®å¤ 7: Resident æ¨¡å—é¢„ç®—ï¼ˆé˜²æ­¢ç¢ç‰‡å’Œ OOMï¼‰
        self.resident_budget_gb = float(os.getenv("WSM_RESIDENT_BUDGET_GB", "3.0"))  # é»˜è®¤ 3GB
        self.resident_max_modules = int(os.getenv("WSM_RESIDENT_MAX_MODULES", "200"))  # é»˜è®¤æœ€å¤š 200 ä¸ªï¼ˆè¶³å¤Ÿ 80 å±‚ * 2 normï¼‰
        self._resident_bytes_used = 0  # å·²ä½¿ç”¨çš„ resident é¢„ç®—

        # â˜… ä¿®å¤ 8: KV I/O å¸¦å®½ä»²è£
        self.kv_throttle_threshold = int(os.getenv("WSM_KV_THROTTLE_THRESHOLD", "3"))  # H2D backlog é˜ˆå€¼
        self.kv_throttle_ms = int(os.getenv("WSM_KV_THROTTLE_MS", "50"))  # throttle æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
        self._h2d_pending_count = 0  # weight_h2d stream å¾…å¤„ç†äº‹ä»¶æ•°
        self._last_kv_throttle_time = 0  # ä¸Šæ¬¡ throttle KV çš„æ—¶é—´
        
        # åœ¨ __init__ é‡Œæ–°å¢ä¸€ä¸ªå¼€å…³ï¼ˆé»˜è®¤ Falseï¼›ç”±ç¯å¢ƒå˜é‡æ§åˆ¶ï¼‰
        self.evict_finished_group = (os.getenv("WSM_EVICT_FINISHED", "1") == "1")
        
        # åœ¨ __init__ ç»“å°¾é™„è¿‘ã€å…¶å®ƒé…ç½®æ—è¾¹åŠ ï¼š
        # self.cpu_rolling_mode   = (os.getenv("WSM_CPU_ROLLING_MODE",  "1") == "1")   # å¼€å¯â€œå±‚å±‚æ»šåŠ¨â€
        # self.cpu_wrap_around    = (os.getenv("WSM_CPU_WRAP_AROUND",   "1") == "1")   # æ”¯æŒä¸‹ä¸€è½®å›åˆ° L0
        # self.cpu_roll_stride    = int(os.getenv("WSM_CPU_ROLL_STRIDE","1"))          # æ¯æ¬¡å³ç§»å‡ å±‚ï¼Œé»˜è®¤ 1
        # self.cpu_roll_sync      = (os.getenv("WSM_CPU_ROLL_SYNC",     "1") == "1")   # è§¦å‘ååŒæ­¥ç¡®ä¿çª—å£ï¼ˆç®€å•å¯é ï¼‰

        # # ---- Group-window policy (GPU) ----
        # ======== [å·²åºŸå¼ƒçš„æ¿€è¿›é¢„å–ç­–ç•¥ - ä¿ç•™ä¾›å‚è€ƒ] ========
        # # æŒ‰æ—§ç­–ç•¥ï¼šå½“å‰(i) attn+ffnï¼›i+1..i+3 çš„ attn+ffnï¼›ä»¥åŠ i+4 çš„ attn
        # self.future_both_layers = int(os.getenv("WSM_FUTURE_BOTH_LAYERS", "3"))   # i+1..i+3 ä¸¤ç»„
        # self.buffer_attn_ahead  = int(os.getenv("WSM_BUFFER_ATTN_AHEAD",  "4"))   # i+4 çš„ attn
        # self.group_wrap_around  = (os.getenv("WSM_GROUP_WRAP_AROUND", "0") == "1")
        #
        # # éœ€è¦çš„ç»„ä¸Šé™ = 2(å½“å‰) + 2*future_both + 1(buffer attn) = 9
        # _required_groups = 2 + 2*self.future_both_layers + 1
        # # è‹¥å¤–éƒ¨æ²¡æ˜¾å¼è®¾ WSM_GPU_MAX_GROUPSï¼Œåˆ™é‡‡ç”¨æ‰€éœ€ä¸Šé™ï¼›è‹¥è®¾äº†ï¼Œå°±å–æ›´å¤§çš„é‚£ä¸ª
        # try:
        #     env_max = int(os.getenv("WSM_GPU_MAX_GROUPS", "0"))
        # except ValueError:
        #     env_max = 0
        # self.gpu_max_groups = max(self.gpu_max_groups, _required_groups, env_max or 0)
        # print(f"[WSM] Group-window policy: future_both={self.future_both_layers}, "
        #     f"buffer_attn={self.buffer_attn_ahead}, gpu_max_groups={self.gpu_max_groups}")
        #
        # æ³¨ï¼šå½“å‰å®ç°é‡‡ç”¨æ›´ä¿å®ˆçš„é¢„å–ç­–ç•¥ï¼ˆè§ _pre_hook_factory çš„ç»„çº§é¢„å–é€»è¾‘ï¼‰ï¼š
        #     - å½“å‰å±‚ attn (æ‰§è¡Œä¸­) = 1
        #     - å½“å‰å±‚ ffn (å¼‚æ­¥é¢„å–) = 1
        #     - ä¸‹ä¸€å±‚ attn (å¼‚æ­¥é¢„å–) = 1
        #     - å®‰å…¨ä½™é‡ = 1
        #     - å¼¹æ€§ç¼“å†² = 1~2
        #     â†’ æ€»éœ€æ±‚ = 5~7 ç»„ï¼Œé»˜è®¤è®¾ä¸º 6ï¼Œå»ºè®®èŒƒå›´ 6~9
        # ======================================================
        self.cpu_ring_mode   = (os.getenv("WSM_CPU_RING_MODE",  "1") == "1")  # å¼€ï¼šç¯å½¢çª—å£
        self.cpu_ring_offset = int(os.getenv("WSM_CPU_RING_OFFSET", "1"))     # 1 => i+1 èµ·
        
        # --- group-level retention (add in __init__) ---
        self._grp_last_touch: dict[tuple[int, str], float] = {}
        self._grp_retain_ms: int = int(os.getenv("WSM_GRP_RETAIN_MS", "3"))  # é»˜è®¤ 3ms çš„è¶…çŸ­çª—

        
        
        
        # ç‹¬ç«‹ H2D streamï¼ˆä»…åœ¨CUDAè®¾å¤‡ä¸Šåˆ›å»ºï¼‰
        if str(self.device).startswith("cuda") and torch.cuda.is_available():
            self._copy_stream = torch.cuda.Stream(device=self.device)
        else:
            self._copy_stream = None  # CPUæ¨¡å¼ä¸‹ä¸éœ€è¦ç‹¬ç«‹stream
        

        # Resolve transformer blocks (layers)
        blocks: Optional[List[nn.Module]] = None
        if hasattr(model, "layer_infos"):
            try:
                blocks = [info["block"] for info in getattr(model, "layer_infos")]
            except Exception:
                blocks = None
        if blocks is None and hasattr(model, layers_attr):
            blocks = list(getattr(model, layers_attr))
        if not blocks:
            candidates = []
            for _name, child in model.named_children():
                if isinstance(child, (nn.Sequential, nn.ModuleList)):
                    candidates.append(list(child))
            blocks = max(candidates, key=len) if candidates else []
        if not blocks:
            raise RuntimeError("Cannot locate transformer blocks (layers) on model.")

        self.blocks: List[nn.Module] = blocks
        # self.n_layers = len(self.blocks)  # æ·»åŠ n_layerså±æ€§

        # Per-layer streaming units
        self.block_mods: Dict[int, Dict[str, nn.Module]] = {
            i: _collect_block_modules(b) for i, b in enumerate(self.blocks)
        }

        # Update target layers based on actual block count
        self.target_cpu_layers = min(self.target_cpu_layers, len(self.blocks))
        self.target_gpu_layers = min(self.target_gpu_layers, len(self.blocks))

        # æ„å»ºå‚æ•°ååˆ°Parameterå¯¹è±¡çš„æ˜ å°„ï¼ˆç”¨äºç»„çº§é¢„å–ï¼‰
        self.name_to_param: Dict[str, nn.Parameter] = {}
        # æ„å»ºå‚æ•°å½’å±æ¨¡å—æ˜ å°„ï¼šname -> (module_ref, attr_name)
        self.param_owner: Dict[str, tuple] = {}

        for layer_idx, block in enumerate(self.blocks):
            for param_name, param in block.named_parameters():
                full_name = f"layers.{layer_idx}.{param_name}"
                self.name_to_param[full_name] = param

        # æ„å»º param_owner æ˜ å°„
        for module_name, module in self.model.named_modules():
            for attr, p in module.named_parameters(recurse=False):
                full = f"{module_name}.{attr}" if module_name else attr
                self.param_owner[full] = (module, attr)

        # CPU master copy: Parameter object id -> pinned CPU tensor
        self.cpu_stash: Dict[int, torch.Tensor] = {}
        # GPU LRU cache: layer index -> None
        self.gpu_cache = OrderedDict()
        # Per-layer "ready" events recorded on H2D stream
        self.layer_events: Dict[int, torch.cuda.Event] = {}

        # Install pre-forward hooks: ensure current layer + prefetch next ones
        for i, blk in enumerate(self.blocks):
            blk.register_forward_pre_hook(self._pre_hook_factory(i))

        # Move norm modules to GPU once and keep resident
        self._setup_resident_norms()

        # Initialize SSD backend if enabled
        if self.ssd_enabled:
            self._initialize_ssd_backend(ssd_manifest_path, staging_mb)

        # # (Optional) Warm up target GPU layers to reduce initial latency
        # if self.target_gpu_layers > 0:
        #     warm = list(range(min(self.target_gpu_layers, len(self.blocks))))
        #     self.prefetch(warm)
        #     if self.verbose:
        #         print(f"[WSM] GPU warmup prefetch: {warm} (target: {self.target_gpu_layers} layers)")
        
        # (Optional) Warm up at start
        if self.target_gpu_layers > 0:
            if getattr(self, "grouped_mode", False):
                # ç»„çº§ warmupï¼ˆé»˜è®¤ doubletï¼š0.attn,0.ffn,1.attn,1.ffn,...ï¼‰
                scheme = os.getenv("WSM_WARMUP_GROUP_SCHEME", "doublet")
                self.warmup_groups_prefetch(layers=self.target_gpu_layers, scheme=scheme)
            else:
                # æ—§çš„æ•´å±‚ warmup è·¯å¾„
                warm = list(range(min(self.target_gpu_layers, len(self.blocks))))
                self.prefetch(warm)
                if self.verbose:
                    print(f"[WSM] GPU warmup prefetch: {warm} (target: {self.target_gpu_layers} layers)")


    # ---- Retention helpers --------------------------------------------------
    def _refresh_retain_window(self, current_idx: int):
        now = time.monotonic()
        self._last_touch[current_idx] = now
        hi = min(current_idx + max(1, self._pd_current) + 1, len(self.blocks))
        self._retain_set = set(range(current_idx, hi))

    def _should_retain(self, layer_id: int) -> bool:
        if layer_id in self._retain_set:
            return True
        last = self._last_touch.get(layer_id)
        if last is None:
            return False
        return (time.monotonic() - last) < (self._min_retain_ms / 1000.0)
    
    
    
    def _free_gpu_mem_bytes(self) -> int:
        # å…œåº•ï¼šå¦‚æœ device ä¸æ˜¯ CUDAï¼Œè¿”å›ä¸€ä¸ªå¾ˆå¤§çš„å€¼ï¼ˆé¿å… OOM æ£€æŸ¥å¤±è´¥ï¼‰
        if not str(self.device).startswith("cuda"):
            if self.verbose:
                print(f"[WSM] Warning: _free_gpu_mem_bytes called with non-CUDA device: {self.device}")
            return 100 * 1024**3  # è¿”å› 100GB ä½œä¸ºå ä½
        free, total = torch.cuda.mem_get_info(self.device)
        return int(free)

    # def _ensure_gpu_headroom(self, required_bytes: int, exclude: set[tuple[int,str]] | None = None):
    #     """ç¡®ä¿æœ‰ enough_free â‰¥ required + guardï¼Œä¸å¤Ÿåˆ™é€å‡ºï¼ˆæ’é™¤ in_use ä¸ excludeï¼‰ã€‚"""
    #     guard = self._gpu_free_guard_mb * 1024 * 1024
    #     exclude = exclude or set()
    #     tries = 0
    #     while True:
    #         free_now = self._free_gpu_mem_bytes()
    #         if free_now >= required_bytes + guard:
    #             return
    #         # é€å‡ºä¸€ä¸ª LRU ç»„ï¼ˆè·³è¿‡ in_use ä¸ excludeï¼‰
    #         if not self._evict_one_group_from_gpu(exclude=exclude):
    #             # å†æ¸…ä¸€æ¬¡ç¼“å­˜ï¼›ä»ä¸å¤Ÿå°±æŠ›
    #             torch.cuda.empty_cache()
    #             free_now = self._free_gpu_mem_bytes()
    #             if free_now >= required_bytes + guard:
    #                 return
    #             raise torch.cuda.OutOfMemoryError(
    #                 f"insufficient headroom: need={required_bytes/2**20:.2f}MB "
    #                 f"free={free_now/2**20:.2f}MB guard={guard/2**20:.2f}MB")
    #         torch.cuda.empty_cache()
    #         tries += 1
    #         if tries > 64:
    #             raise torch.cuda.OutOfMemoryError("eviction loop exceeded")
    def _ensure_gpu_headroom(self, required_bytes: int, exclude: set[tuple[int,str]] | None = None):
        guard = self._gpu_free_guard_mb * 1024 * 1024
        exclude = exclude or set()
        tries = 0
        while True:
            free_now = self._free_gpu_mem_bytes()
            if free_now >= required_bytes + guard:
                return
            # ç¬¬ä¸€è½®ï¼šå°Šé‡ç•™å­˜
            if self._evict_one_group_from_gpu(exclude=exclude, ignore_retain=False):
                torch.cuda.empty_cache()
                tries += 1
                if tries > 64:
                    raise torch.cuda.OutOfMemoryError("eviction loop exceeded(pass0)")
                continue
            # ç¬¬äºŒè½®ï¼šå¿½ç•¥ç•™å­˜
            if self._evict_one_group_from_gpu(exclude=exclude, ignore_retain=True):
                torch.cuda.empty_cache()
                tries += 1
                if tries > 96:
                    raise torch.cuda.OutOfMemoryError("eviction loop exceeded(pass1)")
                continue
            # ä»ä¸å¤Ÿï¼šå½»åº• OOM
            torch.cuda.empty_cache()
            free_now = self._free_gpu_mem_bytes()
            if free_now >= required_bytes + guard:
                return
            raise torch.cuda.OutOfMemoryError(
                f"insufficient headroom: need={required_bytes/2**20:.2f}MB "
                f"free={free_now/2**20:.2f}MB guard={guard/2**20:.2f}MB")
    
    

    def _evict_one_not_retained(self):
        # Try oldestâ†’newest; evict the first not "retained recently"
        for old in list(self.gpu_cache.keys()):
            if not self._should_retain(old):
                self.gpu_cache.pop(old, None)
                self._evict_layer_to_cpu(old)
                if self.verbose:
                    print(f"[WSM]   evict layer={old}")
                return old
        # Fallback: all are retained; evict LRU anyway (warn)
        old, _ = self.gpu_cache.popitem(last=False)
        if self.verbose:
            print(f"[WSM][WARN] retain window full; evict LRU layer={old}")
        self._evict_layer_to_cpu(old)
        return old

    # -------- SSD backend initialization --------

    def _initialize_ssd_backend(self, manifest_path: str, staging_mb: int):
        """Initialize SSD raw device backend for weight streaming"""
        try:
            from .weights_io_ssd_dram import (
                DirectIOFile, load_resident_to_gpu,
                alloc_pinned_aligned, DTYPE_MAP
            )

            # Load manifest
            self.ssd_manifest = json.loads(Path(manifest_path).read_text())
            raw_device = self.ssd_manifest["raw_device"]
            block_size = self.ssd_manifest["block_size"]

            # Create DirectIO file handle
            self.ssd_dio = DirectIOFile(raw_device, mode="r", block_size=block_size)

            # Create staging buffer for SSD->CPU transfers
            staging_bytes = staging_mb * 1024 * 1024
            staging_bytes = (staging_bytes // block_size) * block_size
            self.staging_buffer = alloc_pinned_aligned(staging_bytes, block_size)

            # Organize parameters by layer
            self._organize_params_by_layer()

            # Check DRAM capacity before proceeding
            self._check_dram_capacity()

            # Load resident weights to GPU
            # Note: This will handle uninitialized parameters by loading from SSD
            load_resident_to_gpu(self.model, self.ssd_manifest, device=self.device)

            # Start background prefetch worker if prefetch is enabled
            if self.prefetch_distance > 0:
                self._start_prefetch_worker()
            else:
                print("âš ï¸  Prefetch disabled - checking if all weights fit in DRAM...")
                self._validate_no_prefetch_mode()

            if self.verbose:
                print(f"[WSM] SSD backend initialized: {raw_device}")
                print(f"[WSM] Staging buffer: {staging_mb}MB, CPU cache: {self.cpu_cache_layers} layers")

        except Exception as e:
            if self.verbose:
                print(f"[WSM] Failed to initialize SSD backend: {e}")
            self.ssd_enabled = False

    def _organize_params_by_layer(self):
        """Organize manifest parameters by layer for efficient access"""
        self.layers_params = {}
        for param in self.ssd_manifest["params"]:
            layer_id = param["layer"]
            if layer_id >= 0:  # Skip non-layer params
                if layer_id not in self.layers_params:
                    self.layers_params[layer_id] = []
                self.layers_params[layer_id].append(param)

    def _advance_cpu_window(self, cur_layer: int):
        """
        åªå‰ç§»ï¼Œä¸åé€€ï¼›ç¡®ä¿å½“å‰å±‚åœ¨çª—å£å†…
        â˜… ä¿®å¤: çª—å£åº”è¯¥åŒ…å«å½“å‰å±‚ï¼Œè€Œä¸æ˜¯æ»å
        """
        # è®¡ç®—çª—å£åŸºå‡†ï¼šç¡®ä¿ cur_layer åœ¨çª—å£å†…ï¼Œä¸”å°½é‡é å‰
        # window = [base, base+cap-1]
        # æˆ‘ä»¬å¸Œæœ› cur_layer åœ¨çª—å£å‰éƒ¨ï¼ˆç•™å‡ºé¢„å–ç©ºé—´ï¼‰
        target_base = max(0, cur_layer - self.cpu_back_margin)

        # ä½†å¦‚æœå½“å‰å±‚å·²ç»è¶…å‡ºçª—å£å³ç«¯ï¼Œå¿…é¡»æ¨è¿›çª—å£
        L0, L1 = self._target_cpu_window()
        if cur_layer > L1:
            # å½“å‰å±‚è¶…å‡ºçª—å£ï¼Œå¼ºåˆ¶æ¨è¿›
            target_base = max(target_base, cur_layer - self.cpu_cache_cap + 1)
            print(f"[WSM DEBUG] Current layer {cur_layer} > window end {L1}, forcing window advance")

        old_base = self.cpu_win_base
        if target_base > self.cpu_win_base:
            self.cpu_win_base = target_base
            print(f"[WSM DEBUG] _advance_cpu_window: cur_layer={cur_layer}, "
                  f"window_base {old_base} -> {self.cpu_win_base} (target={target_base})")
        else:
            print(f"[WSM DEBUG] _advance_cpu_window: cur_layer={cur_layer}, "
                  f"window_base={self.cpu_win_base} unchanged (target={target_base})")

    def _target_cpu_window(self):
        """
        è¿”å›å½“å‰æ»‘åŠ¨çª—å£çš„èŒƒå›´ [L0, L1]
        """
        L0 = self.cpu_win_base
        L1 = min(self.n_layers - 1, self.cpu_win_base + self.cpu_cache_cap - 1)
        return L0, L1

    def _ensure_cpu_window(self):
        """
        ç¡®ä¿æ»‘åŠ¨çª—å£å†…çš„å±‚éƒ½å·²åŠ è½½åˆ° CPU cache
        â˜… å…³é”®ä¿®å¤: å…ˆé€å‡ºçª—å£å¤–çš„å±‚ï¼Œå†åŠ è½½ç¼ºå¤±å±‚ï¼Œä¿æŒå®¹é‡æ’å®š
        """
        L0, L1 = self._target_cpu_window()

        # â˜… DEBUG: æ‰“å°çª—å£å’Œæ¸¸æ ‡çŠ¶æ€
        print(f"[WSM DEBUG] _ensure_cpu_window: window=[{L0}, {L1}], cursor={self._cpu_pf_cursor}, "
              f"win_base={self.cpu_win_base}, cache_size={len(self.cpu_cache)}")

        # â˜… å…³é”®ä¿®å¤: å…ˆæ¸…ç†çª—å£å¤–çš„å±‚ï¼ˆä¸»åŠ¨é€å‡ºï¼‰
        layers_to_evict = []
        for layer_id in list(self.cpu_cache.keys()):
            if layer_id < L0 or layer_id > L1:
                layers_to_evict.append(layer_id)

        if layers_to_evict:
            print(f"[WSM DEBUG] Evicting {len(layers_to_evict)} layers outside window [{L0}, {L1}]")
            print(f"[WSM DEBUG] Layers to evict: {sorted(layers_to_evict)}")
            print(f"[WSM DEBUG] Current cache keys: {sorted(list(self.cpu_cache.keys()))}")
            for layer_id in layers_to_evict:
                with self.cpu_cache_lock:
                    self.cpu_cache.pop(layer_id, None)
                if layer_id in self._cpu_lru:
                    self._cpu_lru.remove(layer_id)
                self._cpu_cached_layers.discard(layer_id)

        # â˜… ä¿®å¤: æ¸¸æ ‡åªèƒ½åœ¨çª—å£å†…ç§»åŠ¨
        if self._cpu_pf_cursor > L1:
            print(f"[WSM DEBUG] Cursor {self._cpu_pf_cursor} > window end {L1}, resetting to {L0}")
            self._cpu_pf_cursor = L0
        else:
            old_cursor = self._cpu_pf_cursor
            self._cpu_pf_cursor = max(self._cpu_pf_cursor, L0)
            if old_cursor != self._cpu_pf_cursor:
                print(f"[WSM DEBUG] Advanced cursor from {old_cursor} to {self._cpu_pf_cursor}")

        # ç»Ÿè®¡éœ€è¦åŠ è½½çš„å±‚
        missing_layers = []
        for L in range(L0, L1 + 1):
            if L not in self.cpu_cache:
                missing_layers.append(L)

        if missing_layers:
            print(f"[WSM DEBUG] Need to load {len(missing_layers)} missing layers: {missing_layers[:5]}...")

        # æŒ‰åºåŠ è½½ç¼ºå¤±å±‚ï¼ˆä»æ¸¸æ ‡ä½ç½®å¼€å§‹ï¼‰
        for L in range(self._cpu_pf_cursor, L1 + 1):
            if L not in self.cpu_cache:
                # â˜… å…³é”®: åŠ è½½å‰æ£€æŸ¥å®¹é‡ï¼Œå¿…è¦æ—¶å…ˆè…¾å‡ºç©ºé—´
                while len(self.cpu_cache) >= self.cpu_cache_cap:
                    # å®¹é‡å·²æ»¡ï¼Œè¸¢æ‰ä¸€ä¸ªæœ€è€çš„å±‚ï¼ˆLRUï¼‰
                    if self._cpu_lru:
                        evict_layer = self._cpu_lru[0]
                        print(f"[WSM DEBUG] Cache full ({len(self.cpu_cache)}/{self.cpu_cache_cap}), evicting layer {evict_layer}")
                        with self.cpu_cache_lock:
                            self.cpu_cache.pop(evict_layer, None)
                        self._cpu_lru.pop(0)
                        self._cpu_cached_layers.discard(evict_layer)
                    else:
                        break

                self._load_layer_to_cpu(L)
                self._cpu_pf_cursor = L + 1

        # å¦‚æœçª—å£å†…æ‰€æœ‰å±‚éƒ½å·²åŠ è½½ï¼Œæ¸¸æ ‡æ¨è¿›åˆ°çª—å£æœ«å°¾+1
        if self._cpu_pf_cursor <= L1:
            self._cpu_pf_cursor = L1 + 1

        print(f"[WSM DEBUG] _ensure_cpu_window done: cache_size={len(self.cpu_cache)}, cursor={self._cpu_pf_cursor}")

    def _ensure_cpu_ring_window(self, cur_layer: int):
        if not self.ssd_enabled:
            return
        # é”šç‚¹ï¼ši + offset
        anchor = (cur_layer + self.cpu_ring_offset) % self.n_layers
        target = self._ring_range(anchor, self.cpu_cache_cap)
        # æ‰“å°æ›´æ¸…æ¥šçš„æ—¥å¿—
        print(f"[WSM][CPU-RING] cur={cur_layer} need={target[:min(len(target), 30)]} ... "
            f"| cap={self.cpu_cache_cap}, n={self.n_layers}")

        # é€å‡ºä¸åœ¨ target é‡Œçš„æ—§å±‚
        for L in list(self.cpu_cache.keys()):
            if L not in target:
                with self.cpu_cache_lock:
                    self.cpu_cache.pop(L, None)
                if L in self._cpu_lru: self._cpu_lru.remove(L)
                self._cpu_cached_layers.discard(L)

        # æŒ‰ç¯å½¢é¡ºåºè¡¥é½ç¼ºå¤±
        for L in target:
            if L in self.cpu_cache:
                # æ›´æ–° LRU
                if L in self._cpu_lru: self._cpu_lru.remove(L)
                self._cpu_lru.append(L)
                continue
            # å®¹é‡æ»¡åˆ™è¸¢æœ€è€
            while len(self.cpu_cache) >= self.cpu_cache_cap:
                ev = self._cpu_lru.pop(0)
                with self.cpu_cache_lock:
                    self.cpu_cache.pop(ev, None)
                self._cpu_cached_layers.discard(ev)
                if self.verbose:
                    print(f"[WSM] CPU cache evict (ring): layer {ev}")
            # åªåŠ è½½æµå¼å‚æ•°ï¼ˆSSDâ†’CPUï¼‰
            self._load_layer_to_cpu(L)

    # åœ¨ WeightStreamingManager ç±»ä¸­æ–°å¢ï¼š
    def _evict_if_over_hwm_locked(self, incoming: int = 0) -> None:
        """
        åœ¨æŒæœ‰ self._cpu_lock çš„å‰æä¸‹è°ƒç”¨ï¼š
        å½“ (å½“å‰å·²ç¼“å­˜å±‚æ•° + incoming) è¶…è¿‡ CPU é«˜æ°´ä½(HWM) æ—¶ï¼Œ
        ä»…é€å‡ºâ€œçª—å£å¤–â€çš„ LRU å±‚ï¼Œç›´åˆ°å®¹é‡å›åˆ° LWM æˆ–æ— å¯é€å‡ºé¡¹ã€‚
        """
        # è‹¥æœªå¯ç”¨ SSD åç«¯æˆ–æ²¡æœ‰æ°´ä½é…ç½®ï¼Œç›´æ¥è¿”å›
        if not getattr(self, "ssd_enabled", False):
            return

        # å½“å‰çª—å£èŒƒå›´
        L0, L1 = self._target_cpu_window()  # e.g. [base, base+cap-1]

        # ç›®æ ‡å®¹é‡ï¼šä¼˜å…ˆæŠŠ (ç°æœ‰ + incoming) å‹å›åˆ° LWM
        cur = len(self.cpu_cache)
        if (cur + incoming) <= self.cpu_hwm:
            return  # ä½äº HWMï¼Œæ— éœ€å¤„ç†

        target_max = max(self.cpu_lwm, 0)
        # ä¼˜å…ˆé€å‡ºçª—å£å¤–çš„å±‚ï¼ˆä» LRU æœ€è€å¼€å§‹ï¼‰
        evicted = 0
        i = 0
        while (len(self.cpu_cache) + incoming) > target_max and i < len(self._cpu_lru):
            lyr = self._cpu_lru[i]
            # åªè¸¢çª—å£å¤–ï¼Œçª—å£å†…ä¿ç•™ï¼Œé¿å…æŠ–åŠ¨
            if lyr < L0 or lyr > L1:
                # çœŸæ­£é€å‡º
                self.cpu_cache.pop(lyr, None)
                self._cpu_lru.pop(i)           # ç§»é™¤è¯¥å±‚çš„ LRU è®°å½•
                self._cpu_cached_layers.discard(lyr)
                evicted += 1
                # ä¸é€’å¢ iï¼Œå› ä¸ºæˆ‘ä»¬ç§»é™¤äº†å½“å‰ä½ç½®ï¼›ç»§ç»­æ£€æŸ¥æ–°çš„ i
            else:
                i += 1

        if evicted and getattr(self, "verbose", False):
            print(f"[WSM] CPU cache evict (HWM): evicted={evicted}, "
                f"after={len(self.cpu_cache)} (win=[{L0},{L1}], lwm={self.cpu_lwm}, hwm={self.cpu_hwm})")


    def _evict_cpu_layers(self, k: int):
        """
        ä¼˜å…ˆè¸¢çª—å£å¤–å±‚ï¼›è‹¥ä¸å¾—ä¸è¸¢çª—å£å†…ï¼Œåˆ™åŒæ­¥å¹³ç§»çª—å£å¹¶åˆ—å‡ºå³ç«¯ must-fetch
        """
        L0, L1 = self._target_cpu_window()
        evicted = 0
        must_fetch = 0
        i = 0

        if self.verbose:
            print(f"[WSM] Evicting {k} CPU layers, window=[{L0}, {L1}], cache_size={len(self.cpu_cache)}")

        # Phase 1: ä¼˜å…ˆè¸¢çª—å£å¤–çš„å±‚ï¼ˆLRU é¡ºåºï¼‰
        while evicted < k and i < len(self._cpu_lru):
            L = self._cpu_lru[i]
            # çª—å£å¤– â†’ å¯ä»¥è¸¢
            if L < L0 or L > L1:
                with self.cpu_cache_lock:
                    self.cpu_cache.pop(L, None)
                self._cpu_lru.pop(i)
                self._cpu_cached_layers.discard(L)
                evicted += 1
                if self.verbose:
                    print(f"[WSM] CPU cache evict (out of window): layer {L}")
                continue
            # çª—å£å†… â†’ æš‚è·³è¿‡ï¼ˆé¿å… thrashï¼‰
            i += 1

        # Phase 2: å¦‚æœè¿˜ä¸å¤Ÿï¼Œå¹³ç§»çª—å£ï¼ˆå³ç§» d å±‚ï¼‰ï¼Œå¹¶æŠŠå³ç«¯ d å±‚æ ‡è®°ä¸º must-fetch
        if evicted < k:
            d = min(k - evicted, self.cpu_cache_cap)  # æœ€å¤šå¹³ç§»ä¸€ä¸ªçª—å£å®½
            self.cpu_win_base += d
            must_fetch = d
            if self.verbose:
                print(f"[WSM] Window shift: base {self.cpu_win_base - d} -> {self.cpu_win_base}, must_fetch={must_fetch}")

        # å¹³ç§»åç¡®ä¿çª—å£ï¼ˆå« must-fetchï¼‰
        if must_fetch > 0:
            self._ensure_cpu_window()

    def _start_prefetch_worker(self):
        """Start background thread for SSD->CPU prefetching"""
        def prefetch_worker():
            # Initial preload: schedule first target_cpu_layers for CPU cache
            if self.ssd_enabled:
                initial_layers = list(range(min(self.target_cpu_layers, len(self.blocks))))
                with self.prefetch_lock:
                    self.prefetch_queue.extend(initial_layers)
                if self.verbose:
                    print(f"[WSM] Scheduled initial CPU preload for layers: {initial_layers}")

            while not self.stop_prefetch.is_set():
                layer_to_prefetch = None

                with self.prefetch_lock:
                    if self.prefetch_queue:
                        layer_to_prefetch = self.prefetch_queue.pop(0)

                if layer_to_prefetch is not None:
                    try:
                        self._load_layer_to_cpu(layer_to_prefetch)
                    except Exception as e:
                        if self.verbose:
                            print(f"[WSM] CPU prefetch failed for layer {layer_to_prefetch}: {e}")

                time.sleep(0.001)  # Small delay to avoid CPU spinning

        self.prefetch_thread = threading.Thread(target=prefetch_worker, daemon=True)
        self.prefetch_thread.start()

    # NOTE: _shrink_cpu_cache_if_needed å·²è¢« _evict_cpu_layers æ›¿ä»£ï¼ˆæ»‘åŠ¨çª—å£æ¨¡å¼ï¼‰

    def _load_layer_to_cpu(self, layer_idx: int):
        """Load layer weights from SSD to CPU cache"""
        print(f"[WSM DEBUG] _load_layer_to_cpu called for layer {layer_idx}, ssd_enabled={self.ssd_enabled}, in_cache={layer_idx in self.cpu_cache}")
        if not self.ssd_enabled or layer_idx in self.cpu_cache:
            print(f"[WSM DEBUG] _load_layer_to_cpu skipping layer {layer_idx}: ssd_enabled={self.ssd_enabled}, in_cache={layer_idx in self.cpu_cache}")
            return

        # â˜… ä¿®å¤ 5: å»é‡ - æ£€æŸ¥æ˜¯å¦å·²åœ¨åŠ è½½ä¸­
        with self._inflight_lock:
            if layer_idx in self._inflight_cpu_layers:
                if self.verbose:
                    print(f"[WSM] Layer {layer_idx} already inflight to CPU, skipping duplicate load")
                return
            # æ ‡è®°ä¸ºåŠ è½½ä¸­
            self._inflight_cpu_layers.add(layer_idx)

        nvtx.range_push(f"ssd_to_cpu_layer_{layer_idx}")

        with self.cpu_cache_lock:
            # Double-check after acquiring lock
            if layer_idx in self.cpu_cache:
                nvtx.range_pop()
                return

            # Manage CPU cache size
            while len(self.cpu_cache) >= self.cpu_cache_layers:
                old_layer, _ = self.cpu_cache.popitem(last=False)
                if self.verbose:
                    print(f"[WSM] Evicted layer {old_layer} from CPU cache")

            if layer_idx not in self.layers_params:
                print(f"[WSM ERROR] Layer {layer_idx} not found in layers_params! Available layers: {list(self.layers_params.keys())[:10]}")
                nvtx.range_pop()
                return

            print(f"[WSM DEBUG] Loading layer {layer_idx} from SSD: {len(self.layers_params[layer_idx])} params in manifest")
            layer_weights = {}

            # Load stream weights for this layer
            # Use a set to track loaded params to avoid duplicates
            loaded_params = set()

            stream_count = sum(1 for p in self.layers_params[layer_idx] if p["policy"] == "stream")
            print(f"[WSM DEBUG] Layer {layer_idx}: {stream_count} stream params to load")

            for param_info in self.layers_params[layer_idx]:
                if param_info["policy"] != "stream":
                    continue

                try:
                    # Import here to avoid circular imports
                    from .weights_io_ssd_dram import DTYPE_MAP

                    param_name = param_info["name"]

                    # Skip if already loaded (manifest may have duplicates)
                    if param_name in loaded_params:
                        continue
                    loaded_params.add(param_name)

                    stride = param_info["stride"]
                    offset = param_info["offset"]
                    nbytes = param_info["nbytes"]

                    # Ensure staging buffer is large enough
                    if stride > len(self.staging_buffer):
                        from .weights_io_ssd_dram import alloc_pinned_aligned
                        block_size = self.ssd_manifest["block_size"]
                        new_size = ((stride + block_size - 1) // block_size) * block_size
                        self.staging_buffer = alloc_pinned_aligned(new_size, block_size)
                        if self.verbose:
                            print(f"[WSM] Expanded staging buffer to {new_size} bytes")

                    # Read from SSD to staging buffer
                    self.ssd_dio.pread_into_tensor(self.staging_buffer, stride, offset)

                    # Convert to proper tensor format
                    param_tensor = torch.empty(
                        param_info["shape"],
                        dtype=DTYPE_MAP[param_info["dtype"]],
                        pin_memory=True
                    )
                    param_tensor.view(-1).view(torch.uint8)[:nbytes].copy_(
                        self.staging_buffer[:nbytes]
                    )

                    layer_weights[param_name] = param_tensor
                    print(f"[WSM DEBUG] âœ“ Loaded {param_name}: {param_tensor.shape} {param_tensor.dtype}")

                except Exception as e:
                    print(f"[WSM ERROR] âœ— Failed to load {param_info.get('name', 'unknown')}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue

            if layer_weights:
                self.cpu_cache[layer_idx] = layer_weights
                # æ›´æ–° LRU è·Ÿè¸ª
                self._cpu_cached_layers.add(layer_idx)
                if layer_idx in self._cpu_lru:
                    self._cpu_lru.remove(layer_idx)
                self._cpu_lru.append(layer_idx)
                print(f"[WSM] âœ… Loaded layer {layer_idx} to CPU cache ({len(layer_weights)} params)")

                # NOTE: æ”¶ç¼©æ£€æŸ¥å·²ç§»è‡³ _ensure_cpu_windowï¼Œæ­¤å¤„ä¸å†éœ€è¦
            else:
                print(f"[WSM ERROR] âŒ Failed to load ANY weights for layer {layer_idx}!")
                print(f"[WSM ERROR]    Total params in manifest: {len(self.layers_params[layer_idx])}")
                print(f"[WSM ERROR]    Stream params: {stream_count}")
                print(f"[WSM ERROR]    Successfully loaded: 0")

        nvtx.range_pop()

        # â˜… ä¿®å¤ 5: ç§»é™¤ inflight æ ‡è®°
        with self._inflight_lock:
            self._inflight_cpu_layers.discard(layer_idx)

    def _check_dram_capacity(self):
        """Check if there's enough DRAM to cache the required weights"""
        if not self.ssd_enabled:
            return

        # Calculate total weight size for stream parameters
        total_stream_bytes = 0
        stream_layers_count = 0

        for layer_id, params in self.layers_params.items():
            layer_bytes = 0
            for param_info in params:
                if param_info["policy"] == "stream":
                    layer_bytes += param_info["nbytes"]

            if layer_bytes > 0:
                total_stream_bytes += layer_bytes
                stream_layers_count += 1

        # Calculate required DRAM for CPU cache
        if stream_layers_count > 0:
            avg_layer_size = total_stream_bytes / stream_layers_count
            required_cache_bytes = avg_layer_size * self.cpu_cache_layers
        else:
            required_cache_bytes = 0

        # Get current system memory info
        memory = psutil.virtual_memory()
        available_bytes = memory.available
        total_bytes = memory.total

        # Convert to human readable
        required_gb = required_cache_bytes / (1024**3)
        available_gb = available_bytes / (1024**3)
        total_gb = total_bytes / (1024**3)

        print(f"ğŸ’¾ DRAM Capacity Check:")
        print(f"   Total system DRAM: {total_gb:.1f} GB")
        print(f"   Available DRAM: {available_gb:.1f} GB")
        print(f"   Required for {self.cpu_cache_layers} layer cache: {required_gb:.1f} GB")
        print(f"   Total stream layers: {stream_layers_count}")
        print(f"   Average layer size: {avg_layer_size/(1024**3):.2f} GB")

        # Check if we have enough memory with safety margin
        safety_margin = 0.25  # align with KVOffloader
        # peak concurrent: current H2D + one next prefetch layer
        peak_concurrent_bytes = avg_layer_size * 2 if stream_layers_count > 0 else 0
        required_with_margin = (required_cache_bytes + peak_concurrent_bytes) * (1 + safety_margin)

        if required_with_margin > available_bytes:
            deficit_gb = (required_with_margin - available_bytes) / (1024**3)
            print(f"âŒ INSUFFICIENT DRAM!")
            print(f"   Deficit: {deficit_gb:.1f} GB (including 10% safety margin)")
            print(f"   Suggestion: Reduce cpu_cache_layers from {self.cpu_cache_layers} to {int(self.cpu_cache_layers * available_bytes / required_with_margin)}")
            raise RuntimeError(f"Insufficient DRAM: need {required_gb:.1f}GB but only {available_gb:.1f}GB available")

        print(f"âœ… DRAM capacity sufficient (margin: {(available_bytes - required_cache_bytes)/(1024**3):.1f} GB)")

    def _validate_no_prefetch_mode(self):
        """Validate that all weights can fit in DRAM when prefetch is disabled"""
        if not self.ssd_enabled:
            return

        # Calculate total size of all stream weights
        total_stream_bytes = 0
        for layer_id, params in self.layers_params.items():
            for param_info in params:
                if param_info["policy"] == "stream":
                    total_stream_bytes += param_info["nbytes"]

        # Get available memory
        memory = psutil.virtual_memory()
        available_bytes = memory.available

        total_gb = total_stream_bytes / (1024**3)
        available_gb = available_bytes / (1024**3)

        print(f"ğŸ” No-Prefetch Mode Validation:")
        print(f"   Total stream weights: {total_gb:.1f} GB")
        print(f"   Available DRAM: {available_gb:.1f} GB")

        # Check if all weights fit with safety margin
        safety_margin = 0.15  # 15% safety margin for no-prefetch mode
        required_with_margin = total_stream_bytes * (1 + safety_margin)

        if required_with_margin > available_bytes:
            deficit_gb = (required_with_margin - available_bytes) / (1024**3)
            print(f"âŒ CANNOT RUN WITHOUT PREFETCH!")
            print(f"   All weights ({total_gb:.1f} GB) cannot fit in available DRAM ({available_gb:.1f} GB)")
            print(f"   Deficit: {deficit_gb:.1f} GB (including 15% safety margin)")
            print(f"ğŸ’¡ Solutions:")
            print(f"   1. Enable prefetch mode: set prefetch_distance > 0")
            print(f"   2. Reduce cpu_cache_layers to enable streaming")
            print(f"   3. Add more DRAM to your system")
            raise RuntimeError(f"Cannot run without prefetch: need {total_gb:.1f}GB but only {available_gb:.1f}GB available")

        print(f"âœ… All weights fit in DRAM - no prefetch mode validated")
        # Set CPU cache to hold all layers since we're not using prefetch
        self.cpu_cache_layers = len(self.layers_params)
        print(f"ğŸ“ Updated CPU cache to hold all {self.cpu_cache_layers} layers")


    def note_compute_advance(self, cur_layer: int):
        """
        ç”± EncoderBlock.forward åœ¨æ¯å±‚å…¥å£è°ƒç”¨ã€‚
        æ»šåŠ¨æ¨¡å¼ä¸‹ï¼šæŠŠ CPU çª—å£å³ç§»ï¼ˆé»˜è®¤ 1 å±‚ï¼‰ï¼Œä»è€Œï¼š
        - é€å‡ºçª—å£å·¦ç«¯ï¼ˆä¾‹å¦‚åˆšåˆšè¿›å…¥è®¡ç®—çš„å±‚ï¼‰
        - æŠŠçª—å£å³ç«¯çš„æ–°å±‚ä» SSDâ†’DRAM é¢„å–è¿›æ¥
        """
        # æ²¡å¼€ SSD åç«¯æˆ–æ²¡å¯ç”¨æ»šåŠ¨ï¼Œå°±ä¸åšäº‹ï¼Œä»æ›´æ–°ä¿ç•™çª—å£ç”¨äº GPU LRU
        self._refresh_retain_window(cur_layer)

        if not self.ssd_enabled or not self.cpu_rolling_mode:
            return

        # å…è®¸çš„ window åŸºå‡†èŒƒå›´ï¼š[0 .. max_base]
        max_base = max(0, self.n_layers - self.cpu_cache_cap)
        # ç›®æ ‡åŸºå‡†ï¼šå½“å‰å±‚åä¸€æ ¼ï¼ˆæˆ– strideï¼‰
        next_base = cur_layer + self.cpu_roll_stride

        if self.cpu_wrap_around:
            # åˆ°äº†æœ€å³ä¾§çª—å£ [max_base, n-1] çš„æœ€å³ç«¯ï¼Œå†â€œæ–°çš„ä¸€è½®â€ä¼šå›åˆ° 0
            if next_base > max_base:
                next_base = 0
        else:
            next_base = min(next_base, max_base)

        if next_base == self.cpu_win_base:
            return  # çª—å£æ²¡å˜åŒ–ï¼Œé¿å…æ— æ„ä¹‰å·¥ä½œ

        old_base = self.cpu_win_base
        self.cpu_win_base = next_base
        if self.verbose:
            print(f"[WSM DEBUG] note_compute_advance: base {old_base} -> {self.cpu_win_base} (cur={cur_layer})")

        if self.cpu_roll_sync:
            # ç®€å•å¯é ï¼šç«‹åˆ»ç¡®ä¿æ–°çª—å£ï¼Œ**åŒæ­¥**è§¦å‘â€œå·¦ç«¯é€å‡º + å³ç«¯åŠ è½½â€
            self._ensure_cpu_window()
        else:
            # ä½é˜»å¡ç‰ˆæœ¬ï¼šåªæ¨è¿› epoch + æŠ•é€’åå°åŠ è½½ï¼Œç”±çº¿ç¨‹ _cpu_prefetch_worker è´Ÿè´£ I/O
            self._advance_cpu_window_by_compute(cur_layer)



    def _advance_cpu_window_by_compute(self, cur_layer: int):
        """
        ä»…ç”±â€œè®¡ç®—çº¿ç¨‹â€è°ƒç”¨ï¼šæ¨è¿› CPU é¢„å–çª—å£å¹¶æŠŠç¼ºå¤±å±‚å…¥é˜Ÿã€‚
        ä¸åšä»»ä½•åŒæ­¥ IOï¼ˆä¸ç›´æ¥ _load_layer_to_cpuï¼‰ã€‚
        """
        with self._cpu_lock:
            # åªå‰ç§»ï¼Œä¸åé€€ï¼›ä¿ç•™ä¸€ç‚¹å†å²å¯ç”±ä½ ç°æœ‰é€»è¾‘å†³å®š
            # è¿™é‡Œç®€å• bump epochï¼Œè®©è¿‡æœŸçš„é¢„å–ä»»åŠ¡è‡ªåŠ¨è¢«ä¸¢å¼ƒ
            self._epoch += 1

            
            L0 = self.cpu_win_base
            L1 = min(self.n_layers - 1, L0 + self.cpu_cache_cap - 1)
            missing = [L for L in range(L0, L1 + 1)
                    if 0 <= L < self.n_layers and (L not in self.cpu_cache) and (L not in self._inflight_cpu_layers)]

            for L in missing:
                self._inflight_cpu_layers.add(L)
                self._cpu_pf_q.put((self._epoch, L))



    def _schedule_cpu_prefetch(self, current_layer: int):
        """
        æ»‘åŠ¨çª—å£é¢„å–ï¼šåªåœ¨å½“å‰å±‚æ¥è¿‘çª—å£æœ«å°¾æ—¶æ¨è¿›çª—å£
        â˜… å…³é”®ä¿®å¤: çª—å£åº”è¯¥å¹³æ»‘æ»‘åŠ¨ï¼Œè€Œä¸æ˜¯è·³è·ƒå¼æ¨è¿›
        """
        if not self.ssd_enabled:
            return

        if getattr(self, "cpu_ring_mode", False):
                # ç¯å½¢çª—å£ï¼šåœ¨ cur+offset èµ·çš„ç¯ä¸Šå– cpu_cache_cap ä¸ªå±‚
                self._ensure_cpu_ring_window(current_layer)
                
        else:

            L0, L1 = self._target_cpu_window()

            # â˜… ä¿®å¤: åªåœ¨å½“å‰å±‚è¶…å‡ºçª—å£æˆ–æ¥è¿‘æœ«å°¾æ—¶ï¼Œæ¨è¿›çª—å£ 1 å±‚
            # è¿™æ ·çª—å£ä¼šå¹³æ»‘æ»‘åŠ¨ï¼š[0,49] â†’ [1,50] â†’ [2,51] â†’ ...
            if current_layer > L1 or (current_layer >= L1 - 5):
                # è®¡ç®—æ–°çš„çª—å£åŸºå‡†ï¼šç¡®ä¿å½“å‰å±‚åœ¨çª—å£å†…ï¼Œä½†åªæ¨è¿›å¿…è¦çš„é‡
                if current_layer > L1:
                    # å½“å‰å±‚å·²è¶…å‡ºçª—å£ï¼Œæ¨è¿›åˆ°åˆšå¥½åŒ…å«å½“å‰å±‚
                    new_base = current_layer - self.cpu_cache_cap + 1
                elif current_layer >= L1 - 5:
                    # å½“å‰å±‚æ¥è¿‘çª—å£æœ«å°¾ï¼Œæ¨è¿› 1 å±‚
                    new_base = self.cpu_win_base + 1
                else:
                    new_base = self.cpu_win_base

                new_base = max(0, new_base)

                if new_base > self.cpu_win_base:
                    print(f"[WSM DEBUG] Layer {current_layer} near/beyond window end {L1}, advancing base {self.cpu_win_base} -> {new_base}")
                    self.cpu_win_base = new_base

            # ç¡®ä¿çª—å£å†…çš„å±‚éƒ½å·²åŠ è½½
            self._ensure_cpu_window()

    def _touch_cpu_layer(self, layer_idx: int):
        """
        æ ‡è®°æŸå±‚æœ€è¿‘è¢«ä½¿ç”¨ï¼Œæ›´æ–° LRU é˜Ÿåˆ—
        åº”åœ¨å±‚çš„ forward å¼€å§‹æ—¶è°ƒç”¨ï¼ˆMHA æˆ– FFN å…¥å£ï¼‰
        """
        if layer_idx in self._cpu_lru:
            self._cpu_lru.remove(layer_idx)
            self._cpu_lru.append(layer_idx)

    def wait_for_preload_ready(self, timeout: float = 300.0) -> bool:
        """
        ç­‰å¾…é¢„åŠ è½½å®Œæˆï¼šGPUæœ‰target_gpu_layerså±‚ï¼ŒCPUæœ‰target_cpu_layerså±‚
        â˜… ä¿®å¤ 9: æ”¯æŒè·³è¿‡ç­‰å¾…ï¼Œå…è®¸è¾¹è·‘è¾¹æ»šåŠ¨é¢„å–

        Args:
            timeout: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            bool: æ˜¯å¦åœ¨è¶…æ—¶å‰å®Œæˆé¢„åŠ è½½
        """
        import time

        # â˜… ä¿®å¤ 9: æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼Œå…è®¸è·³è¿‡é¢„åŠ è½½ç­‰å¾…
        skip_wait = os.getenv("WSM_SKIP_PRELOAD_WAIT", "0") == "1"
        if skip_wait:
            if self.verbose:
                print("[WSM] âš¡ WSM_SKIP_PRELOAD_WAIT=1: Skipping preload wait, will prefetch on-the-fly")
            return True

        # ä»ç¯å¢ƒå˜é‡è¯»å– timeoutï¼ˆå…è®¸ç¼©çŸ­ï¼‰
        timeout_override = os.getenv("WSM_PRELOAD_TIMEOUT")
        if timeout_override is not None:
            try:
                timeout = float(timeout_override)
                if self.verbose:
                    print(f"[WSM] Using custom timeout from WSM_PRELOAD_TIMEOUT: {timeout}s")
            except ValueError:
                pass

        if self.verbose:
            print(f"[WSM] Waiting for preload: target GPU={self.target_gpu_layers}, target CPU={self.target_cpu_layers}, timeout={timeout}s")

        start_time = time.time()

        while time.time() - start_time < timeout:
            gpu_ready = len(self.gpu_cache) >= self.target_gpu_layers
            
            if getattr(self, "grouped_mode", False):
                # ç›®æ ‡ç»„æ•°ï¼šæ¯å±‚ 2 ç»„ï¼ˆattn+ffnï¼‰ï¼Œå—ç»„é¢„ç®—ä¸Šé™çº¦æŸï¼›è‡³å°‘éœ€è¦ 1 ç»„å³å¯å¼€è·‘
                group_target = min(max(1, 2 * max(1, self.target_gpu_layers)), self.gpu_max_groups)
                group_ready  = len(self._gpu_group_lru) >= group_target or ((0, "attn") in self._gpu_group_lru)
                gpu_ready = gpu_ready or group_ready
            
            
            cpu_ready = len(self.cpu_cache) >= self.target_cpu_layers if self.ssd_enabled else True

            if gpu_ready and cpu_ready:
                if self.verbose:
                    print(f"[WSM] Preload completed: {len(self.gpu_cache)} GPU layers + {len(self.cpu_cache)} CPU layers ready")
                self.gpu_preload_complete.set()
                self.cpu_preload_complete.set()
                return True

            if self.verbose and int(time.time() - start_time) % 5 == 0:  # Progress update every 5 seconds
                print(f"[WSM] Preload progress: GPU {len(self.gpu_cache)}/{self.target_gpu_layers}, CPU {len(self.cpu_cache)}/{self.target_cpu_layers}")

            time.sleep(0.1)

        # â˜… ä¿®å¤ 9: è¶…æ—¶æ—¶ç»™å‡ºå»ºè®®
        print(f"[WSM] âš ï¸  Preload timeout after {timeout}s: GPU {len(self.gpu_cache)}/{self.target_gpu_layers}, CPU {len(self.cpu_cache)}/{self.target_cpu_layers}")
        print(f"[WSM] ğŸ’¡ Tip: Set WSM_SKIP_PRELOAD_WAIT=1 to skip waiting and prefetch on-the-fly")
        print(f"[WSM] ğŸ’¡ Or set WSM_PRELOAD_TIMEOUT=<seconds> to adjust timeout")
        return False

    # -------- CPU/GPU movement primitives --------

    def _setup_resident_norms(self):
        """
        Move norm modules to GPU and exclude them from streaming/eviction.
        â˜… ä¿®å¤ 7: éµå®ˆé¢„ç®—ä¸Šé™ï¼Œé˜²æ­¢ç¢ç‰‡å’Œ OOM
        """
        if self.verbose:
            print("[WSM] Setting up resident norm layers...")
            print(f"[WSM] Resident budget: {self.resident_budget_gb:.1f} GB, max {self.resident_max_modules} modules")

        norm_count = 0
        skipped_count = 0
        budget_bytes = int(self.resident_budget_gb * 1024**3)

        for layer_id, modules_dict in self.block_mods.items():
            for module_name, module in modules_dict.items():
                if module_name.startswith("norm_"):
                    # æ£€æŸ¥é¢„ç®—
                    if norm_count >= self.resident_max_modules:
                        skipped_count += 1
                        if self.verbose and skipped_count <= 5:
                            print(f"[WSM] Skipping {module_name} (layer {layer_id}): max modules reached")
                        continue

                    # è®¡ç®—æ¨¡å—å¤§å°
                    module_bytes = sum(
                        p.numel() * p.element_size()
                        for p in module.parameters()
                    )

                    if self._resident_bytes_used + module_bytes > budget_bytes:
                        skipped_count += 1
                        if self.verbose and skipped_count <= 5:
                            print(f"[WSM] Skipping {module_name} (layer {layer_id}): budget exhausted "
                                  f"({self._resident_bytes_used/(1024**3):.2f}/{self.resident_budget_gb:.1f} GB)")
                        continue

                    try:
                        module.to(self.device)
                        self._resident_bytes_used += module_bytes
                        norm_count += 1
                        if self.verbose and norm_count <= 10:
                            print(f"[WSM] Layer {layer_id}: {module_name} -> {self.device} "
                                  f"({module_bytes/(1024**2):.1f} MB, resident)")
                    except Exception as e:
                        if self.verbose:
                            print(f"[WSM] Warning: failed to move {module_name} to {self.device}: {e}")

        if self.verbose:
            print(f"[WSM] {norm_count} norm modules set as GPU resident "
                  f"({self._resident_bytes_used/(1024**3):.2f} GB used)")
            if skipped_count > 0:
                print(f"[WSM] {skipped_count} norm modules kept in DRAM (budget limit)")

    def _check_and_throttle_kv(self):
        """
        â˜… ä¿®å¤ 8: æ£€æŸ¥ weight_h2d backlogï¼Œå¿…è¦æ—¶ throttle KV I/O
        é¿å… KV æŠ¢å¸¦å®½å¯¼è‡´æƒé‡è¿Ÿè¿Ÿä¸ä¸Šæ¥
        """
        # æ£€æŸ¥ weight_h2d stream æ˜¯å¦ç¹å¿™
        h2d_stream = getattr(self.streams, "weight_h2d", None) if hasattr(self, "streams") else None
        if h2d_stream is None:
            return

        try:
            # æ£€æŸ¥ stream æ˜¯å¦å®Œæˆï¼ˆFalse = è¿˜åœ¨å¿™ï¼‰
            is_busy = not h2d_stream.query()
        except Exception:
            is_busy = False

        if is_busy:
            self._h2d_pending_count += 1
        else:
            self._h2d_pending_count = max(0, self._h2d_pending_count - 1)

        # å¦‚æœ backlog è¶…è¿‡é˜ˆå€¼ï¼Œthrottle KV
        if self._h2d_pending_count >= self.kv_throttle_threshold:
            current_time = time.time()
            # é¿å…è¿‡äºé¢‘ç¹çš„ throttleï¼ˆè‡³å°‘é—´éš” 100msï¼‰
            if current_time - self._last_kv_throttle_time > 0.1:
                if self.kv_offloader is not None and hasattr(self.kv_offloader, "throttle_writes_for"):
                    try:
                        self.kv_offloader.throttle_writes_for(self.kv_throttle_ms)
                        if self.verbose:
                            print(f"[WSM] Throttled KV writes for {self.kv_throttle_ms}ms "
                                  f"(H2D backlog: {self._h2d_pending_count})")
                        self._last_kv_throttle_time = current_time
                    except Exception as e:
                        if self.verbose:
                            print(f"[WSM] KV throttle failed: {e}")

    def _record_layer_ready_event(self, idx: int):
        """Record an event on the weight H2D stream marking all enqueued copies for this layer."""
        if not torch.cuda.is_available() or getattr(self.streams, "weight_h2d", None) is None:
            return
        evt = self.layer_events.get(idx)
        if evt is None:
            evt = torch.cuda.Event(blocking=False)
            self.layer_events[idx] = evt
        evt.record(self.streams.weight_h2d)

        # â˜… ä¿®å¤ 8: æ£€æŸ¥å¹¶ throttle KV
        self._check_and_throttle_kv()

        # è®°å½•ä¸€æ¬¡ H2D æ´»åŠ¨æ—¶é•¿ï¼Œç”¨äº PCIE è¿‘ä¼¼å ç”¨åº¦ä¼°è®¡ï¼ˆEMAï¼‰
        try:
            start = torch.cuda.Event(enable_timing=True)
            end   = torch.cuda.Event(enable_timing=True)
            start.record(self.streams.weight_h2d)
            end.record(self.streams.weight_h2d)
            end.synchronize()
            dt_ms = start.elapsed_time(end)  # ~0ï¼Œä½†å¯ä»¥ä½œä¸ºä¸€æ¬¡é‡‡æ ·
            self._last_h2d_ms = max(self._last_h2d_ms * 0.9, dt_ms)
        except Exception:
            pass

        # Light-weight GC of event pool to avoid pending growth
        try:
            from . import stream_mnt
            stream_mnt._get_event_pool(self.device).gc(limit=512)
        except Exception:
            pass


    def _wait_layer_ready(self, idx: int):
        """Wait on the layer's ready event on the current stream (fallback: wait for H2D stream)."""
        evt = self.layer_events.get(idx)
        if self.verbose:
            print(f"[WSM] _wait_layer_ready layer={idx} event={'exists' if evt else 'None'}")
        if evt is not None:
            if self.verbose:
                print(f"[WSM] _wait_layer_ready layer={idx} waiting on event...")
            torch.cuda.current_stream(self.device).wait_event(evt)
            if self.verbose:
                print(f"[WSM] _wait_layer_ready layer={idx} event wait done")
        else:
            if self.verbose:
                print(f"[WSM] _wait_layer_ready layer={idx} fallback to wait_weight_ready_on_current...")
            self.streams.wait_weight_ready_on_current(self.device)
            if self.verbose:
                print(f"[WSM] _wait_layer_ready layer={idx} fallback done")

    def _ensure_param_cpu_stash_inplace(self, p: torch.nn.Parameter):
        """
        Replace p.data with a pinned CPU master copy and stash it by Parameter id.
        This avoids holding duplicate CPU tensors.

        WARNING: This should NOT be called on meta tensors! They should be handled by SSD backend.
        """
        pid = id(p)
        if pid in self.cpu_stash:
            return

        # CPU stash å·²åºŸå¼ƒï¼Œä¸å†ä½¿ç”¨ï¼ˆCPU stub æ–¹æ¡ˆä¸éœ€è¦ stashï¼‰
        # ä¿ç•™ç©ºå®ç°ï¼Œé¿å…ç ´åç°æœ‰è°ƒç”¨
        pass

    def _ensure_param_on_gpu(self, p: torch.nn.Parameter, layer_idx: Optional[int] = None, param_name: Optional[str] = None):
        """
        ç¡®ä¿å‚æ•°åœ¨ GPU ä¸Šï¼ˆä» CPU cache åŠ è½½ï¼‰
        ç§»é™¤äº† meta device æ”¯æŒï¼Œå‚æ•°è¦ä¹ˆæ˜¯ 0-size CPU stubï¼Œè¦ä¹ˆå·²åœ¨ GPU
        """
        nvtx.range_push("param_h2d")

        # å·²ç»åœ¨ GPU ä¸Šä¸”é stubï¼šç›´æ¥è¿”å›
        if p.is_cuda and p.numel() > 0:
             nvtx.range_pop()
             return

        # CPU stub æˆ– CPU å‚æ•°ï¼šä» SSD/CPU cache åŠ è½½
        if self.ssd_enabled:
            # ç¡®ä¿è¯¥å±‚å·²åŠ è½½åˆ° CPU cache
            if layer_idx is not None and layer_idx not in self.cpu_cache:
                 # æ£€æŸ¥æ˜¯å¦æ­£åœ¨åŠ è½½ä¸­
                with self._inflight_lock:
                    is_inflight = layer_idx in self._inflight_cpu_layers

                if is_inflight:
                    # æ­£åœ¨è¢«åå°çº¿ç¨‹åŠ è½½ï¼Œç­‰å¾…å®Œæˆ
                    try:
                        self._wait_cpu_ready(layer_idx, timeout=10.0)
                    except Exception as e:
                        if self.verbose:
                            print(f"[WSM] Timeout waiting for layer {layer_idx} to load: {e}")
                        nvtx.range_pop()
                        return
                else:
                    # ä¸åœ¨åŠ è½½ä¸­ï¼Œä¹Ÿä¸åœ¨ç¼“å­˜ä¸­ï¼Œéœ€è¦ç«‹å³åŠ è½½
                    try:
                        self._load_layer_to_cpu(layer_idx)
                    except Exception as e:
                        if self.verbose:
                            print(f"[WSM] Failed to load layer {layer_idx} to CPU cache: {e}")
                        nvtx.range_pop()
                        return

            # ä» CPU cache åŠ è½½åˆ° GPU
            if (layer_idx is not None and param_name is not None and
                layer_idx in self.cpu_cache and param_name in self.cpu_cache[layer_idx]):

                nvtx.range_push("cpu_cache_to_gpu")
                with torch.cuda.stream(self.streams.weight_h2d):
                    cached_tensor = self.cpu_cache[layer_idx][param_name]
                    p_gpu = cached_tensor.to(self.device, non_blocking=True)

                # ç›´æ¥æ›¿æ¢ param.dataï¼ˆæ— éœ€ meta æ£€æŸ¥ï¼‰
                p.data = p_gpu

                nvtx.range_pop()
                nvtx.range_pop()
                return
            else:
                # æ— æ³•ä» CPU cache è·å–ï¼Œè¯´æ˜è¿™æ˜¯ä¸ª resident å‚æ•°ï¼Œåº”è¯¥å·²ç»è¢«åŠ è½½
                if self.verbose:
                    print(f"[WSM] Warning: param {param_name} not in CPU cache (layer {layer_idx}), skipping")
                nvtx.range_pop()
                return

        # CPU å‚æ•°ï¼šä» CPU cacheï¼ˆSSD æ¨¡å¼ï¼‰æˆ– CPU stashï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰åŠ è½½åˆ° GPU
        if p.device.type == "cpu":
            # SSD æ¨¡å¼ï¼šä¼˜å…ˆä½¿ç”¨ CPU cache
            if (self.ssd_enabled and layer_idx is not None and param_name is not None and
                layer_idx in self.cpu_cache and param_name in self.cpu_cache[layer_idx]):

                nvtx.range_push("cpu_cache_to_gpu")
                with torch.cuda.stream(self.streams.weight_h2d):
                    cached_tensor = self.cpu_cache[layer_idx][param_name]
                    p_gpu = cached_tensor.to(self.device, non_blocking=True)
                p.data = p_gpu
                nvtx.range_pop()
                nvtx.range_pop()
                return

            # ä¼ ç»Ÿæ¨¡å¼ï¼šç›´æ¥å°† CPU å‚æ•°ç§»åŠ¨åˆ° GPU
            # æ³¨æ„ï¼šå‚æ•°å·²ç»åœ¨ CPU ä¸Šä¸”æœ‰å®Œæ•´æ•°æ®ï¼Œç›´æ¥ transfer
            if p.numel() > 0:  # ç¡®ä¿ä¸æ˜¯ stub
                nvtx.range_push("weight_h2d_stream")
                with torch.cuda.stream(self.streams.weight_h2d):
                    nvtx.range_push("cpu_to_gpu_transfer")
                    p_gpu = p.data.to(self.device, non_blocking=True)
                    nvtx.range_pop()
                nvtx.range_pop()
                p.data = p_gpu
            else:
                # 0-size stubï¼šä¸åº”è¯¥å‡ºç°åœ¨ä¼ ç»Ÿæ¨¡å¼
                if self.verbose:
                    print(f"[WSM] Warning: encountered 0-size CPU stub in traditional streaming mode")
                nvtx.range_pop()
                return

        nvtx.range_pop()

    def _evict_param_to_cpu(self, p: torch.nn.Parameter):
        """
        é©±é€å‚æ•°å‡ºGPUï¼šå°† param.data è®¾ä¸º 0-size CPU tensorï¼ˆstubï¼‰
        ä¸å†ä½¿ç”¨ CPU stashï¼Œå› ä¸ºæˆ‘ä»¬åœ¨æ„å»ºæœŸå°±åˆ›å»ºäº† stub
        """
        # åˆ›å»º 0-size CPU stubï¼ˆä¿æŒ dtypeï¼‰
        stub = torch.empty(0, dtype=p.dtype, device="cpu")
        p.data = stub

    def _ensure_module_on_gpu(self, m: nn.Module, layer_idx: Optional[int] = None, module_name: Optional[str] = None):
        """Ensure all params/buffers of module m are on GPU."""
        # For modules, we need to handle parameter replacement differently
        # because meta parameters cannot be assigned via .data =

        params_to_replace = {}  # {local_param_name: new_param}
        params_full_names = {}  # {local_param_name: full_param_name}

        for local_param_name, p in m.named_parameters(recurse=False):  # Non-recursive to get direct params
            # Build full parameter name for CPU cache lookup
            # module_name is like "wq", "wk", etc. from _get_modules_dict()
            # param_name from named_parameters() is just "weight" or "bias"
            # We need to determine the parent module path (attention or feed_forward)
            full_param_name = None
            if layer_idx is not None and module_name is not None:
                # Determine parent module based on module_name
                if module_name in ["wq", "wk", "wv", "wo"]:
                    parent_module = "attention"
                elif module_name in ["w1", "w2", "w3"]:
                    parent_module = "feed_forward"
                else:
                    parent_module = None

                if parent_module:
                    full_param_name = f"layers.{layer_idx}.{parent_module}.{module_name}.{local_param_name}"
                else:
                    # Fallback for unknown modules
                    full_param_name = f"layers.{layer_idx}.{module_name}.{local_param_name}"

            # Check if this is a meta parameter that needs loading from SSD
            is_meta = (p.device.type == "meta" or getattr(p, "is_meta", False))

            if is_meta and self.ssd_enabled and full_param_name:
                # Try to load from CPU cache
                if layer_idx in self.cpu_cache and full_param_name in self.cpu_cache[layer_idx]:
                    cached_tensor = self.cpu_cache[layer_idx][full_param_name]
                    expected = tuple(getattr(getattr(m, local_param_name), "shape", ()))
                    chosen_name = full_param_name
                    chosen_tensor = cached_tensor

                    if expected and tuple(cached_tensor.shape) != expected:
                        # åœ¨åŒå±‚åŒæ—é‡Œæ‰¾å½¢çŠ¶èƒ½å¯¹ä¸Šçš„å¤‡é€‰
                        sibling_candidates = []
                        def _add(name):
                            t = self.cpu_cache[layer_idx].get(name)
                            if t is not None:
                                sibling_candidates.append((name, t))

                        if module_name in ("wq", "wk", "wv"):
                            for alt in ("wq", "wk", "wv"):
                                _add(f"layers.{layer_idx}.attention.{alt}.{local_param_name}")
                        elif module_name in ("w1", "w2", "w3"):
                            for alt in ("w1", "w2", "w3"):
                                _add(f"layers.{layer_idx}.feed_forward.{alt}.{local_param_name}")

                        for alt_name, t in sibling_candidates:
                            if tuple(t.shape) == expected:
                                chosen_name, chosen_tensor = alt_name, t
                                break
                        else:
                            raise RuntimeError(
                                f"[WSM] SSD manifest shape mismatch for {full_param_name}: "
                                f"expected {expected}, got {tuple(cached_tensor.shape)}. "
                                f"Check MANIFEST <-> params.json/model size."
                            )
                    with torch.cuda.stream(self.streams.weight_h2d):
                        p_gpu = chosen_tensor.to(self.device, non_blocking=True)
                        
                    params_to_replace[local_param_name] = nn.Parameter(p_gpu, requires_grad=p.requires_grad)
                    # params_full_names[local_param_name] = full_param_name
                    params_full_names[local_param_name] = chosen_name if 'chosen_name' in locals() else full_param_name
                    
                    if self.verbose:
                        print(f"[WSM DEBUG] âœ“ Loaded meta param {full_param_name} to GPU: {p_gpu.shape}")
                    if chosen_name != full_param_name and self.verbose:
                        print(f"[WSM] âš ï¸ shape-fix: remapped {full_param_name} -> {chosen_name}")
                else:
                    # CPU cache miss - è¿™æ˜¯æ­£å¸¸çš„é¢„åŠ è½½æµç¨‹ï¼Œä¸æ˜¯é”™è¯¯
                    if self.verbose >= 2:  # åªåœ¨è¯¦ç»†æ¨¡å¼ä¸‹æ˜¾ç¤º
                        print(f"[WSM DEBUG] Cache miss: {full_param_name} not in CPU cache, loading layer {layer_idx}...")
                    # å°è¯•ç«‹å³åŠ è½½è¯¥å±‚
                    if layer_idx not in self.cpu_cache:
                        try:
                            if self.verbose >= 2:  # åªåœ¨è¯¦ç»†æ¨¡å¼ä¸‹æ˜¾ç¤º
                                print(f"[WSM DEBUG] Triggering on-demand load for layer {layer_idx}...")
                            self._load_layer_to_cpu(layer_idx)
                            # é‡è¯•ä¸€æ¬¡
                            if layer_idx in self.cpu_cache and full_param_name in self.cpu_cache[layer_idx]:
                                cached_tensor = self.cpu_cache[layer_idx][full_param_name]
                                with torch.cuda.stream(self.streams.weight_h2d):
                                    p_gpu = cached_tensor.to(self.device, non_blocking=True)
                                params_to_replace[local_param_name] = nn.Parameter(p_gpu, requires_grad=p.requires_grad)
                                params_full_names[local_param_name] = full_param_name
                                if self.verbose:
                                    print(f"[WSM DEBUG] âœ“ Loaded meta param {full_param_name} to GPU after retry: {p_gpu.shape}")
                        except Exception as e:
                            if self.verbose:
                                print(f"[WSM ERROR] Failed to load layer {layer_idx}: {e}")
            else:
                # Regular parameter - use standard method
                self._ensure_param_on_gpu(p, layer_idx, full_param_name)

        # Replace meta parameters
        # for param_name, new_param in params_to_replace.items():
        #     # ä½¿ç”¨ _parameters å­—å…¸ç›´æ¥æ›¿æ¢ï¼Œè¿™æ˜¯ PyTorch çš„å†…éƒ¨æœºåˆ¶
        #     m._parameters[param_name] = new_param

        #     # æ›´æ–°å…¨åæ˜ å°„ï¼ˆå½±å“é©±é€æ­£ç¡®æ€§ï¼‰
        #     full_param_name = params_full_names.get(param_name)
        #     if full_param_name:
        #         # æ›´æ–° name_to_paramï¼šå…¨å -> Parameter å¯¹è±¡
        #         self.name_to_param[full_param_name] = getattr(m, param_name)
        #         # æ›´æ–° param_ownerï¼šå…¨å -> (module, attr_name)
        #         self.param_owner[full_param_name] = (m, param_name)
        
        for param_name, new_param in params_to_replace.items():
            # 1) æ›¿æ¢åˆ°æ¨¡å—
            m._parameters[param_name] = new_param
            # 2) æ›´æ–°å…¨åæ˜ å°„
            full_param_name = params_full_names.get(param_name)
            if full_param_name:
                # name -> Parameter å¯¹è±¡
                try:
                    pobj = getattr(m, param_name)
                except Exception:
                    pobj = new_param
                self.name_to_param[full_param_name] = pobj
                # name -> (module, attr)
                self.param_owner[full_param_name] = (m, param_name)
                
        # for b in m.buffers(recurse=True):
        #     if b.device.type == "cpu":
        #         with torch.cuda.stream(self.streams.weight_h2d):
        #             b_gpu = b.detach().to(self.device, non_blocking=True)
        #         try:
        #             b.data = b_gpu
        #         except Exception:
        #             pass
        for b in m.buffers(recurse=True):
        # å¯¹äº meta bufferï¼Œå…ˆ to_empty(materialize) å†å¡«å……ï¼›å¯¹äºå·²æœ‰CPU/GPU bufferï¼Œä¿æŒåŸé€»è¾‘
            if getattr(b, "is_meta", False):
                try:
                    b = b.to_empty(device=self.device)  # å…³é”®å­—å‚æ•°ï¼
                except Exception:
                    pass
            elif b.device.type == "cpu":
                with torch.cuda.stream(self.streams.weight_h2d):
                    b_gpu = b.detach().to(self.device, non_blocking=True)
                try:
                    b.data = b_gpu
                except Exception:
                    pass

    def _evict_module_to_cpu(self, m: nn.Module):
        """Evict module m: params point back to CPU master; buffers copied back to CPU."""
        for p in m.parameters(recurse=True):
            self._evict_param_to_cpu(p)
        for b in m.buffers(recurse=True):
            if b.device.type != "cpu":
                try:
                    b_cpu = b.detach().cpu()
                    try:
                        b_cpu = b_cpu.pin_memory()
                    except Exception:
                        pass
                    b.data = b_cpu
                except Exception:
                    pass

    # -------- Layer-level ensure / prefetch / evict --------

    def _ring_range(self, start: int, count: int) -> list[int]:
        n = self.n_layers
        return [ (start + k) % n for k in range(count) ]

    
    # def _pre_hook_factory(self, idx: int):
    #     def _pre_hook(_module, _inputs):
    #         # 1) CPU æ»‘çª—è§¦è¾¾ï¼ˆSSDâ†’DRAMï¼‰ï¼ŒåŸæ ·ä¿ç•™
    #         self._touch_cpu_layer(idx)
    #         if self.ssd_enabled:
    #             self._schedule_cpu_prefetch(idx)

    #         # 2) è‹¥æ²¡å¼€ç»„çº§æ¨¡å¼ï¼Œé€€å›å±‚çº§ ensureï¼ˆä¸å»ºè®®ï¼‰
    #         if not self.grouped_mode:
    #             self.ensure_on_gpu(idx, wait=True)
    #             return

    #         # 3) ç»„çº§é¢„ç®—æ„ŸçŸ¥çš„é¢„å–è®¡åˆ’
    #         #    - å¿…åšï¼šå½“å‰å±‚ FFNï¼ˆä¸å½“å‰ ATTN è®¡ç®—é‡å ï¼‰
    #         #    - è¿œè·ç¦»ï¼šä¸‹ä¸€å±‚èµ·ï¼Œè¿ç»­ K ä¸ª ATTNï¼ˆç¯å›å–æ¨¡ï¼‰
    #         K = max(0, int(getattr(self, "group_prefetch_depth", 0)))
    #         plan: list[tuple[int, str]] = []

    #         # 3.1 å½“å‰å±‚ FFN
    #         plan.append((idx % self.n_layers, "ffn"))

    #         # 3.2 æœªæ¥ K ä¸ª ATTNï¼ˆi+1 .. i+Kï¼‰
    #         for k in range(1, K + 1):
    #             nxt = (idx + k) % self.n_layers
    #             plan.append((nxt, "attn"))

    #         # 4) ç»“åˆ gpu_max_groups åšä¸ªè½»é‡è£å‰ªï¼šé¢„ç•™ 1 ä¸ªå®‰å…¨åé¢
    #         used = len(self._gpu_group_lru)
    #         budget = max(0, self.gpu_max_groups - used - 1)
    #         if budget < len(plan):
    #             plan = plan[:budget]

    #         # 5) å¼‚æ­¥é¢„å–ï¼ˆä¸é˜»å¡ï¼‰ï¼Œå¤±è´¥ä¸æŠ›
    #         for lid, grp in plan:
    #             try:
    #                 self.prefetch_group_async(lid, grp)
    #             except Exception as e:
    #                 if self.verbose:
    #                     print(f"[WSM] Group prefetch failed in hook for L{lid}.{grp}: {e}")

    #     return _pre_hook

    def _pre_hook_factory(self, idx: int):
        def _pre_hook(_module, _inputs):
            # 1) CPUï¼šLRU è§¦è¾¾ + æ»‘çª—
            self._touch_cpu_layer(idx)
            if self.ssd_enabled:
                self._schedule_cpu_prefetch(idx)  # ä¿æŒä½ ç°æœ‰çš„æ»‘çª—/æ»šåŠ¨é€»è¾‘

            # 2) å…³é—­â€œå±‚çº§æ¿€è¿›é¢„å–â€åœ¨ç»„æ¨¡å¼ä¸‹çš„å¹²æ‰°ï¼ˆåªä¿ç•™ç»„çº§ï¼‰
            if (not self.grouped_mode) and (self.aggressive_gpu_prefetch > 0):
                prefetch_targets = []
                for offset in range(1, self.aggressive_gpu_prefetch + 1):
                    nxt = idx + offset
                    if nxt < self.n_layers:
                        prefetch_targets.append(nxt)
                if prefetch_targets:
                    try:
                        self.prefetch(prefetch_targets)
                        if self.verbose:
                            print(f"[WSM] Async GPU prefetch (layer-level): layers {prefetch_targets}")
                    except Exception as e:
                        if self.verbose:
                            print(f"[WSM] Async layer-prefetch failed for {prefetch_targets}: {e}")
            if self.grouped_mode and hasattr(self, "prefetch_group_async"):
                used   = len(self._gpu_group_lru)
                budget = max(0, self.gpu_max_groups - used - 1)  # é¢„ç•™ 1 ç»„
                plan: list[tuple[int,str]] = []
                # å½“å‰å±‚ FFN ä¸ MHA é‡å ï¼Œæ”¶ç›Šæœ€å¤§
                if budget > 0:
                    plan.append((idx % self.n_layers, "ffn"))
                    budget -= 1
                # â˜† æŠŠ ATTN çš„é¢„å–èŒƒå›´ä» â€œ+1 å±‚â€ æ‰©åˆ° â€œ+D å±‚â€
                D = max(1, int(getattr(self, "group_prefetch_depth", 1)))
                for off in range(1, D+1):
                    if budget <= 0: break
                    nxt = (idx + off) % self.n_layers
                    plan.append((nxt, "attn"))
                    budget -= 1

                # å‘èµ·å¼‚æ­¥é¢„å–ï¼ˆä¸é˜»å¡ï¼‰
                for lid, grp in plan:
                    try:
                        self.prefetch_group_async(lid, grp)
                    except Exception as e:
                        if self.verbose:
                            print(f"[WSM] Group prefetch failed in hook for layer {lid}/{grp}: {e}")

            else:
                # éç»„æ¨¡å¼ï¼šå…œåº•ï¼Œè‡³å°‘ç¡®ä¿å½“å‰å±‚æƒé‡åœ¨ GPU
                self.ensure_on_gpu(idx, wait=True)

        return _pre_hook


    def ensure_on_gpu(self, idx: int, wait: bool):
        """Ensure layer idx is present on GPU (respecting LRU); optionally wait for readiness."""
        # åœ¨æ­£å¼ H2D å‰è¯„ä¼°ä¸€æ¬¡ PD

        self._decide_pd()

        nvtx.range_push(f"ensure_layer_{idx}")
        
        # åœ¨ meta åˆå§‹åŒ–+SSD æ¨¡å¼ä¸‹ï¼Œç¡®ä¿æœ‰ CPU å±‚ç¼“å­˜å¯ç”¨ï¼ˆæŒ‰éœ€ä» SSD æ‹‰èµ·ï¼‰
        if self.ssd_enabled and (idx not in self.cpu_cache):
                try:
                    self._load_layer_to_cpu(idx)
                except Exception:
                    pass
                
        self._record_memory_stats("ensure_start", idx)

        # prepare retention window
        self._refresh_retain_window(idx)
        if idx in self.gpu_cache:
            nvtx.range_push(f"cache_hit_layer_{idx}")
            self.gpu_cache.move_to_end(idx)
            if wait:
                nvtx.range_push(f"wait_layer_{idx}")
                self._wait_layer_ready(idx)
                nvtx.range_pop()
            nvtx.range_pop()
        else:
            nvtx.range_push(f"cache_miss_layer_{idx}")

            # Evict until space is available
            while len(self.gpu_cache) >= self.max_cached_layers:
                self._record_memory_stats("evict_start", -1)
                self._evict_one_not_retained()
                self._record_memory_stats("evict_end", -1)

            # H2D for current layer (skip resident norms)
            nvtx.range_push(f"h2d_transfer_layer_{idx}")
            self._record_memory_stats("h2d_start", idx)
            for module_name, mod in self.block_mods[idx].items():
                if not module_name.startswith("norm_"):
                    nvtx.range_push(f"h2d_{module_name}")
                    self._ensure_module_on_gpu(mod, idx, module_name)
                    nvtx.range_pop()
            self._record_memory_stats("h2d_end", idx)
            nvtx.range_pop()

            self.gpu_cache[idx] = None

            # Record "layer ready" event on H2D stream after enqueuing all copies
            nvtx.range_push(f"record_event_layer_{idx}")
            self._record_layer_ready_event(idx)
            nvtx.range_pop()

            if self.verbose:
                # åˆ¤æ–­æƒé‡ä»å“ªé‡ŒåŠ è½½
                source = "SSDâ†’CPUâ†’GPU" if (self.ssd_enabled and idx in self.cpu_cache) else \
                         "SSDâ†’GPU" if self.ssd_enabled else \
                         "CPUâ†’GPU"
                print(f"[WSM] ->GPU layer={idx} ({source})")
            if wait:
                nvtx.range_push(f"wait_layer_{idx}")
                self._wait_layer_ready(idx)
                nvtx.range_pop()

            nvtx.range_pop()  # cache_miss

        self._record_memory_stats("ensure_end", idx)
        nvtx.range_pop()  # ensure_layer
        
        
    def _bytes_of(self, t):
        itemsize = {torch.float16:2, torch.bfloat16:2, torch.float32:4, torch.int8:1,
                    torch.uint8:1, torch.int32:4, torch.int64:8, torch.float64:8}.get(t.dtype, 2)
        return t.numel() * itemsize

    # ---------------- Group residency introspection & printing ----------------
    def _snapshot_gpu_groups(self):
        """
        è¿”å›å½“å‰åœ¨ GPU çš„ç»„å¿«ç…§ï¼š[(layer, 'attn'/'ffn', in_use_bool), ...]
        ä»¥ LRU é¡ºåºï¼ˆæœ€è€åœ¨å‰ï¼‰è¿”å›ã€‚
        """
        lst = []
        # for (lyr, grp) in list(self._gpu_group_lru):  # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…éå†æ—¶è¢«ä¿®æ”¹
        #     lst.append((int(lyr), str(grp), (lyr, grp) in self._gpu_groups_in_use))
        # return lst
        inflight_keys = set(self._gpu_group_inflight.keys())
        for (lyr, grp) in list(self._gpu_group_lru):  # å¤åˆ¶ï¼Œé¿å…éå†ä¸­ä¿®æ”¹
            key = (int(lyr), str(grp))
            in_use = key in self._gpu_groups_in_use
            inflight = key in inflight_keys
            lst.append((key[0], key[1], in_use, inflight))
        return lst


    def _snapshot_cpu_groups(self):
        """
        è¿”å›å½“å‰åœ¨ CPU cache ä¸­çš„ç»„å¿«ç…§ï¼š
        [(layer, 'attn'/'ffn', present_cnt, total_cnt), ...]
        åªè¦è¯¥ç»„è‡³å°‘æœ‰ä¸€ä¸ªå‚æ•°åœ¨ CPU cache ä¸­å°±ä¼šå±•ç¤ºï¼›ä¾¿äºè§‚å¯Ÿâ€œéƒ¨åˆ†å‘½ä¸­â€ã€‚
        """
        summary = []
        cache_keys = list(getattr(self, "cpu_cache", {}).keys())
        for lyr in sorted(cache_keys):
            layer_dict = self.cpu_cache.get(lyr, {})
            if not isinstance(layer_dict, dict):
                continue
            for grp_name, suffixes in GROUPS.items():
                tot = len(suffixes)
                hit = 0
                for suf in suffixes:
                    if f"layers.{lyr}.{suf}" in layer_dict:
                        hit += 1
                if hit > 0:
                    summary.append((int(lyr), grp_name, hit, tot))
        return summary

    def print_group_residency(self, current: tuple[int, str] | None = None, header: str | None = None):
        """
        æ‰“å°â€œå½“å‰ GPU/CPU ä¸Šæœ‰å“ªäº›ç»„â€çš„ä¸€è¡Œæ‘˜è¦ã€‚
        - current: å¯é€‰ï¼Œç”¨æ¥é«˜äº®å½“å‰æ­£åœ¨è®¡ç®—çš„ (layer, group)
        - header:  å¯é€‰ï¼Œè‡ªå®šä¹‰å‰ç¼€ï¼›é»˜è®¤ '[WSM][groups]'
        """
        try:
            gpu = self._snapshot_gpu_groups()
            cpu = self._snapshot_cpu_groups()
        except Exception as e:
            print(f"[WSM][groups] snapshot failed: {e}")
            return

        def _fmt_gpu(item):
            # L, G, in_use = item
            # star = "*" if in_use else ""
            # cur  = "âš‘" if (current is not None and (L, G) == current) else ""
            # return f"L{L}.{G}{star}{cur}"
            
            if len(item) == 3:
                L, G, in_use = item
                inflight = ((L, G) in self._gpu_group_inflight)
            else:
                L, G, in_use, inflight = item
            star = "*" if in_use else ""
            dot  = "â€¦" if (not in_use and inflight) else ""
            cur  = "âš‘" if (current is not None and (L, G) == current) else ""
            return f"L{L}.{G}{star}{cur}{dot}"

        def _fmt_cpu(item):
            L, G, hit, tot = item
            cur  = "âš‘" if (current is not None and (L, G) == current) else ""
            suffix = "" if hit == tot else f"({hit}/{tot})"
            return f"L{L}.{G}{suffix}{cur}"

        gpu_txt = ", ".join(_fmt_gpu(x) for x in gpu) if gpu else "â€”"
        cpu_txt = ", ".join(_fmt_cpu(x) for x in cpu) if cpu else "â€”"
        pfx = header or "[WSM][groups]"
        print(f"{pfx} GPU: {gpu_txt} | CPU: {cpu_txt}")

    
    def _mark_group_in_use(self, layer_idx: int, group: str):
        key = (layer_idx, group)
        with self._group_lock:
            self._gpu_groups_in_use.add(key)
        self._touch_group(layer_idx, group)
        if self.verbose:
            print(f"[WSM] Marked group {key} as IN_USE")
        try:
            if os.getenv("WSM_PRINT_GROUPS", "1") == "1":
                self.print_group_residency(current=key)
        except Exception:
            pass

    def _unmark_group_in_use(self, layer_idx: int, group: str):
        key = (layer_idx, group)
        with self._group_lock:
            self._gpu_groups_in_use.discard(key)
        # ç»„è®¡ç®—åˆšç»“æŸå†è§¦è¾¾ä¸€æ¬¡ï¼Œé¿å…â€œç«‹å³è¢«è¸¢â€æŠ–åŠ¨
        self._touch_group(layer_idx, group)
        if self.verbose:
            print(f"[WSM] Unmarked group {key} from IN_USE")
        # â€”â€” æ”¶æ•›é˜€é—¨ï¼šæŠŠ _gpu_group_lru æ”¶å›é¢„ç®—ï¼ˆå¿…è¦æ—¶å¿½ç•¥ retainï¼‰â€”â€”
        # self._shrink_gpu_groups_now(exclude={key})
        
        if getattr(self, "evict_finished_group", False):
            # ç›´æ¥è¸¢åˆšç»“æŸçš„ç»„
            self._evict_group_immediately(layer_idx, group)
            # å†æŒ‰ä¸Šé™åšä¸€æ¬¡æ”¶ç¼©ï¼ˆé˜²å¾¡æ€§ï¼‰
            self._shrink_gpu_groups_now()
        else:
            # ä¿æŒåŸè¡Œä¸ºï¼šæŠŠåˆšç»“æŸçš„ç»„æ’é™¤æ‰ï¼Œåªæ”¶ç¼©â€œå…¶å®ƒç»„â€åˆ°ä¸Šé™
            self._shrink_gpu_groups_now(exclude={key})


    def _evict_group_immediately(self, layer_idx: int, group: str):
        key = (layer_idx, group)
        for suf in GROUPS[group]:  # ä¾‹å¦‚ 'attn' -> wq, wk, wv, wo
            name = f"layers.{layer_idx}.{suf}"
            p = self.name_to_param.get(name)
            if p is not None and p.is_cuda and p.numel() > 0:
                self._evict_param_to_cpu(p)
        with self._group_lock:
            if key in self._gpu_group_lru:
                self._gpu_group_lru.remove(key)
            
    def _group_is_resident(self, layer_idx: int, group: str) -> bool:
        """
        è¯¥ç»„(attn/ffn)çš„æ‰€æœ‰å‚æ•°æ˜¯å¦å·²åœ¨ GPU ä¸”ä¸ºéç©ºå¼ é‡ã€‚
        """
        suffixes = GROUPS.get(group)
        if suffixes is None:
            raise ValueError(f"unknown group '{group}', expected one of {tuple(GROUPS.keys())}")
        for suf in suffixes:
            pname = f"layers.{layer_idx}.{suf}"
            p = self.name_to_param.get(pname)
            if (p is None) or (not p.is_cuda) or (p.numel() == 0):
                return False
        return True


    def _record_group_ready_event(self, layer_idx: int, group: str) -> None:
        """
        åœ¨æƒé‡ H2D æµä¸Šè®°å½•**ç»„çº§** ready äº‹ä»¶ã€‚
        éœ€åœ¨æŠŠæœ¬ç»„æ‰€æœ‰å‚æ•°çš„ H2D å…¥é˜Ÿåè°ƒç”¨ï¼ˆè§ ensure_group_on_gpu/prefetch_group_asyncï¼‰ã€‚
        """
        if not torch.cuda.is_available():
            return
        h2d = getattr(self.streams, "weight_h2d", None)
        if h2d is None:
            return

        key = (layer_idx, group)
        evt = self._group_ready_events.get(key)
        if evt is None:
            evt = torch.cuda.Event(blocking=False)
            self._group_ready_events[key] = evt
        evt.record(h2d)

        # ä¸å±‚çº§äº‹ä»¶ä¸€æ ·ï¼Œåšä¸€æ¬¡ KV æµé‡ä»²è£çš„è½»é‡æ£€æŸ¥
        try:
            self._check_and_throttle_kv()
        except Exception:
            pass


    def wait_group_ready(
        self,
        layer_idx: int,
        group: str,
        compute_stream: "torch.cuda.Stream | None" = None,
    ) -> None:
        """
        åªç­‰å¾… (layer_idx, group) è¿™ç»„æƒé‡åœ¨ H2D æµä¸Šçš„å®Œæˆäº‹ä»¶ã€‚
        - å¿«è·¯å¾„ï¼šè¯¥ç»„å·²é©»ç•™äº GPU -> ç›´æ¥è¿”å›ï¼›
        - å¦åˆ™è‹¥å­˜åœ¨ç»„çº§äº‹ä»¶ -> å°† compute_streamï¼ˆæˆ–å½“å‰æµï¼‰æŒ‚åˆ°è¯¥äº‹ä»¶ä¸Šï¼›
        - å¦åˆ™ï¼ˆæ— äº‹ä»¶ï¼‰ç›´æ¥è¿”å›ï¼Œä¸å»ç­‰å¾…æ•´ä¸ª H2D æµï¼Œé¿å…è¯¯ç­‰åˆ«ç»„ä¼ è¾“ã€‚
        """
        # 1) å¿«è·¯å¾„ï¼šè¯¥ç»„å·²åœ¨ GPU
        try:
            if self._group_is_resident(layer_idx, group):
                return
        except KeyError as e:
            raise ValueError(f"unknown group '{group}': {e}")

        # 2) å–è¯¥ç»„çš„äº‹ä»¶
        evt = self._group_ready_events.get((layer_idx, group))
        if evt is None:
            # æ²¡æœ‰ç»„çº§äº‹ä»¶ï¼šè¯´æ˜è¯¥ç»„çš„ H2D å¯èƒ½å°šæœªå…¥é˜Ÿ/è®°å½•ã€‚ä¸ºäº†ä¸è¯¯ç­‰å…¶å®ƒç»„ï¼Œç›´æ¥è¿”å›ã€‚
            return

        # 3) åœ¨ç›®æ ‡ compute stream ä¸Šç­‰å¾…
        try:
            # å…¼å®¹ self.device ä¸º strï¼ˆå¦‚ "cuda:0"ï¼‰æˆ– torch.device
            dev_obj = torch.device(self.device) if isinstance(self.device, str) else self.device
            s = compute_stream or torch.cuda.current_stream(dev_obj)
            s.wait_event(evt)
        except Exception:
            # æç«¯æƒ…å†µä¸‹çš„å…œåº•ï¼šä¸æŠ›å¼‚å¸¸ï¼Œé¿å…æŠŠå‰å‘å¡æ­»
            pass


    
    # def wait_group_ready(self, layer_idx: int, group: str, compute_stream=None):
    #     """
    #     åœ¨ compute_stream ä¸Šç­‰å¾…è¯¥å±‚æƒé‡çš„ H2D äº‹ä»¶ã€‚
    #     æˆ‘ä»¬ç”¨â€œå±‚çº§â€ ready äº‹ä»¶ä½œä¸ºç»„çš„ barrierï¼ˆè¯¥å±‚æœ€è¿‘ä¸€æ¬¡ H2D å…¥é˜Ÿåè®°å½•ï¼‰ã€‚
    #     """
    #     evt = self.layer_events.get(layer_idx)
    #     if evt is None:
    #         # å›é€€ï¼šç­‰å¾… weight_h2d æµ drain
    #         try:
    #             self.streams.wait_weight_ready_on_current(self.device)
    #         except Exception:
    #             pass
    #         return
    #     if compute_stream is None:
    #         torch.cuda.current_stream(self.device).wait_event(evt)
    #     else:
    #         compute_stream.wait_event(evt)

    def _touch_group(self, layer_idx: int, group: str):
        self._grp_last_touch[(layer_idx, group)] = time.monotonic()

    def _should_retain_group(self, layer_idx: int, group: str) -> bool:
        t = self._grp_last_touch.get((layer_idx, group))
        if t is None: 
            return False
        return (time.monotonic() - t) < (self._grp_retain_ms / 1000.0)

    def _shrink_gpu_groups_now(self, exclude: set[tuple[int, str]] = frozenset()):
        # åˆ†ä¸¤è½®ï¼šç¬¬ä¸€è½®å°Šé‡â€œæœ€è¿‘è§¦è¾¾â€ï¼›ç¬¬äºŒè½®å¿½ç•¥ä¹‹
        for pass_idx in (0, 1):
            while True:
                with self._group_lock:
                    if len(self._gpu_group_lru) <= self.gpu_max_groups:
                        return
                ok = self._evict_one_group_from_gpu(
                    exclude=exclude, 
                    ignore_retain=(pass_idx == 1)
                )
                if not ok:
                    # æ²¡æœ‰å¯è¸¢çš„å€™é€‰ï¼ˆå…¨åœ¨ç”¨/éƒ½åœ¨æ’é™¤/éƒ½åˆšè§¦è¾¾ï¼‰
                    if self.verbose:
                        print(f"[WSM] shrink pass{pass_idx}: no candidate; "
                            f"overflow={len(self._gpu_group_lru)} > cap={self.gpu_max_groups}")
                    break  # è¿›å…¥ä¸‹ä¸€è½®ï¼ˆè‹¥è¿˜æœ‰ï¼‰


    def _evict_one_group_from_gpu(self, exclude=(), ignore_retain: bool = False):
        """
        LRU æ·˜æ±°ä¸€ä¸ªç»„ï¼›è·³è¿‡ excludeã€IN_USEï¼›å¯é€‰æ‹©å¿½ç•¥â€œæœ€è¿‘è§¦è¾¾â€ç•™å­˜ã€‚
        ä¼˜å…ˆé€‰æ‹© FFNï¼ˆæ˜¾å­˜æ›´å¤§ï¼‰ï¼Œå¦åˆ™æŒ‰ LRUã€‚
        """
        cand_idx = None
        cand_key = None
        cand_is_ffn = False
        inflight_keys = set(self._gpu_group_inflight.keys())

        with self._group_lock:
            for i, (lyr, grp) in enumerate(list(self._gpu_group_lru)):
                key = (lyr, grp)
                if key in exclude:
                    continue
                if key in self._gpu_groups_in_use:
                    if self.verbose:
                        print(f"[WSM] Skipping eviction of IN_USE group {key}")
                    continue
                if key in inflight_keys:             # æ­£åœ¨åŠ è½½
                    continue
                if (not ignore_retain) and self._should_retain_group(lyr, grp):
                    continue

                # é€‰æ‹©ç­–ç•¥ï¼šä¼˜å…ˆ FFNï¼›å¦åˆ™ç¬¬ä¸€ä¸ªå¯è¡Œçš„ LRU
                if grp == "ffn":
                    cand_idx, cand_key, cand_is_ffn = i, key, True
                    break  # å·²æ‰¾åˆ° FFNï¼Œç›´æ¥ç”¨
                if cand_key is None:  # å…ˆè®°å½•ä¸€ä¸ªé FFN å€™é€‰
                    cand_idx, cand_key, cand_is_ffn = i, key, False

            if cand_key is None:
                if self.verbose:
                    print("[WSM] No groups available for eviction (all in_use or excluded)")
                return False

            # çœŸæ­£æ‰§è¡Œé©±é€
            lyr, grp = cand_key
            for suf in GROUPS[grp]:
                name = f"layers.{lyr}.{suf}"
                p = self.name_to_param.get(name)
                if p is None:
                    continue
                if p.is_cuda and p.numel() > 0:
                    self._evict_param_to_cpu(p)

            self._gpu_group_lru.pop(cand_idx)
            torch.cuda.empty_cache()
            if self.verbose:
                print(f"[WSM] Evicted group {cand_key} from GPU"
                    f"{' (FFN preferred)' if cand_is_ffn else ''}")
            return True


    def _ensure_gpu_room(self, need_bytes, exclude=()):
        guard = self.gpu_free_guard_mb * 1024 * 1024
        while True:
            free, _ = torch.cuda.mem_get_info(self.device.index)
            if free >= need_bytes + guard and len(self._gpu_group_lru) < self.gpu_max_groups:
                return
            if not self._evict_one_group_from_gpu(exclude=exclude):
                break
        # å†æ¬¡æ£€æŸ¥ï¼Œå¦‚æœè¿˜ä¸å¤Ÿï¼Œè®©ä¸Šå±‚å¤„ç† OOM

    def _install_param_tensor(self, pname: str, dst_gpu_tensor: torch.Tensor):
        """
        æŠŠ dst_gpu_tensor å®‰è£…åˆ°æ¨¡å‹çš„å‚æ•° pname ä¸Šï¼š
        ç›´æ¥æ›¿æ¢ param.dataï¼ˆCPU stub â†’ GPU tensorï¼‰
        """
        param = self.name_to_param.get(pname)
        if param is None:
            # å‚æ•°ä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯æ–°å‚æ•°æˆ–æ˜ å°„æœªæ›´æ–°
            if self.verbose:
                print(f"[WSM] Warning: param {pname} not in name_to_param, skipping")
            return

        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ›¿æ¢å‚æ•°å¯¹è±¡ï¼ˆdtype/shape ä¸åŒ¹é…æ—¶ï¼‰
        # æ³¨æ„ï¼š0-size CPU stub çš„ shape æ˜¯ (0,)ï¼Œä¸çœŸå®æƒé‡ä¸åŒï¼Œæ‰€ä»¥ä¼šè§¦å‘æ›¿æ¢
        need_replace = (
            param.dtype != dst_gpu_tensor.dtype
            or param.shape != dst_gpu_tensor.shape
        )

        if need_replace:
            # ç”¨çœŸæ­£çš„æ•°æ®åˆ›å»ºä¸€ä¸ªæ–°çš„ Parameter å¹¶æ›¿æ¢åˆ°æ¨¡å—ä¸Š
            if pname in self.param_owner:
                mod, attr = self.param_owner[pname]
                new_p = nn.Parameter(dst_gpu_tensor, requires_grad=False)
                setattr(mod, attr, new_p)
                self.name_to_param[pname] = new_p

                if self.verbose:
                    old_info = f"{param.device}/{param.dtype}/{param.shape}"
                    new_info = f"{new_p.device}/{new_p.dtype}/{new_p.shape}"
                    print(f"[WSM] Replaced param {pname}: {old_info} -> {new_info}")
            else:
                # Fallback: ç›´æ¥èµ‹å€¼ data
                param.data = dst_gpu_tensor
                self.name_to_param[pname] = param
        else:
            # åŒ dtype/device/shapeï¼Œèµ° copy_ è¦†ç›–ï¼ˆå¸¸è§„è·¯å¾„ï¼‰
            param.data.copy_(dst_gpu_tensor)

    def _move_to_gpu(self, pname: str, src_cpu_tensor: torch.Tensor, exclude: set[tuple[int,str]] | None = None):
        """
        CPUâ†’GPU æ¬è¿ + å®‰è£…ï¼ˆä½¿ç”¨æ‹·è´ + æ›¿æ¢ param.data æ–¹å¼ï¼‰
        ç§»é™¤äº† meta device ç‰©åŒ–é€»è¾‘ï¼Œæ”¹ç”¨ç®€å•çš„å‚æ•°æ•°æ®æ›¿æ¢
        """

        # 1) è®¡ç®—æ‰€éœ€ç©ºé—´å¹¶ç¡®ä¿ GPU headroom
        need_bytes = src_cpu_tensor.numel() * src_cpu_tensor.element_size()
        self._ensure_gpu_headroom(need_bytes, exclude=exclude)

        # 2) è·å– H2D stream
        h2d_stream = getattr(self.streams, "weight_h2d", None) if hasattr(self, "streams") else None
        if h2d_stream is None:
            h2d_stream = self._copy_stream  # fallback

        # 3) H2D ä¼ è¾“ï¼ˆåœ¨ weight_h2d æµä¸­è¿›è¡Œï¼‰
        try:
            if h2d_stream is not None:
                # CUDAè®¾å¤‡ï¼šä½¿ç”¨ weight_h2d streamï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡ï¼‰
                with torch.cuda.stream(h2d_stream):
                    dst = src_cpu_tensor.to(self.device, non_blocking=True)
            else:
                # CPUè®¾å¤‡æˆ–æ— streamï¼šç›´æ¥ä¼ è¾“
                dst = src_cpu_tensor.to(self.device, non_blocking=False)
        except torch.cuda.OutOfMemoryError:
            # æœ€åå…œåº•ï¼šå†é€å‡ºä¸€ä¸ªç»„ã€æ¸…ç¼“å­˜ã€é‡è¯•ä¸€æ¬¡
            if self._evict_one_group_from_gpu(exclude=exclude or set()):
                torch.cuda.empty_cache()
                if h2d_stream is not None:
                    with torch.cuda.stream(h2d_stream):
                        dst = src_cpu_tensor.to(self.device, non_blocking=True)
                else:
                    dst = src_cpu_tensor.to(self.device, non_blocking=False)
            else:
                # æ— æ³•é€å‡ºä»»ä½•ä¸œè¥¿ï¼ŒæŠ›å‡º OOM
                raise

        # 4) å®‰è£…å‚æ•°åˆ°æ¨¡å‹ï¼ˆä½¿ç”¨ä¿é™©ä¸æœºåˆ¶ï¼‰
        self._install_param_tensor(pname, dst)
    
        return dst

    def ensure_group_on_gpu(self, layer_idx: int, group: str):
        """é˜»å¡å¼ï¼šç¡®ä¿ (layer_idx, group) åœ¨ GPUï¼›è‹¥åå°ä»»åŠ¡åœ¨é£åˆ™ç­‰å¾…ï¼Œè¶…æ—¶åˆ™åŒæ­¥å…œåº•ã€‚"""
        wanted = GROUPS[group]

        # å…ˆçœ‹æ˜¯å¦å·²æœ‰åå°ä»»åŠ¡åœ¨é£
        key = (layer_idx, group)
        inflight_evt = self._gpu_group_inflight.get(key)

        if inflight_evt is not None:
            # ç­‰ä¸€å°æ®µæ—¶é—´ï¼Œç»™åå°ä¸€ä¸ªå®Œæˆçš„æœºä¼šï¼ˆé¿å…é‡å¤æ‹·è´ï¼‰
            if not inflight_evt.wait(timeout=3.0):
                if self.verbose:
                    print(f"[WSM] Group {key} inflight timeout; will fallback to sync load")
        

        # ç¡®ä¿ CPU å±‚å·²ç»åœ¨ç¼“å­˜ï¼ˆåå°æ²¡å®Œæˆå°±ç«‹åˆ»å…œåº•ï¼Œä½†è¿™åœ¨ç¡®ä¿å‡½æ•°é‡Œï¼Œæ¥å—é˜»å¡ï¼‰
        try:
            self._wait_cpu_ready(layer_idx, timeout=2.0)
        except Exception:
            # å…œåº•åŒæ­¥æ‹‰å±‚
            self._load_layer_to_cpu(layer_idx)

        layer_cache = self.cpu_cache.get(layer_idx, {})

        # é€ param æ£€æŸ¥/å¤åˆ¶åˆ° GPUï¼ˆèµ° H2D streamï¼›æ­¤å¤„å¯é˜»å¡ï¼‰
        for suf in wanted:
            name = f"layers.{layer_idx}.{suf}"
            src = layer_cache.get(name)
            if src is None:
                # å°è¯•ä»æ¨¡å‹æœ¬èº«è·å–å‚æ•°ï¼ˆä¼ ç»Ÿ streaming æ¨¡å¼ï¼‰
                if not self.ssd_enabled:
                    src = self._get_param_from_model(layer_idx, suf)
                if src is None:
                    # æœ€åæ‰å°è¯•ä» SSD åŠ è½½
                    src = self._load_param_from_ssd(name)
            self._move_to_gpu(name, src, exclude={key})

        # è®°å½• ready äº‹ä»¶
        self._record_layer_ready_event(layer_idx)
        self._record_group_ready_event(layer_idx, group) 
        # æ›´æ–°ç»„ LRU
        # if key in self._gpu_group_lru:
        #     self._gpu_group_lru.remove(key)
        # self._gpu_group_lru.append(key)
        # while len(self._gpu_group_lru) > self.gpu_max_groups:
        #     self._evict_one_group_from_gpu(exclude={key})

        with self._group_lock:
            if key in self._gpu_group_lru:
                self._gpu_group_lru.remove(key)
            self._gpu_group_lru.append(key)
            # å…è®¸çŸ­æš‚è¶…é¢ï¼›è‹¥ç¡®å®è¦æ”¶ç¼©ï¼Œå°è¯•ä¸€æ¬¡é©±é€å³å¯
            if len(self._gpu_group_lru) > self.gpu_max_groups:
                ok = self._evict_one_group_from_gpu(exclude={key})
                if not ok and self.verbose:
                    print(f"[WSM] cannot evict under gpu_max_groups={self.gpu_max_groups}; allow temporary overflow={len(self._gpu_group_lru)}")

        self._touch_group(layer_idx, group)
        
    # --- inside class WeightStreamingManager ---
    def _read_layer_from_ssd(self, layer_idx: int) -> Dict[str, torch.Tensor]:
        """
        Read one full layer's streamable params from SSD into pinned-CPU tensors
        and return {param_name: tensor} without mutating cpu_cache.
        """
        if not self.ssd_enabled:
            raise RuntimeError("SSD backend not enabled")
        if layer_idx < 0 or layer_idx >= self.n_layers:
            raise IndexError(f"layer {layer_idx} out of range 0..{self.n_layers-1}")
        if layer_idx not in self.layers_params:
            raise KeyError(f"Layer {layer_idx} not in manifest")

        from .weights_io_ssd_dram import DTYPE_MAP, alloc_pinned_aligned

        layer_weights: Dict[str, torch.Tensor] = {}
        params = self.layers_params[layer_idx]

        for pinfo in params:
            if pinfo.get("policy") != "stream":
                continue

            stride = int(pinfo["stride"])
            offset = int(pinfo["offset"])
            nbytes = int(pinfo["nbytes"])

            # ensure staging buffer big enough
            if stride > len(self.staging_buffer):
                bs = int(self.ssd_manifest["block_size"])
                new_sz = ((stride + bs - 1) // bs) * bs
                self.staging_buffer = alloc_pinned_aligned(new_sz, bs)

            # SSD â†’ staging (pinned)
            self.ssd_dio.pread_into_tensor(self.staging_buffer, stride, offset)

            # materialize pinned CPU tensor with correct shape/dtype
            t = torch.empty(
                tuple(pinfo["shape"]),
                dtype=DTYPE_MAP[pinfo["dtype"]],
                pin_memory=True
            )
            # copy exact bytes
            t.view(-1).view(torch.uint8)[:nbytes].copy_(self.staging_buffer[:nbytes])

            layer_weights[pinfo["name"]] = t

        if not layer_weights:
            raise RuntimeError(f"no stream params loaded for layer {layer_idx}")
        return layer_weights

    # def _cpu_prefetch_worker(self):
    #     while not self._stopped:
    #         try:
    #             epoch, L = self._cpu_pf_q.get(timeout=0.1)
    #         except queue.Empty:
    #             continue

    #         # è¯» SSD â†’ æ„å»ºä¸´æ—¶å­—å…¸ï¼ˆä¸æŒé”ï¼‰
    #         tmp = self._read_layer_from_ssd(L)

    #         with self._cpu_lock:
    #             if epoch != self._epoch:
    #                 # çª—å£å·²å‰ç§»ï¼Œä¸¢å¼ƒè¿‡æœŸç»“æœ
    #                 self._inflight_cpu_layers.discard(L)
    #                 self._cpu_pf_q.task_done()
    #                 continue
    #             self._evict_if_over_hwm_locked(incoming=1)  # åªè¸¢çª—å£å¤–ï¼ˆè§ä¸‹ï¼‰
    #             self.cpu_cache[L] = tmp
    #             self._touch_cpu_lru_locked(L)

    #         self._inflight_cpu_layers.discard(L)
    #         self._cpu_pf_q.task_done()
    #         print(f"[WSM] âœ… Loaded layer {L} to CPU cache ({len(tmp)} params)")
    def _cpu_prefetch_worker(self):
        while not (self._stopped or self._stop_event.is_set()):
            try:
                item = self._cpu_pf_q.get(timeout=0.1)
            except queue.Empty:
                continue

            # æ”¯æŒå…³åœçš„å“¨å…µ
            if item is None or (isinstance(item, tuple) and item[1] is None):
                continue

            epoch, layer_idx = item

            # è¯» SSD â†’ ä¸´æ—¶å­—å…¸ï¼ˆä¸åŠ é”ï¼‰
            try:
                tmp = self._read_layer_from_ssd(layer_idx)
            except Exception as e:
                # è¯»å–å¤±è´¥ï¼Œæ¸…ç† inflight å¹¶ç»§ç»­
                with self._cpu_lock:
                    self._inflight_cpu_layers.discard(layer_idx)
                print(f"[WSM][cpu_pf] SSD read failed for layer {layer_idx}: {e}", flush=True)
                self._cpu_pf_q.task_done()
                continue

            # è½åœ°åˆ° CPU cacheï¼ˆåŠ é”ï¼‰ï¼Œè¿‡æœŸä»»åŠ¡ä¸¢å¼ƒ
            with self._cpu_lock:
                if epoch != self._epoch:
                    # çª—å£å·²å‰ç§»ï¼Œä¸¢å¼ƒè¿‡æœŸç»“æœ
                    self._inflight_cpu_layers.discard(layer_idx)
                    self._cpu_pf_q.task_done()
                    continue

                # å›æ»å¼æ”¶ç¼©ï¼šä»…è¸¢çª—å£å¤–ï¼ˆé¿å…æŠ–åŠ¨ï¼‰
                self._evict_if_over_hwm_locked(incoming=1)

                self.cpu_cache[layer_idx] = tmp
                # ç»´æŠ¤ LRUï¼šæœ€è¿‘ä½¿ç”¨ç§»åˆ°æœ«å°¾
                if layer_idx in self._cpu_lru:
                    self._cpu_lru.remove(layer_idx)
                self._cpu_lru.append(layer_idx)

                self._inflight_cpu_layers.discard(layer_idx)

            self._cpu_pf_q.task_done()
            print(f"[WSM] âœ… Loaded layer {layer_idx} to CPU cache ({len(tmp)} params)")


    def prefetch_group_async(self, layer_idx: int, group: str):
        if layer_idx < 0 or layer_idx >= self.n_layers:
            return

        # æ¨è¿›çª—å£ & å…¥é˜Ÿï¼ˆä¸åšåŒæ­¥ IOï¼‰
        try:
            self._advance_cpu_window_by_compute(layer_idx)
        except Exception:
            pass

        key = (layer_idx, group)
        # å·²åœ¨é£åˆ™è¿”å›
        if key in self._gpu_group_inflight:
            return

        evt = threading.Event()
        self._gpu_group_inflight[key] = evt

        def _task():
            try:
                # ç­‰ CPU å±‚ readyï¼ˆç»™ä¸ªçŸ­è¶…æ—¶ï¼‰
                self._wait_cpu_ready(layer_idx, timeout=5.0)

                # æ­£ç¡®å‘½ä¸­ DRAMï¼šå…ˆæŒ‰å±‚å–ï¼Œå†æŒ‰å
                layer_cache = self.cpu_cache.get(layer_idx, {})

                for suf in GROUPS[group]:
                    name = f"layers.{layer_idx}.{suf}"
                    src = layer_cache.get(name)
                    if src is None:
                        src = self._load_param_from_ssd(name)  # å…œåº•
                    self._move_to_gpu(name, src)  # èµ° weight_h2d æµ

                # è®°å±‚çº§ ready äº‹ä»¶ï¼ˆæ‰€æœ‰ H2D å…¥é˜Ÿä¹‹åï¼‰
                self._record_layer_ready_event(layer_idx)
                self._record_group_ready_event(layer_idx, group) 
                # æ›´æ–°ç»„ LRUï¼Œå¹¶æŒ‰ä¸Šé™é€å‡ºï¼ˆä¿æŠ¤å½“å‰ keyï¼‰
                # å…è®¸çŸ­æš‚è¶…é¢ï¼›è‹¥ç¡®å®è¦æ”¶ç¼©ï¼Œå°è¯•ä¸€æ¬¡é©±é€å³å¯
                with self._group_lock:
                    if key in self._gpu_group_lru:
                        self._gpu_group_lru.remove(key)
                    self._gpu_group_lru.append(key)
                    if len(self._gpu_group_lru) > self.gpu_max_groups:
                        ok = self._evict_one_group_from_gpu(exclude={key})
                        if not ok and self.verbose:
                            print(f"[WSM][prefetch] cannot evict under gpu_max_groups={self.gpu_max_groups}; allow temporary overflow={len(self._gpu_group_lru)}")

            except Exception as e:
                print(f"[WSM][prefetch_group_async] {layer_idx}/{group} failed: {e}", flush=True)
            finally:
                evt.set()
                self._gpu_group_inflight.pop(key, None)

        self._touch_group(layer_idx, group)
        
        if os.getenv("WSM_PRINT_GROUPS", "1") == "1":
            self.print_group_residency(current=(layer_idx, group),
                header="[WSM][groups][prefetch]")

        # åå°æ‰§è¡Œ
        t = threading.Thread(target=_task, name=f"wsm_gpf_{layer_idx}_{group}", daemon=True)
        self._threads.append(t)
        t.start()

    # def prefetch_group_async(self, layer_idx: int, group: str):
    #     """éé˜»å¡ï¼šåå°æŠŠ (layer_idx, group) ä» SSDâ†’CPUâ†’GPUã€‚å¤±è´¥æ—¶è¦èƒ½è‡ªæ„ˆï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹ã€‚"""
    #     if layer_idx < 0 or layer_idx >= self.n_layers:
    #         return

    #     # å…ˆæ¨è¿›çª—å£å¹¶å…¥é˜Ÿ CPU é¢„å–ï¼ˆç»ä¸é˜»å¡ï¼‰
    #     try:
    #         self._advance_cpu_window_by_compute(layer_idx)
    #     except Exception:
    #         # å…¼å®¹æ—§åˆ†æ”¯ï¼šç›´æ¥è°ƒ scheduleï¼ˆä¸æŠ›å‡ºï¼‰
    #         self._schedule_cpu_prefetch(layer_idx)

    #     key = (layer_idx, group)
    #     # è‹¥å·²æœ‰åŒç»„ä»»åŠ¡åœ¨é£ï¼Œç›´æ¥è¿”å›ï¼ˆé¿å…é‡å¤ï¼‰
    #     if key in self._gpu_group_inflight:
    #         return

    #     evt = threading.Event()
    #     self._gpu_group_inflight[key] = evt

    #     def _task():
    #         try:
    #             # ç­‰ CPU å±‚å°±ç»ªï¼ˆå†…éƒ¨æœ‰è¶…æ—¶åâ€œç«‹å³åŠ è½½â€çš„å…œåº•ï¼Œä½†é‚£æ˜¯åå°çº¿ç¨‹ï¼Œä¸ä¼šå µå‰å‘ï¼‰
    #             self._wait_cpu_ready(layer_idx, timeout=5.0)

    #             # ä» DRAM å–åˆ°è¿™ä¸€å±‚çš„å‚æ•°å­—å…¸
    #             layer_cache = self.cpu_cache.get(layer_idx, {})

    #             # é€ param å‘èµ· H2Dï¼ˆèµ°ç‹¬ç«‹ weight_h2d streamï¼Œéé˜»å¡ï¼‰
    #             for suf in GROUPS[group]:
    #                 name = f"layers.{layer_idx}.{suf}"
    #                 src = layer_cache.get(name)
    #                 if src is None:
    #                     # DRAM missï¼šå…œåº•ä» SSD è¯»â€œå•å‚æ•°â€
    #                     src = self._load_param_from_ssd(name)
    #                 self._move_to_gpu(name, src)  # å†…éƒ¨å·²ç”¨ weight_h2d stream

    #             # è®°å½•è¿™ä¸€å±‚çš„ ready äº‹ä»¶ï¼ˆæ‰€æœ‰ H2D å…¥é˜Ÿä¹‹åï¼‰
    #             self._record_layer_ready_event(layer_idx)

    #             # æ›´æ–°ç»„ LRUï¼ˆå—é™äº gpu_max_groupsï¼‰
    #             if key in self._gpu_group_lru:
    #                 self._gpu_group_lru.remove(key)
    #             self._gpu_group_lru.append(key)
    #             while len(self._gpu_group_lru) > self.gpu_max_groups:
    #                 self._evict_one_group_from_gpu(exclude={key})

    #         except Exception as e:
    #             # å‡ºé”™æ—¶æ‰“å°ä½†ä¸è¦å½±å“å‰å‘ï¼›æœ€é‡è¦çš„æ˜¯æŠŠ inflight äº‹ä»¶ç½®ä½ï¼Œé¿å…ä¸»çº¿ç¨‹æ­»ç­‰
    #             print(f"[WSM][prefetch_group_async] {layer_idx}/{group} failed: {e}", flush=True)
    #         finally:
    #             evt.set()
    #             # æ¸…ç† inflight æ ‡è®°
    #             self._gpu_group_inflight.pop(key, None)

    #     # åå°æ‰§è¡Œ
    #     self._bg_submit(_task)


    def _maybe_schedule_cpu_prefetch(self, cur_layer: int):
        # ç›®æ ‡çª—å£ [cur_layer+1, cur_layer+self.cpu_prefetch_distance]
        # ä½ å·²æœ‰çš„ CPU é¢„å–çº¿ç¨‹å¯åœ¨æ­¤æ›´æ–°ç›®æ ‡ï¼›æ²¡æœ‰åˆ™ç®€å•åœ°æŠŠè¯¥çª—å£ä¾æ¬¡ _load_layer_to_cpu()
        target = range(cur_layer+1, min(self.n_layers, cur_layer + 1 + self.cpu_prefetch_distance))
        for L in target:
            with self._cpu_lock:
                # åªå…è®¸â€œè®¡ç®—çº¿ç¨‹â€æ¨è¿›çª—å£ï¼ˆè¯¦è§ä¸‹ä¸€èŠ‚ï¼‰
                self._advance_cpu_window_by_compute(cur_layer)
                epoch = self._epoch
                L0 = self.cpu_win_base
                L1 = self.cpu_win_base + self.cpu_cache_cap - 1
                for L in range(L0, L1 + 1):
                    if L in self.cpu_cache or L in self._inflight_cpu_layers:
                        continue
                    self._inflight_cpu_layers.add(L)
                    self._cpu_pf_q.put((epoch, L)) 
        # åŒæ—¶é€æ­¥æ·˜æ±°è¾ƒæ—©çš„ CPU ç¼“å­˜ï¼ˆæ¯”å¦‚ < cur_layer-2ï¼‰
        self._evict_cpu_layers_older_than(cur_layer-2)

    def _wait_cpu_ready(self, layer_idx: int, timeout: float = 5.0):
        """ç­‰å¾…CPUç¼“å­˜å‡†å¤‡å¥½æŒ‡å®šå±‚"""
        import time
        start = time.time()
        while layer_idx not in self.cpu_cache:
            if time.time() - start > timeout:
                # è¶…æ—¶æ—¶å°è¯•ç«‹å³åŠ è½½
                if self.verbose:
                    print(f"[WSM] CPU cache layer {layer_idx} timeout, loading immediately...")
                try:
                    self._load_layer_to_cpu(layer_idx)
                    return
                except Exception as e:
                    raise TimeoutError(f"CPU cache layer {layer_idx} not ready after {timeout}s: {e}")
            time.sleep(0.01)

    def _get_param_from_model(self, layer_idx: int, param_suffix: str) -> Optional[torch.Tensor]:
        """
        ä»æ¨¡å‹æœ¬èº«è·å–å‚æ•°ï¼ˆä¼ ç»Ÿ streaming æ¨¡å¼ï¼‰
        param_suffix: ä¾‹å¦‚ "attention.wq.weight"
        è¿”å›: CPU tensor æˆ– None
        """
        try:
            if not hasattr(self.model, "layers") or layer_idx >= len(self.model.layers):
                return None

            layer = self.model.layers[layer_idx]

            # è§£æå‚æ•°è·¯å¾„ï¼šä¾‹å¦‚ "attention.wq.weight" -> ["attention", "wq", "weight"]
            parts = param_suffix.split('.')
            obj = layer
            for part in parts:
                if not hasattr(obj, part):
                    return None
                obj = getattr(obj, part)

            # obj åº”è¯¥æ˜¯ä¸€ä¸ª Parameter æˆ– Tensor
            if isinstance(obj, torch.nn.Parameter):
                tensor = obj.data
            elif isinstance(obj, torch.Tensor):
                tensor = obj
            else:
                return None

            # ç¡®ä¿æ˜¯ CPU tensorï¼ˆå¦‚æœå·²ç»åœ¨ GPU ä¸Šï¼Œè¿”å› None è®©å…¶ä»–é€»è¾‘å¤„ç†ï¼‰
            if tensor.device.type == "cpu" and tensor.numel() > 0:
                return tensor

            return None

        except Exception as e:
            if self.verbose:
                print(f"[WSM] Error getting param from model: layer={layer_idx}, suffix={param_suffix}, error={e}")
            return None

    def _load_param_from_ssd(self, param_name: str) -> torch.Tensor:
        """ä»SSDåŠ è½½å•ä¸ªå‚æ•°"""
        if not self.ssd_enabled:
            raise RuntimeError("SSD backend not enabled")

        # è§£æå‚æ•°åè·å–å±‚å·
        # ä¾‹å¦‚: "layers.5.attention.wq.weight"
        parts = param_name.split('.')
        if len(parts) < 3 or parts[0] != "layers":
            raise ValueError(f"Invalid param name: {param_name}")

        layer_idx = int(parts[1])
        if layer_idx not in self.layers_params:
            raise KeyError(f"Layer {layer_idx} not in manifest")

        # åœ¨manifestä¸­æŸ¥æ‰¾å‚æ•°
        for param_info in self.layers_params[layer_idx]:
            if param_info["name"] == param_name:
                # ä½¿ç”¨ç°æœ‰çš„SSDåŠ è½½é€»è¾‘
                from .weights_io_ssd_dram import DTYPE_MAP, alloc_pinned_aligned
                stride = param_info["stride"]
                offset = param_info["offset"]
                nbytes = param_info["nbytes"]

                # ç¡®ä¿staging bufferè¶³å¤Ÿå¤§
                if stride > len(self.staging_buffer):
                    block_size = self.ssd_manifest["block_size"]
                    new_size = ((stride + block_size - 1) // block_size) * block_size
                    self.staging_buffer = alloc_pinned_aligned(new_size, block_size)
                    if self.verbose:
                        print(f"[WSM] Expanded staging buffer to {new_size} bytes for {param_name}")

                # ä»SSDè¯»å–
                self.ssd_dio.pread_into_tensor(self.staging_buffer, stride, offset)

                # è½¬æ¢ä¸ºproper tensor
                param_tensor = torch.empty(
                    param_info["shape"],
                    dtype=DTYPE_MAP[param_info["dtype"]],
                    pin_memory=True
                )
                param_tensor.view(-1).view(torch.uint8)[:nbytes].copy_(
                    self.staging_buffer[:nbytes]
                )
                return param_tensor

        raise KeyError(f"Param {param_name} not found in layer {layer_idx} manifest")

    def _bg_submit(self, task):
        """æäº¤åå°ä»»åŠ¡åˆ°çº¿ç¨‹æ± """
        import threading
        t = threading.Thread(target=task, daemon=True)
        t.start()

    def _cpu_layer_ready(self, layer_idx: int) -> bool:
        """æ£€æŸ¥CPUå±‚æ˜¯å¦å°±ç»ª"""
        return layer_idx in self.cpu_cache

    def _evict_cpu_layers_older_than(self, layer_idx: int):
        """æ·˜æ±°æ—©äºæŒ‡å®šå±‚çš„CPUç¼“å­˜"""
        if not self.ssd_enabled:
            return

        with self.cpu_cache_lock:
            to_evict = [L for L in list(self.cpu_cache.keys()) if L < layer_idx]
            for L in to_evict:
                self.cpu_cache.pop(L, None)
                if self.verbose:
                    print(f"[WSM] Evicted CPU cache layer {L}")

    def warmup_groups_prefetch(self, layers: Optional[int] = None,
                            scheme: str = "doublet",
                            blocking_first: bool = True) -> None:
        """
        Warmup on GPU at *group* granularity (attn/ffn).

        Args:
            layers: è®¡åˆ’çƒ­èº«çš„å‰è‹¥å¹²å±‚ï¼ˆé»˜è®¤ä½¿ç”¨ warmup_layersï¼‰
            scheme: "doublet" => 0.attn,0.ffn,1.attn,1.ffn,...
                    "attn-first" => å…ˆé“ºä¸€åˆ— attnï¼Œå†é“º ffn
            blocking_first: ç¬¬ä¸€ä¸ªç»„ç”¨é˜»å¡å¼ ensureï¼Œç¡®ä¿è‡³å°‘ä¸€ç»„å°±ç»ª
        """
        if layers is None:
            layers = max(1, int(getattr(self, "warmup_layers", 1)))
        layers = min(layers, self.n_layers)

        # SSD æ¨¡å¼å…ˆæŠŠ CPU çª—å£æš–èµ·æ¥ï¼Œé¿å…é¦–æ¬¡ç»„è£…è½½ miss
        if self.ssd_enabled:
            try:
                self.warmup_cpu_cache()
            except Exception:
                self._ensure_cpu_window()

        # ç”Ÿæˆ warmup è®¡åˆ’ï¼ˆå— GPU ç»„é¢„ç®—çº¦æŸï¼‰
        plan: list[tuple[int, str]] = []
        if scheme == "attn-first":
            for i in range(layers):
                if len(plan) >= self.gpu_max_groups: break
                plan.append((i, "attn"))
            for i in range(layers):
                if len(plan) >= self.gpu_max_groups: break
                plan.append((i, "ffn"))
        else:  # "doublet"
            for i in range(layers):
                if len(plan) >= self.gpu_max_groups: break
                plan.append((i, "attn"))
                if len(plan) >= self.gpu_max_groups: break
                plan.append((i, "ffn"))

        # å»é‡ï¼ˆç†è®ºä¸Šä¸åº”é‡å¤ï¼Œè¿™é‡Œç¨³å¦¥å¤„ç†ä¸€ä¸‹ï¼‰
        uniq, seen = [], set()
        for item in plan:
            if item not in seen:
                seen.add(item)
                uniq.append(item)
        plan = uniq

        # æ‰§è¡Œï¼šé¦–ç»„é˜»å¡ç¡®ä¿å°±ç»ªï¼Œå…¶ä½™å¼‚æ­¥
        for k, (lid, grp) in enumerate(plan):
            if blocking_first and k == 0:
                self.ensure_group_on_gpu(lid, grp)     # é˜»å¡å¼
            else:
                self.prefetch_group_async(lid, grp)    # å¼‚æ­¥å¼

        if self.verbose:
            msg = ", ".join(f"L{l}.{g}" for (l, g) in plan)
            print(f"[WSM] Warmup group prefetch: {msg} | cap={self.gpu_max_groups}")
            try:
                self.print_group_residency(current=plan[0] if plan else None,
                                        header="[WSM][groups][warmup]")
            except Exception:
                pass
    

    def warmup_cpu_cache(self):
        """
        é¢„çƒ­ CPU cacheï¼šå¹‚ç­‰å¼åŠ è½½åˆå§‹çª—å£
        å¯è¢«å¤šæ¬¡è°ƒç”¨ï¼Œåªåœ¨é¦–æ¬¡æ‰§è¡Œå®é™…åŠ è½½
        """
        if self._warm_done:
            if self.verbose:
                print("[WSM] CPU warmup already done, skipping")
            return

        if self.verbose:
            print("[WSM] Starting CPU cache warmup...")

        # ä½¿ç”¨æ»‘åŠ¨çª—å£æœºåˆ¶åŠ è½½åˆå§‹çª—å£
        self._ensure_cpu_window()
        self._warm_done = True

        if self.verbose:
            print(f"[WSM] CPU warmup complete: {len(self.cpu_cache)} layers in cache")

    def prefetch(self, ids: List[int], warm: bool = False):
        """
        Asynchronously prefetch a list of layer indices, respecting the LRU budget.

        Args:
            ids: Layer indices to prefetch
            warm: If True, treat as warmup (idempotent, uses sliding window)
        """
        if warm:
            # é¢„çƒ­æ¨¡å¼ï¼šå¹‚ç­‰å¼åŠ è½½
            self.warmup_cpu_cache()
            return

        if not ids:
            return
        ids = ids[: max(1, min(self._pd_current, self.pd_cap))]
        nvtx.range_push(f"prefetch_layers_{ids}")

        for idx in ids:
            nvtx.range_push(f"prefetch_layer_{idx}")
            self._refresh_retain_window(idx)

            if idx in self.gpu_cache:
                self.gpu_cache.move_to_end(idx)
                nvtx.range_pop()
                continue

            # Evict until space is available
            while len(self.gpu_cache) >= self.max_cached_layers:
                self._evict_one_not_retained()

            # H2D for this prefetched layer (skip resident norms)
            nvtx.range_push(f"prefetch_h2d_layer_{idx}")
            for module_name, mod in self.block_mods[idx].items():
                if not module_name.startswith("norm_"):
                    nvtx.range_push(f"prefetch_h2d_{module_name}")
                    self._ensure_module_on_gpu(mod, idx, module_name)
                    nvtx.range_pop()
            nvtx.range_pop()

            self.gpu_cache[idx] = None

            # Record ready event (do not wait)
            nvtx.range_push(f"prefetch_record_event_{idx}")
            self._record_layer_ready_event(idx)
            nvtx.range_pop()

            if self.verbose:
                # åˆ¤æ–­æƒé‡ä»å“ªé‡ŒåŠ è½½
                source = "SSDâ†’CPUâ†’GPU" if (self.ssd_enabled and idx in self.cpu_cache) else \
                         "SSDâ†’GPU" if self.ssd_enabled else \
                         "CPUâ†’GPU"
                print(f"[WSM] prefetch layer={idx} ({source})")

            nvtx.range_pop()  # prefetch_layer

        nvtx.range_pop()  # prefetch_layers
        
        
    # --------- PD è‡ªé€‚åº”ï¼ˆæ»å›+EMAï¼‰ ---------
    def _decide_pd(self):
        """
        ä¼°è®¡ PCIE å¿™é—²ï¼ˆEMAï¼‰ï¼Œå¹¶ç»“åˆ pinned æ°´ä½ï¼ˆè‹¥å¯å¾—ï¼‰åšæ»å›è°ƒæ•´ï¼š
        - å¿™ï¼šPCIE>hi æˆ– pinned<low -> PD=1ï¼Œæš‚åœ KV å†™ throttle_ms
        - é—²ï¼šPCIE<lo ä¸” pinned>high -> PD=min(PD+1, cap)
        - ä¸­ï¼šä¿æŒ
        """
        # è¿‘ä¼¼ä¼°è®¡ PCIE åˆ©ç”¨ç‡ï¼šä»¥ H2D stream çš„ backlog ä¸æœ€è¿‘ H2D è§¦å‘ä¸º proxy
        busy_proxy = 1.0
        try:
            # å¦‚æœ H2D stream ä¸Šè¿˜æœ‰å·¥ä½œæœªå®Œæˆï¼Œåˆ™è¶‹å‘ 1ï¼Œå¦åˆ™è¶‹å‘ 0
            busy_proxy = 0.9 if (not self.streams.weight_h2d.query()) else 0.1
        except Exception:
            pass
        # EMA
        self._pcie_ema = self._ema_alpha * busy_proxy + (1.0 - self._ema_alpha) * self._pcie_ema

        # pinned æ°´ä½ï¼šæ­¤å¤„æ— æ³•è¯»å–ç³»ç»Ÿ pinned æ± ï¼Œé‡‡ç”¨ä¿å®ˆä¼°è®¡ 0.5ï¼›å¦‚æœä½ æœ‰ HostPinnedExtentPoolï¼Œå¯åœ¨å¤–å±‚æ³¨å…¥
        pinned_free_ratio = 0.5

        # å¿™æ€ï¼šé™ PDã€æš‚åœå†™
        if (self._pcie_ema >= self.pcie_hi) or (pinned_free_ratio <= self.pin_lo):
            # step-down, not cliff-drop
            self._pd_current = max(1, self._pd_current - 1)
            if self.kv_offloader is not None:
                try:
                    self.kv_offloader.throttle_writes_for(self.throttle_ms)
                except Exception:
                    pass
            return

        # é—²æ€ï¼šå‡ PD
        if (self._pcie_ema <= self.pcie_lo) and (pinned_free_ratio >= self.pin_hi):
            self._pd_current = min(self._pd_current + 1, self.pd_cap)
            return
        # ä¸­æ€ï¼šä¸å˜


    def _evict_layer_to_cpu(self, idx: int):
        """Evict a layer back to CPU (parameters: pointer switch; buffers: copied)."""
        for name, mod in self.block_mods[idx].items():
            # Keep norm modules resident on GPU.
            if name.startswith("norm_"):
                continue
            self._evict_module_to_cpu(mod)
        # Clear any stale ready event (will be recreated next time)
        self.layer_events.pop(idx, None)
        if self.verbose:
            print(f"[WSM]   evict layer={idx}")

    # -------- Compatibility shims for layers.py --------

    def ensure_weights_cuda(self, layer_id: int, modules: Dict[str, nn.Module], priority: bool = False):
        """
        Compatibility for layers.py: ensure provided layer's modules are on GPU.
        `priority=True` implies 'wait' for readiness.
        """
        if layer_id >= len(self.blocks):
            return
        self.ensure_on_gpu(layer_id, wait=priority)
        # Extra safety: if caller passes a different module dict, ensure those too.
        for name, module in modules.items():
            try:
                self._ensure_module_on_gpu(module, layer_id, name)
            except Exception as e:
                if self.verbose:
                    print(f"[WSM] Warning: ensure {name} on GPU failed (layer {layer_id}): {e}")

    def prefetch_weights(self, layer_ids: List[int], modules_dict: Dict[int, Dict[str, nn.Module]] = None):
        """Compatibility for layers.py: prefetch by layer ids."""
        self.prefetch(layer_ids)

    # -------- Memory stats / fragmentation reporting --------

    def _get_memory_info(self):
        """Return current CUDA allocator stats for the configured device."""
        if not torch.cuda.is_available():
            return None

        allocated = torch.cuda.memory_allocated(self.device)
        reserved = torch.cuda.memory_reserved(self.device)
        max_allocated = torch.cuda.max_memory_allocated(self.device)
        max_reserved = torch.cuda.max_memory_reserved(self.device)
        memory_stats = torch.cuda.memory_stats(self.device)

        return {
            "timestamp": time.time(),
            "allocated_mb": allocated / 1024**2,
            "reserved_mb": reserved / 1024**2,
            "max_allocated_mb": max_allocated / 1024**2,
            "max_reserved_mb": max_reserved / 1024**2,
            "fragmentation_ratio": (reserved - allocated) / reserved if reserved > 0 else 0.0,
            "allocation_count": memory_stats.get("allocation.all.current", 0),
            "segment_count": memory_stats.get("segment.all.current", 0),
            "large_pool_allocated": memory_stats.get("allocated_bytes.large_pool.current", 0) / 1024**2,
            "small_pool_allocated": memory_stats.get("allocated_bytes.small_pool.current", 0) / 1024**2,
        }

    def _record_memory_stats(self, operation: str, layer_id: int = -1):
        """Append a snapshot of allocator stats if monitoring is enabled."""
        if not self.monitor_fragmentation:
            return
        info = self._get_memory_info()
        if info:
            info["operation"] = operation
            info["layer_id"] = layer_id
            self.memory_stats.append(info)
            if info["fragmentation_ratio"] > self.fragmentation_threshold:
                print(
                    f"âš ï¸  High fragmentation: {info['fragmentation_ratio']:.3f} "
                    f"during {operation} (layer {layer_id})"
                )

    def get_fragmentation_report(self):
        """Summarize fragmentation/allocator information collected so far."""
        if not self.memory_stats:
            return {"error": "No memory statistics collected"}

        fragmentation_ratios = [s["fragmentation_ratio"] for s in self.memory_stats]
        segment_counts = [s["segment_count"] for s in self.memory_stats]

        report = {
            "total_operations": len(self.memory_stats),
            "max_fragmentation": max(fragmentation_ratios),
            "avg_fragmentation": sum(fragmentation_ratios) / len(fragmentation_ratios),
            "min_fragmentation": min(fragmentation_ratios),
            "max_segments": max(segment_counts),
            "avg_segments": sum(segment_counts) / len(segment_counts),
            "high_fragmentation_count": sum(1 for r in fragmentation_ratios if r > self.fragmentation_threshold),
            "peak_allocated_mb": max(s["allocated_mb"] for s in self.memory_stats),
            "peak_reserved_mb": max(s["reserved_mb"] for s in self.memory_stats),
        }

        if report["max_fragmentation"] > 0.4:
            report["severity"] = "CRITICAL"
        elif report["max_fragmentation"] > 0.3:
            report["severity"] = "HIGH"
        elif report["max_fragmentation"] > 0.15:
            report["severity"] = "MEDIUM"
        else:
            report["severity"] = "LOW"

        return report

    def save_memory_stats(self, filename: str):
        """Persist memory statistics + report to a JSON file."""
        if not self.memory_stats:
            print("âš ï¸  No memory statistics to save")
            return

        with open(filename, "w") as f:
            json.dump(
                {
                    "memory_stats": self.memory_stats,
                    "fragmentation_report": self.get_fragmentation_report(),
                    "config": {
                        "max_cached_layers": self.max_cached_layers,
                        "prefetch_distance": self.prefetch_distance,
                        "device": self.device,
                        "fragmentation_threshold": self.fragmentation_threshold,
                    },
                },
                f,
                indent=2,
            )
        print(f"ğŸ’¾ Memory statistics saved to {filename}")

    def clear_memory_stats(self):
        """Clear collected stats and reset CUDA peak tracking."""
        self.memory_stats.clear()
        torch.cuda.reset_peak_memory_stats(self.device)

    # -------- Cleanup and resource management --------

    def cleanup(self):
        """Clean up SSD resources and background threads"""
        if self.verbose:
            print("[WSM] Cleaning up resources...")

        # Stop prefetch thread
        if self.ssd_enabled:
            self.stop_prefetch.set()
            if self.prefetch_thread and self.prefetch_thread.is_alive():
                self.prefetch_thread.join(timeout=1.0)

        # Close SSD DirectIO handle
        if hasattr(self, 'ssd_dio') and self.ssd_dio:
            try:
                self.ssd_dio.close()
            except Exception:
                pass

        # Clear caches
        with self.cpu_cache_lock:
            self.cpu_cache.clear()
        self.gpu_cache.clear()

    def shutdown(self, wait: bool = True, timeout: float = 2.0):
        self._stopped = True
        self._stop_event.set()
        try:
            self._cpu_pf_q.put_nowait((self._epoch, None))  # å”¤é†’ worker
        except Exception:
            pass
        if wait:
            for t in list(self._threads):
                try:
                    t.join(timeout=timeout)
                except Exception:
                    pass

    def __del__(self):
        try:
            self.shutdown(wait=False)
        except Exception:
            pass
        
        
    def get_ssd_stats(self) -> Dict[str, Any]:
        """Get SSD backend statistics"""
        if not self.ssd_enabled:
            return {"ssd_enabled": False}

        return {
            "ssd_enabled": True,
            "cpu_cache_size": len(self.cpu_cache),
            "cpu_cached_layers": list(self.cpu_cache.keys()),
            "cpu_cache_max": self.cpu_cache_layers,
            "prefetch_queue_size": len(self.prefetch_queue),
            "staging_buffer_size": len(self.staging_buffer) if self.staging_buffer is not None else 0,
            "total_layers": len(self.layers_params),
        }
