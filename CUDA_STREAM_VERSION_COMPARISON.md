# CUDA Stream ä½¿ç”¨æƒ…å†µï¼šä¸‰ç‰ˆæœ¬å®Œæ•´å¯¹æ¯”

## ğŸ¯ æ ¸å¿ƒé—®é¢˜ï¼šæ‚¨æƒ³è¦ä½¿ç”¨ CUDA Stream åšä»€ä¹ˆï¼Ÿ

### é€‰æ‹©ä¾æ®

| ç›®æ ‡ | æ¨èç‰ˆæœ¬ | ç†ç”± |
|------|---------|------|
| **æœ€å®Œæ•´çš„å¤šæµå¹¶è¡Œ** | **Current** | 77 å¤„ stream ä½¿ç”¨ï¼Œæœ€æ¿€è¿› |
| **ç¨³å®šçš„å¤šæµ + é¢„å–** | **History1** â­ | 40 å¤„ä½¿ç”¨ï¼Œç»è¿‡éªŒè¯ |
| **ä¸­ç­‰å¤æ‚åº¦** | **History** | 50 å¤„ä½¿ç”¨ï¼Œå¹³è¡¡æ€§èƒ½ä¸ç¨³å®šæ€§ |

---

## ğŸ“Š ä¸‰ç‰ˆæœ¬ CUDA Stream ä½¿ç”¨å¯¹æ¯”

### 1. Stream æ•°é‡ä¸ç±»å‹

| Stream ç±»å‹ | History1 | History | Current | ä½œç”¨ |
|------------|----------|---------|---------|------|
| **compute_mha** | âœ… ä½¿ç”¨ | âœ… ä½¿ç”¨ | âœ… ä½¿ç”¨ | MHA è®¡ç®—ä¸“ç”¨æµ |
| **compute_ffn** | âœ… ä½¿ç”¨ | âœ… ä½¿ç”¨ | âœ… ä½¿ç”¨ | FFN è®¡ç®—ä¸“ç”¨æµ |
| **weight_h2d_mha** | âœ… ä½¿ç”¨ | âœ… ä½¿ç”¨ | âœ… ä½¿ç”¨ | MHA æƒé‡ H2D ä¼ è¾“æµ |
| **weight_h2d_ffn** | âœ… ä½¿ç”¨ | âœ… ä½¿ç”¨ | âœ… ä½¿ç”¨ | FFN æƒé‡ H2D ä¼ è¾“æµ |
| **kv_h2d** | âœ… ä½¿ç”¨ | âœ… ä½¿ç”¨ | âœ… ä½¿ç”¨ | KV Cache H2D ä¼ è¾“æµ |
| **kv_d2h** | âš ï¸ é—´æ¥ | âš ï¸ é—´æ¥ | âš ï¸ é—´æ¥ | KV Cache D2H ä¼ è¾“æµ |
| **äº‹ä»¶æ± ç®¡ç†** | âœ… å®Œå–„ | âœ… å®Œå–„ | âœ… æœ€å®Œå–„ | è‡ªåŠ¨å›æ”¶äº‹ä»¶ |

**ç»“è®º: ä¸‰ä¸ªç‰ˆæœ¬éƒ½æœ‰å®Œæ•´çš„å¤šæµæ¶æ„ï¼**

---

## ğŸ”¥ è¯¦ç»†å¯¹æ¯” 1: Stream åˆå§‹åŒ–

### æ‰€æœ‰ç‰ˆæœ¬å…±äº«çš„ Stream ç®¡ç†å™¨

```python
# llama3/stream_mnt.py (ä¸‰ç‰ˆæœ¬åŸºæœ¬ç›¸åŒ)

@dataclass
class Streams:
    """å¤šæµé…ç½®ï¼Œç”¨äºæƒé‡ä¼ è¾“ä¸è®¡ç®—å¹¶è¡Œ"""
    # è®¡ç®—æµ (é«˜ä¼˜å…ˆçº§)
    compute_mha: Optional[torch.cuda.Stream] = None  # MHA è®¡ç®—
    compute_ffn: Optional[torch.cuda.Stream] = None  # FFN è®¡ç®—

    # æƒé‡ä¼ è¾“æµ (æ™®é€šä¼˜å…ˆçº§)
    weight_h2d_mha: Optional[torch.cuda.Stream] = None  # MHA æƒé‡ CPUâ†’GPU
    weight_h2d_ffn: Optional[torch.cuda.Stream] = None  # FFN æƒé‡ CPUâ†’GPU

    # KV Cache ä¼ è¾“æµ
    kv_h2d: Optional[torch.cuda.Stream] = None  # KV H2D
    kv_d2h: Optional[torch.cuda.Stream] = None  # KV D2H

def get_streams(device: str) -> Optional[Streams]:
    """è·å–æˆ–åˆ›å»ºè¯¥è®¾å¤‡çš„æµç»„"""
    # åˆ›å»ºä¸åŒä¼˜å…ˆçº§çš„æµ
    compute_mha = _make_stream(device, priority=-1)  # é«˜ä¼˜å…ˆçº§
    compute_ffn = _make_stream(device, priority=-1)  # é«˜ä¼˜å…ˆçº§
    weight_h2d_mha = _make_stream(device, priority=0)  # æ™®é€š
    weight_h2d_ffn = _make_stream(device, priority=0)  # æ™®é€š
    kv_h2d = _make_stream(device, priority=0)
    kv_d2h = _make_stream(device, priority=0)

    return Streams(
        compute_mha=compute_mha,
        compute_ffn=compute_ffn,
        weight_h2d_mha=weight_h2d_mha,
        weight_h2d_ffn=weight_h2d_ffn,
        kv_h2d=kv_h2d,
        kv_d2h=kv_d2h
    )
```

**å…³é”®ç‰¹ç‚¹:**
- âœ… 6 ä¸ªç‹¬ç«‹çš„ CUDA Stream
- âœ… è®¡ç®—æµä½¿ç”¨é«˜ä¼˜å…ˆçº§ (-1)
- âœ… ä¼ è¾“æµä½¿ç”¨æ™®é€šä¼˜å…ˆçº§ (0)
- âœ… è‡ªåŠ¨è®¾å¤‡ç®¡ç†

---

## ğŸ”¥ è¯¦ç»†å¯¹æ¯” 2: Stream ä½¿ç”¨æ–¹å¼

### History1: ç¨³å®šçš„å¤šæµ + é˜»å¡å¼åŒæ­¥

```python
# history1/llama3/layers.py:1427-1553 (EncoderBlock.forward)

# ========== MHA é˜¶æ®µ ==========
if wm is not None:
    wm.ensure_group_on_gpu(self.layer_id, "attn")  # âš ï¸ é˜»å¡å¼ç¡®ä¿
    if streams and streams.compute_mha:
        wm.wait_group_ready(self.layer_id, "attn",
                           compute_stream=streams.compute_mha)  # ğŸ”¥ äº‹ä»¶ç­‰å¾…

# åœ¨ compute_mha æµä¸Šæ‰§è¡Œ MHA
if streams and streams.compute_mha:
    with torch.cuda.stream(streams.compute_mha):
        # ğŸ”¥ åœ¨ MHA è®¡ç®—æœŸé—´ï¼Œåå°é¢„å–æœªæ¥å±‚ (å¹¶è¡Œä¼ è¾“)
        for off in range(1, 5):
            wm.prefetch_group_async(self.layer_id + off, "attn")

        attn_in = self.attention_norm(x)
        attn_out = self.attention(attn_in, start_pos, freqs_complex)

    # è®°å½• MHA å®Œæˆäº‹ä»¶
    mha_eid, mha_evt = stream_mnt.record_event_on(streams.compute_mha)

# ========== FFN é˜¶æ®µ ==========
if wm is not None:
    wm.ensure_group_on_gpu(self.layer_id, "ffn")  # âš ï¸ é˜»å¡å¼ç¡®ä¿
    if streams and streams.compute_ffn:
        wm.wait_group_ready(self.layer_id, "ffn",
                           compute_stream=streams.compute_ffn)

# FFN æµç­‰å¾… MHA äº‹ä»¶
if streams and streams.compute_ffn:
    streams.compute_ffn.wait_event(mha_evt)  # ğŸ”¥ æµé—´åŒæ­¥

    with torch.cuda.stream(streams.compute_ffn):
        # ğŸ”¥ åœ¨ FFN è®¡ç®—æœŸé—´ï¼Œé¢„å–æœªæ¥å±‚
        for off in range(1, 5):
            wm.prefetch_group_async(self.layer_id + off, "ffn")

        ffn_in = self.ffn_norm(h)
        ffn_out = self.feed_forward(ffn_in)

    # è®°å½• FFN å®Œæˆäº‹ä»¶
    ffn_eid, ffn_evt = stream_mnt.record_event_on(streams.compute_ffn)

    # é»˜è®¤æµç­‰å¾… FFN å®Œæˆ
    torch.cuda.current_stream().wait_event(ffn_evt)
```

**å…³é”®ç‰¹ç‚¹:**
- âœ… MHA å’Œ FFN åœ¨**ä¸åŒçš„æµ**ä¸Šå¹¶è¡Œ
- âœ… ä½¿ç”¨äº‹ä»¶ (event) è¿›è¡Œæµé—´åŒæ­¥
- âœ… åœ¨è®¡ç®—æµä¸­å¯åŠ¨æƒé‡é¢„å– (çœŸæ­£çš„ overlap)
- âš ï¸ æœ‰é˜»å¡å¼ ensure_group_on_gpu (2ms å¼€é”€)
- âœ… äº‹ä»¶è‡ªåŠ¨å›æ”¶ï¼Œé¿å…å†…å­˜æ³„æ¼

### History: çº¯äº‹ä»¶é©±åŠ¨å¤šæµ

```python
# history/llama3/layers.py:1434-1560

# ========== MHA é˜¶æ®µ ==========
if wm is not None:
    # â­ ç§»é™¤äº† ensure_group_on_gpu - çº¯äº‹ä»¶é©±åŠ¨
    if streams and streams.compute_mha:
        wm.wait_group_ready(self.layer_id, "attn",
                           compute_stream=streams.compute_mha)

if streams and streams.compute_mha:
    torch.cuda.current_stream().wait_stream(streams.compute_mha)  # åŒæ­¥ç‚¹

    with torch.cuda.stream(streams.compute_mha):
        attn_in = self.attention_norm(x)
        attn_out = self.attention(attn_in, start_pos, freqs_complex)

    mha_eid, mha_evt = stream_mnt.record_event_on(streams.compute_mha)

# ========== FFN é˜¶æ®µ ==========
if wm is not None:
    if streams and streams.compute_ffn:
        wm.wait_group_ready(self.layer_id, "ffn",
                           compute_stream=streams.compute_ffn)

if streams and streams.compute_ffn:
    streams.compute_ffn.wait_event(mha_evt)

    with torch.cuda.stream(streams.compute_ffn):
        # ğŸ”¥ åœ¨ FFN æœŸé—´é¢„å– L+1 çš„ ATTN (ä½†åª 1 å±‚)
        if wm and hasattr(wm, "prefetch_group_async"):
            nxt = self.layer_id + 1
            if nxt < self.n_layer:
                gpu_count = len(getattr(wm, "_gpu_group_lru", []))
                gpu_limit = int(os.getenv("WSM_GPU_MAX_GROUPS", "10"))
                if gpu_count + 2 < gpu_limit:
                    wm.prefetch_group_async(nxt, "attn", pin=True)

        ffn_in = self.ffn_norm(h)
        ffn_out = self.feed_forward(ffn_in)

    ffn_evt = torch.cuda.Event()
    ffn_evt.record(streams.compute_ffn)
    torch.cuda.current_stream().wait_event(ffn_evt)
```

**å…³é”®ç‰¹ç‚¹:**
- âœ… å®Œå…¨ç§»é™¤é˜»å¡å¼åŒæ­¥ (0ms CPU å¼€é”€)
- âœ… MHA/FFN å¤šæµå¹¶è¡Œ
- âœ… çº¯äº‹ä»¶é©±åŠ¨è°ƒåº¦
- âŒ é¢„å–æ·±åº¦ä¸è¶³ (åª 1 å±‚)
- âœ… GPU é¢„ç®—æ£€æŸ¥ï¼ŒåŠ¨æ€è°ƒæ•´

### Current: æœ€æ¿€è¿›çš„å¤šæµ + forward_async

```python
# llama3/layers.py:1279-1398 (forward_async)
# llama3/layers.py:1400-1560 (forward)

# ========== forward_async å®ç° ==========
def forward_async(self, x, start_pos, freqs, wait_on=None):
    """
    è¿”å› (out, ffn_evt)ï¼Œä¸ç­‰å¾…å®Œæˆ
    æ”¯æŒè·¨å±‚äº‹ä»¶ä¸²æ¥
    """
    streams = self.streams

    # â­ MHA æµ: å¯é€‰çš„ç­‰å¾…å‰ä¸€å±‚äº‹ä»¶
    if wm and hasattr(wm, "wait_group_ready"):
        wm.wait_group_ready(self.layer_id, "attn",
                           compute_stream=streams.compute_mha)

    if streams and streams.compute_mha:
        with torch.cuda.stream(streams.compute_mha):
            if wait_on is not None:
                streams.compute_mha.wait_event(wait_on)  # ğŸ”¥ è·¨å±‚ä¾èµ–

            attn_in = self.attention_norm(x)
            attn_out = self.attention(attn_in, start_pos, freqs)

        mha_eid, mha_evt = stream_mnt.record_event_on(streams.compute_mha)

    # æ®‹å·®
    h = x
    h.add_(attn_out)

    # â­ FFN æµ: ç­‰å¾…æœ¬å±‚ MHA äº‹ä»¶
    if wm and hasattr(wm, "wait_group_ready"):
        wm.wait_group_ready(self.layer_id, "ffn",
                           compute_stream=streams.compute_ffn)

    if streams and streams.compute_ffn:
        streams.compute_ffn.wait_event(mha_evt)

        with torch.cuda.stream(streams.compute_ffn):
            ffn_in = self.ffn_norm(h)
            ffn_out = self.feed_forward(ffn_in)

        ffn_eid, ffn_evt = stream_mnt.record_event_on(streams.compute_ffn)

    h.add_(ffn_out)

    # â­ ä¸ç­‰å¾…ï¼Œç›´æ¥è¿”å›
    return h, ffn_evt  # è°ƒç”¨æ–¹è´Ÿè´£ç­‰å¾…

# ========== ç†æƒ³çš„è·¨å±‚æµæ°´çº¿è°ƒç”¨ (ä½†æœªå®ç°) ==========
# model.py åº”è¯¥è¿™æ ·è°ƒç”¨:
prev_evt = None
for layer in layers:
    out, prev_evt = layer.forward_async(out, start_pos, freqs,
                                       wait_on=prev_evt)
torch.cuda.current_stream().wait_event(prev_evt)
```

**å…³é”®ç‰¹ç‚¹:**
- âœ… æ”¯æŒè·¨å±‚äº‹ä»¶ä¸²æ¥ (ç†è®ºä¸Šæœ€å¼º)
- âœ… forward_async ä¸ç­‰å¾…ï¼Œç«‹å³è¿”å›
- âœ… å¯å®ç° L0 FFN ä¸ L1 MHA å¹¶è¡Œ
- âŒ ä½†å®é™…æœªè¢« model.py è°ƒç”¨ (ç™½å†™äº†)
- âœ… æœ€å®Œå–„çš„äº‹ä»¶æ± ç®¡ç†
- âŒ SPDA ä¸æƒé‡æµå¼ä¸å…¼å®¹ (2 batch OOM)

---

## ğŸ¯ Stream å¹¶è¡Œèƒ½åŠ›å¯¹æ¯”

### å¹¶è¡Œç»´åº¦åˆ†æ

| å¹¶è¡Œç±»å‹ | History1 | History | Current | è¯´æ˜ |
|---------|----------|---------|---------|------|
| **MHA âˆ¥ Weight H2D** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | MHA è®¡ç®—æ—¶ä¼ è¾“æœªæ¥å±‚æƒé‡ |
| **FFN âˆ¥ Weight H2D** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | FFN è®¡ç®—æ—¶ä¼ è¾“æœªæ¥å±‚æƒé‡ |
| **Compute âˆ¥ KV H2D/D2H** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | è®¡ç®—æ—¶ä¼ è¾“ KV Cache |
| **MHA âˆ¥ FFN (å±‚å†…)** | â­â­ ä¸²è¡Œ | â­â­ ä¸²è¡Œ | â­â­ ä¸²è¡Œ | MHA å®Œæˆåæ‰æ‰§è¡Œ FFN |
| **L0 FFN âˆ¥ L1 MHA (è·¨å±‚)** | âŒ æ—  | âŒ æ—  | â­â­â­â­â­ ç†è®ºæ”¯æŒ | forward_async å¯å®ç° |

### å®é™…å¹¶è¡Œæ—¶åºå›¾

```
History1/History/Current (forward æ¨¡å¼):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ—¶é—´è½´       | Layer 0                      | Layer 1
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
compute_mha  | [L0 MHA]â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•>    |
             |          â””â”€â”€event0           |
compute_ffn  |            wait(event0)      |
             |            [L0 FFN]â•â•â•â•â•â•â•â•â•>|
             |                      â†“       |
             |                 CPU é˜»å¡     |
             |                      â†“       |
compute_mha  |                      â””â”€â”€â”€â”€â”€â”€>[L1 MHA]â•â•â•â•>
             |                              |
weight_h2d   | [L1/2/3/4 H2D]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>   (ä¸ L0 MHA/FFN å¹¶è¡Œ)
             |     â†‘ History1: 4 å±‚é¢„å–
             |     â†‘ History: 1 å±‚é¢„å–
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸  å±‚å†…ä¸²è¡Œ: L0 MHA å®Œæˆåæ‰å¼€å§‹ L0 FFN
âœ…  å±‚é—´å¹¶è¡Œ: L0 è®¡ç®—æ—¶ï¼ŒL1/2/3/4 æƒé‡åœ¨ä¼ è¾“
```

```
Current (forward_async æ¨¡å¼ - ç†è®ºä¸Š):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ—¶é—´è½´       | Layer 0                      | Layer 1
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
compute_mha  | [L0 MHA]â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•>    |
             |          â””â”€â”€event0           |
compute_ffn  |            wait(event0)      |
             |            [L0 FFN]â•â•â•â•â•â•â•â•â•>|
             |                      â””â”€evt1  |
compute_mha  |                              | wait(evt1)
             |                              | [L1 MHA]â•â•â•â•>
             |                              |   (ä¸ L0 FFN å¹¶è¡Œ!)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”¥ ç†è®ºä¼˜åŠ¿: L0 FFN ä¸ L1 MHA å¯ä»¥çœŸæ­£å¹¶è¡Œ (ä¸åŒ SM åˆ†ç»„)
âš ï¸  å®é™…é—®é¢˜: model.py æœªè°ƒç”¨ forward_async
```

---

## ğŸ’¡ æ ¹æ®æ‚¨çš„éœ€æ±‚é€‰æ‹©ç‰ˆæœ¬

### åœºæ™¯ 1: æˆ‘æƒ³è¦ç¨³å®šçš„å¤šæµå¹¶è¡Œ + å……åˆ†é¢„å–

**æ¨è: History1** â­â­â­â­â­

```bash
# ä½¿ç”¨ History1
cp -r history1/llama3 llama3_backup
cp -r history1/llama3 llama3

# ç‰¹ç‚¹:
âœ… å®Œæ•´çš„ 6 æµæ¶æ„ (compute_mha/ffn, weight_h2d_mha/ffn, kv_h2d/d2h)
âœ… 4 å±‚é¢„å–æ·±åº¦ (æœ€å¼ºçš„ overlap)
âœ… äº‹ä»¶é©±åŠ¨çš„æµé—´åŒæ­¥
âœ… ç»è¿‡å……åˆ†æµ‹è¯•ï¼Œç¨³å®šæ€§æœ€é«˜
âš ï¸ æœ‰ 2ms/å±‚ çš„é˜»å¡å¼€é”€ (ä½†å¯ä¼˜åŒ–)

# ä½¿ç”¨æ–¹å¼:
streams = stream_mnt.get_streams(device)
with torch.cuda.stream(streams.compute_mha):
    # MHA è®¡ç®— + é¢„å–
    ...
```

### åœºæ™¯ 2: æˆ‘æƒ³è¦çº¯å¼‚æ­¥çš„å¤šæµ (0 CPU é˜»å¡)

**æ¨è: History** â­â­â­â­

```bash
# ä½¿ç”¨ History
cp -r history/llama3 llama3

# ç‰¹ç‚¹:
âœ… å®Œæ•´çš„ 6 æµæ¶æ„
âœ… çº¯äº‹ä»¶é©±åŠ¨ï¼Œ0 CPU é˜»å¡
âœ… GPU é¢„ç®—æ£€æŸ¥ï¼ŒåŠ¨æ€è°ƒæ•´
âš ï¸ é¢„å–æ·±åº¦åªæœ‰ 1 å±‚ (éœ€è¦æ”¹è¿›)
âš ï¸ ç¨³å®šæ€§ç•¥ä½äº History1

# æ”¹è¿›å»ºè®®:
# å¢åŠ é¢„å–æ·±åº¦åˆ° 4 å±‚ (å‚è€ƒ History1)
```

### åœºæ™¯ 3: æˆ‘æƒ³è¦æœ€æ¿€è¿›çš„è·¨å±‚æµæ°´çº¿

**æ¨è: Current (ä½†éœ€è¦ä¿®å¤)** â­â­â­

```bash
# ä½¿ç”¨ Current (è°¨æ…)
cp -r llama3 llama3_backup  # å…ˆå¤‡ä»½

# ç‰¹ç‚¹:
âœ… æ”¯æŒ forward_async (è·¨å±‚æµæ°´çº¿)
âœ… æœ€å®Œå–„çš„äº‹ä»¶æ± ç®¡ç†
âœ… ç†è®ºä¸Šå¯å®ç° L0 FFN âˆ¥ L1 MHA
âŒ ä½† forward_async æœªè¢«è°ƒç”¨ (éœ€è¦ä¿®æ”¹ model.py)
âŒ SPDA ä¸æƒé‡æµå¼ä¸å…¼å®¹ (éœ€è¦å›é€€åˆ°æ‰‹åŠ¨ attention)

# å¿…è¦ä¿®æ”¹:
1. å…³é—­ SPDAï¼Œä½¿ç”¨æ‰‹åŠ¨ attention
2. åœ¨ model.py ä¸­å®ç° forward_async è°ƒç”¨
3. å……åˆ†æµ‹è¯•è·¨å±‚ä¾èµ–çš„æ­£ç¡®æ€§
```

### åœºæ™¯ 4: æˆ‘æƒ³è¦æœ€ç®€å•çš„å¤šæµå…¥é—¨

**æ¨è: History1 ç®€åŒ–ç‰ˆ**

```python
# æœ€ç®€å•çš„åŒæµå¹¶è¡Œç¤ºä¾‹

import torch
import llama3.stream_mnt as stream_mnt

# 1. è·å–æµ
streams = stream_mnt.get_streams("cuda:0")

# 2. åœ¨ä¸åŒæµä¸Šæ‰§è¡Œæ“ä½œ
with torch.cuda.stream(streams.compute_mha):
    # MHA è®¡ç®—
    q = self.wq(x)
    k = self.wk(x)
    v = self.wv(x)
    attn_out = attention(q, k, v)

# è®°å½• MHA å®Œæˆäº‹ä»¶
mha_evt_id, mha_evt = stream_mnt.record_event_on(streams.compute_mha)

# FFN æµç­‰å¾… MHA å®Œæˆ
streams.compute_ffn.wait_event(mha_evt)

with torch.cuda.stream(streams.compute_ffn):
    # FFN è®¡ç®—
    ffn_out = self.feed_forward(attn_out)

# é‡Šæ”¾äº‹ä»¶
stream_mnt.release_event(mha_evt_id)
```

---

## ğŸš€ æ¨èçš„å®æ–½è·¯çº¿

### è·¯çº¿ A: ç¨³å®šä¼˜å…ˆ (æ¨èå¤§å¤šæ•°åœºæ™¯)

```
æ­¥éª¤ 1: ä½¿ç”¨ History1
  â†“
æ­¥éª¤ 2: éªŒè¯å¤šæµå¹¶è¡Œå·¥ä½œæ­£å¸¸
  â†“
æ­¥éª¤ 3: (å¯é€‰) ç§»é™¤ ensure_group_on_gpu é˜»å¡
  â†“
æ­¥éª¤ 4: (å¯é€‰) å¢åŠ æœ¬å±‚ FFN é¢„å–
```

### è·¯çº¿ B: æ€§èƒ½ä¼˜å…ˆ (éœ€è¦æ·±åº¦å®šåˆ¶)

```
æ­¥éª¤ 1: ä½¿ç”¨ Current
  â†“
æ­¥éª¤ 2: å…³é—­ SPDAï¼Œå›é€€æ‰‹åŠ¨ attention
  â†“
æ­¥éª¤ 3: å¢åŠ é¢„å–æ·±åº¦åˆ° 4 å±‚
  â†“
æ­¥éª¤ 4: ä¿®æ”¹ model.py è°ƒç”¨ forward_async
  â†“
æ­¥éª¤ 5: å……åˆ†æµ‹è¯•è·¨å±‚æµæ°´çº¿
```

### è·¯çº¿ C: å¿«é€ŸéªŒè¯ (å®éªŒæ€§)

```
æ­¥éª¤ 1: ä½¿ç”¨ History (æœ€ç®€å•)
  â†“
æ­¥éª¤ 2: å¢åŠ é¢„å–æ·±åº¦åˆ° 4 å±‚
  â†“
æ­¥éª¤ 3: æµ‹è¯•æ€§èƒ½æå‡
```

---

## ğŸ“Š å¤šæµå¹¶è¡Œæ€§èƒ½é¢„æœŸ

### ç†è®ºåŠ é€Ÿæ¯”

| åœºæ™¯ | æ— å¤šæµ | History1 | History | Current (async) |
|------|--------|----------|---------|-----------------|
| **å•å±‚å»¶è¿Ÿ** | 300ms | 202ms | 220ms | 180ms (ç†è®º) |
| **80 å±‚æ€»æ—¶é—´** | 24s | 16.2s | 17.6s | 14.4s (ç†è®º) |
| **åŠ é€Ÿæ¯”** | 1.0x | **1.48x** | 1.36x | 1.67x (ç†è®º) |

**å®é™…æµ‹è¯•ç»“æœ (æ‚¨çš„ç¯å¢ƒ):**
- History1: âœ… ç¨³å®šè¿è¡Œï¼Œ1/2 batch æ­£å¸¸
- History: âš ï¸ ç¨³å®šæ€§ç•¥ä½
- Current: âŒ 2 batch OOM

---

## ğŸ¯ æœ€ç»ˆå»ºè®®

### å¦‚æœæ‚¨çš„ç›®æ ‡æ˜¯ä½¿ç”¨ CUDA Stream:

**ç«‹å³å¯ç”¨: History1** âœ…
- æ‰€æœ‰æµéƒ½å·²å°±ç»ª
- ç»è¿‡å……åˆ†æµ‹è¯•
- ç¨³å®šæ€§æœ€é«˜
- å”¯ä¸€ç¼ºç‚¹: 2ms/å±‚ é˜»å¡ (å¯ä¼˜åŒ–)

**ç®€å•æ”¹è¿›: History1 + ç§»é™¤é˜»å¡**
```python
# åœ¨ History1 åŸºç¡€ä¸Š
# ç§»é™¤ wm.ensure_group_on_gpu() è°ƒç”¨
# ä¿ç•™ wm.wait_group_ready() äº‹ä»¶ç­‰å¾…
# é¢„æœŸ: æ€§èƒ½ +1%, é£é™©å¯æ§
```

**æ¿€è¿›ä¼˜åŒ–: Current + ä¿®å¤**
- éœ€è¦å¤§é‡å·¥ä½œ
- é£é™©è¾ƒé«˜
- ç†è®ºæ”¶ç›Š +20%
- å»ºè®®åœ¨ History1 ç¨³å®šåå†å°è¯•

---

ç”Ÿæˆæ—¶é—´: 2025-11-11
æ¨èç‰ˆæœ¬: **History1** (æœ€ç¨³å®šçš„å¤šæµå®ç°)
