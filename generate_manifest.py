#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿæˆruntime manifestçš„è„šæœ¬ï¼ˆæ–°å¢ from-shards-streamï¼šå¯¹ Meta åˆ†ç‰‡é€å±‚streamingåˆå¹¶å¹¶ç›´æ¥æ‰“åŒ…ï¼‰

ç”¨é€”:
1) from-hf              : ä» HF æ¨¡å‹ç›®å½•/ID å¯¼å‡ºæ•´åˆæƒé‡ -> æ‰“åŒ…åˆ° raw è®¾å¤‡ -> ç”Ÿæˆ manifest
2) from-checkpoint      : ä»å•ä¸ª consolidated.pth/.safetensors æ‰“åŒ… -> ç”Ÿæˆ manifest
3) from-meta            : ä» shapes_meta.json ç”Ÿæˆ runtime_manifest.json
4) template             : ç”Ÿæˆ manifest æ¨¡æ¿
5) from-shards-stream   : â˜… æ¨èï¼šä» Meta åˆ†ç‰‡ç›®å½•ï¼ˆconsolidated.00..NN.pthï¼‰é€å±‚ streaming åˆå¹¶åç›´æ¥æ‰“åŒ…ï¼ˆä¸ä¼šOOMï¼‰

ç¤ºä¾‹:
  # é€å±‚streamingï¼ˆä¸ä¼šæŠŠå…¨æ¨¡å‹ä¸€æ¬¡æ€§è½½å…¥å†…å­˜ï¼‰
  python generate_manifest.py from-shards-stream /home/roger/.llama/checkpoints/Llama3.1-70B /dev/nvme0n1p4 \
      --meta-out /data1/70b-fixed.shapes_meta.json \
      --manifest-out /data1/70b-fixed.runtime_manifest.json \
      --yes
"""

import sys
import os
import re
import json
import glob
import argparse
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, Iterable

import torch

# é¡¹ç›®å†…æ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from llama3.weights_io_ssd_dram import (
    build_runtime_manifest,
    pack_any_to_raw,
    get_logical_block_size,
    O_DIRECT,
    O_LARGEFILE,
)

# -------------------------
# é€šç”¨å·¥å…·
# -------------------------

def extract_model_name(ckpt_path: str) -> str:
    path = Path(ckpt_path)
    name = path.name if path.is_dir() else path.stem
    name = name.lower()
    name = re.sub(r'[^a-z0-9.-]', '-', name)
    name = re.sub(r'-+', '-', name).strip('-')
    return name or "llama-model"

def check_raw_device(device_path: str) -> None:
    print(f"\nğŸ” æ£€æŸ¥è®¾å¤‡: {device_path}")
    if not Path(device_path).exists():
        print(f"âŒ è®¾å¤‡ä¸å­˜åœ¨: {device_path}")
        print(f"\nå¯ç”¨çš„å—è®¾å¤‡:")
        os.system("lsblk | grep -E 'nvme|sd'")
        sys.exit(1)
    try:
        fd = os.open(device_path, os.O_RDONLY | O_DIRECT | O_LARGEFILE)
        block_size = get_logical_block_size(fd)
        os.close(fd)
        print(f"âœ… è®¾å¤‡å¯è®¿é—®")
        print(f"   å—å¤§å°: {block_size} bytes")
    except PermissionError:
        print(f"âŒ æƒé™ä¸è¶³ï¼Œè¯·ä½¿ç”¨ sudo è¿è¡Œ")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ— æ³•è®¿é—®è®¾å¤‡: {e}")
        sys.exit(1)

def _load_model_args_from_dir(path: str) -> Dict[str, int]:
    p = Path(path)
    root = p if p.is_dir() else p.parent
    for fname in ("params.json", "config.json"):
        f = root / fname
        if f.exists():
            js = json.loads(f.read_text(encoding="utf-8"))
            dim        = int(js.get("dim") or js.get("hidden_size"))
            n_layers   = int(js.get("n_layers") or js.get("num_hidden_layers"))
            n_heads    = int(js.get("n_heads") or js.get("num_attention_heads"))
            n_kv_heads = int(js.get("n_kv_heads") or js.get("num_key_value_heads") or n_heads)
            head_dim   = dim // n_heads
            inter      = js.get("intermediate_size") or js.get("ffn_hidden_size")
            inter      = int(inter) if inter is not None else None
            return {
                "dim": dim, "n_layers": n_layers, "n_heads": n_heads,
                "n_kv_heads": n_kv_heads, "head_dim": head_dim,
                "intermediate_size": inter
            }
    raise FileNotFoundError(f"params.json/config.json not found beside {path}")

def _hf_to_internal_name(name: str) -> str:
    n = name
    if n.startswith("model."): n = n[len("model."):]
    n = n.replace(".self_attn.q_proj.", ".attention.wq.")
    n = n.replace(".self_attn.k_proj.", ".attention.wk.")
    n = n.replace(".self_attn.v_proj.", ".attention.wv.")
    n = n.replace(".self_attn.o_proj.", ".attention.wo.")
    n = n.replace(".input_layernorm.", ".attention_norm.")
    n = n.replace(".post_attention_layernorm.", ".ffn_norm.")
    n = n.replace(".mlp.gate_proj.", ".feed_forward.w1.")
    n = n.replace(".mlp.up_proj.",   ".feed_forward.w3.")
    n = n.replace(".mlp.down_proj.", ".feed_forward.w2.")
    if n == "model.embed_tokens.weight": n = "embed_tokens.weight"
    if n == "lm_head.weight": n = "output.weight"
    return n

# -------------------------
# ç°æœ‰åŠŸèƒ½ï¼šfrom-meta / from-checkpoint / from-hf / template
# ï¼ˆä¿æŒä¸å˜ï¼Œç•¥å»å·²æœ‰å®ç°ï¼‰
# -------------------------

def generate_from_shapes_meta(shapes_meta_path: str, output_path: Optional[str] = None) -> None:
    if not Path(shapes_meta_path).exists():
        raise FileNotFoundError(f"âŒ shapes_metaæ–‡ä»¶ä¸å­˜åœ¨: {shapes_meta_path}")

    if output_path is None:
        meta_filename = Path(shapes_meta_path).stem
        if meta_filename.endswith('.shapes_meta'):
            model_name = meta_filename[:-len('.shapes_meta')]
        else:
            model_name = meta_filename
        output_path = f"/data1/{model_name}.runtime_manifest.json"

    print(f"ğŸ”„ ä»shapes_metaç”Ÿæˆmanifest...")
    print(f"   è¾“å…¥: {shapes_meta_path}")
    print(f"   è¾“å‡º: {output_path}")
    result = build_runtime_manifest(shapes_meta_path, output_path)
    print(f"âœ… Manifestç”ŸæˆæˆåŠŸ: {result}")

def generate_from_checkpoint(ckpt_path: str, raw_device: str,
                             meta_out: Optional[str] = None,
                             manifest_out: Optional[str] = None,
                             output_dir: Optional[str] = None,
                             header_reserve: int = 4*1024*1024,
                             auto_confirm: bool = False) -> None:
    print(f"âš ï¸  è­¦å‘Š: è¿™å°†ä¼šè¦†ç›– {raw_device} ä¸Šçš„ç°æœ‰æ•°æ®!")
    if not auto_confirm:
        confirm = input(f"ç¡®è®¤è¦ç»§ç»­å—? (yes/no): ")
        if confirm.lower() != 'yes':
            print("âŒ æ“ä½œå·²å–æ¶ˆ"); return

    model_name = extract_model_name(ckpt_path)
    print(f"\nğŸ”„ ä» checkpoint æ‰“åŒ…åˆ° raw è®¾å¤‡...")
    print(f"   Checkpoint: {ckpt_path}")
    print(f"   æ¨¡å‹åå­— : {model_name}")
    print(f"   Raw è®¾å¤‡ : {raw_device}")
    print(f"   å¤´éƒ¨ä¿ç•™: {header_reserve} bytes")

    if output_dir is None: output_dir = "/data1"
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    if meta_out is None:     meta_out = str(Path(output_dir) / f"{model_name}.shapes_meta.json")
    if manifest_out is None: manifest_out = str(Path(output_dir) / f"{model_name}.runtime_manifest.json")

    print(f"\nğŸ“¦ Step 1: æ‰“åŒ…æƒé‡åˆ°rawè®¾å¤‡...")
    shapes_meta_path = pack_any_to_raw(
        ckpt_path, raw_device,
        shapes_meta_out=meta_out,
        header_reserve_bytes=header_reserve
    )
    print(f"âœ… shapes_metaç”Ÿæˆ: {shapes_meta_path}")

    print(f"\nğŸ“ Step 2: ç”Ÿæˆruntime manifest...")
    build_runtime_manifest(shapes_meta_path, manifest_out)
    print(f"âœ… runtime_manifestç”Ÿæˆ: {manifest_out}")

def generate_template(raw_device: str, output_path: Optional[str] = None,
                     n_layers: int = 80, model_name: str = "llama-model") -> None:
    if output_path is None:
        output_path = f"/data1/{model_name}.runtime_manifest.json"
    print(f"ğŸ“ ç”Ÿæˆmanifestæ¨¡æ¿...")
    try:
        fd = os.open(raw_device, os.O_RDONLY | O_DIRECT | O_LARGEFILE)
        block_size = get_logical_block_size(fd); os.close(fd)
    except Exception:
        print(f"âš ï¸ æ— æ³•æ‰“å¼€è®¾å¤‡ {raw_device}ï¼Œç”¨é»˜è®¤å—å¤§å° 4096"); block_size=4096
    manifest = {"version":1,"raw_device":raw_device,"block_size":block_size,"header_reserve":4*1024*1024,"params":[]}
    Path(output_path).write_text(json.dumps(manifest, indent=2), encoding='utf-8')
    print(f"âœ… æ¨¡æ¿å·²ç”Ÿæˆ: {output_path}")

# -------------------------
# æ–°åŠŸèƒ½ï¼šfrom-shards-stream ï¼ˆé€å±‚åˆå¹¶ + ç›´æ¥æ‰“åŒ…ï¼‰
# -------------------------

ATTN_TAG_RE = re.compile(r"\.attention\.(wq|wk|wv|wo)\.weight$")
FFN_TAG_RE  = re.compile(r"\.feed_forward\.(w1|w2|w3)\.weight$")

def _iter_shards(dir_path: str) -> Iterable[str]:
    shards = sorted(glob.glob(str(Path(dir_path) / "consolidated.*.pth")))
    if not shards:
        # å…œåº•: *.pthï¼ˆæ’é™¤æˆ‘ä»¬è‡ªå·±å¯èƒ½ç”Ÿæˆçš„ consolidated.pthï¼‰
        shards = [p for p in sorted(glob.glob(str(Path(dir_path) / "*.pth"))) if Path(p).name != "consolidated.pth"]
    if not shards:
        raise FileNotFoundError(f"åœ¨ {dir_path} æœªæ‰¾åˆ° consolidated.*.pth åˆ†ç‰‡")
    return shards

def _concat_axis_hint(name: str, dim: int) -> int:
    # è¿”å›ä¼˜å…ˆæ‹¼æ¥è½´ï¼ˆ0æˆ–1ï¼‰ï¼›è‹¥é ATT/FFNï¼Œè¿”å› -1 äº¤ç»™è‡ªåŠ¨åˆ¤æ–­
    m = ATTN_TAG_RE.search(name)
    if m:
        tag = m.group(1)
        if tag in ("wq", "wk", "wv"): return 0
        else: return 1  # wo
    m = FFN_TAG_RE.search(name)
    if m:
        tag = m.group(1)
        if tag in ("w1", "w3"): return 0
        else: return 1  # w2
    return -1

def _streaming_iter_from_sharded_dir(dir_path: str) -> Iterable[Tuple[str, torch.Tensor]]:
    """
    é€å±‚ streamingï¼šæ¯æ¬¡åªåˆå¹¶ä¸€ä¸ªâ€œå±‚â€çš„æ‰€æœ‰å‚æ•°åˆ°å®Œæ•´çŸ©é˜µï¼Œç„¶å yield ç»™ pack å†™ rawï¼Œéšåé‡Šæ”¾å†…å­˜ã€‚
    å…ˆå¤„ç†å…¨å±€å‚æ•°ï¼ˆembed_tokens.weight / output.weightï¼‰ï¼Œå† 0..n_layers-1ã€‚
    """
    args = _load_model_args_from_dir(dir_path)
    DIM, N = args["dim"], args["n_layers"]
    shards = list(_iter_shards(dir_path))

    def collect_and_yield(keys_predicate):
        """
        å¯¹æ»¡è¶³ keys_predicate(name) çš„å‚æ•°ï¼Œè·¨æ‰€æœ‰ shard æ”¶é›†åˆ†ç‰‡å¹¶åˆå¹¶å yieldã€‚
        ä»…ä¿ç•™å°‘æ•°â€œå…¨å±€â€å‚æ•°ï¼Œæ•°é‡å¾ˆå°ï¼Œå†…å­˜å¯æ§ã€‚
        """
        buckets: Dict[str, Dict] = {}  # name -> {axis, parts: [T], fixed: DIM}
        def add_piece(name, t: torch.Tensor):
            n = _hf_to_internal_name(name)
            if not keys_predicate(n): return
            if t.ndim != 2:
                # é2Dï¼ˆä¾‹å¦‚ 1D LN æƒé‡ï¼‰ï¼Œè‹¥é‡å¤å‡ºç°å½¢çŠ¶ä¸€è‡´å°±å¿½ç•¥åç»­
                if n not in buckets: buckets[n] = {"axis": None, "parts":[t.detach().cpu()], "fixed": None}
                return
            axis = buckets.get(n, {}).get("axis")
            fixed = DIM
            if axis is None:
                # å†³å®šæ‹¼æ¥è½´ï¼šä¼˜å…ˆè§„åˆ™ï¼ˆè‹¥æ˜¯ Attn/FFNï¼‰ï¼›å¦åˆ™è‡ªåŠ¨ï¼šè°ç­‰äº DIMï¼Œå¦ä¸€ç»´æ‹¼æ¥
                hint = _concat_axis_hint(n, DIM)
                r, c = int(t.shape[0]), int(t.shape[1])
                if hint == 0:
                    if c == DIM: axis=0
                    elif r == DIM: t=t.T.contiguous(); axis=0
                    else: axis=0  # å°è¯•ç…§0è½´æ‹¼ï¼Œä¸‹é¢å†æ ¡éªŒ
                elif hint == 1:
                    if r == DIM: axis=1
                    elif c == DIM: t=t.T.contiguous(); axis=1
                    else: axis=1
                else:
                    # è‡ªåŠ¨ï¼šè°ç­‰äº DIMï¼Œå¦ä¸€ç»´æ‹¼æ¥
                    if c == DIM: axis=0
                    elif r == DIM: axis=1
                    elif r != DIM and c != DIM:
                        # å°è¯•è½¬ç½®
                        if r == DIM or c == DIM: t=t.T.contiguous(); r,c=c,r
                        if c == DIM: axis=0
                        elif r == DIM: axis=1
                        else:
                            # çœ‹èµ·æ¥æ˜¯å®Œæ•´çŸ©é˜µæˆ–å¤åˆ¶ï¼›ç›´æ¥å½“å®Œæ•´çŸ©é˜µå¯¹å¾…
                            buckets[n] = {"axis": None, "parts":[t.detach().cpu()], "fixed": None}
                            return
                buckets.setdefault(n, {"axis":axis, "parts":[], "fixed":fixed})
            else:
                # å¦‚æœéœ€è¦ï¼ŒæŒ‰å·²å®šè½´åšä¸€æ¬¡è½¬ç½®ï¼Œä¿è¯å›ºå®šè¾¹= DIM
                r, c = int(t.shape[0]), int(t.shape[1])
                if axis == 0 and c != DIM and r == DIM: t = t.T.contiguous()
                if axis == 1 and r != DIM and c == DIM: t = t.T.contiguous()
            buckets[n]["parts"].append(t.detach().cpu())

        # æ”¶é›†
        for sp in shards:
            sd = torch.load(sp, map_location="cpu")
            for k, v in sd.items():
                if isinstance(v, torch.Tensor): add_piece(k, v)
            del sd

        # åˆå¹¶å¹¶ yield
        for n, meta in buckets.items():
            parts, axis = meta["parts"], meta["axis"]
            if axis is None or len(parts) == 1:
                yield n, parts[0]
            else:
                cat = torch.cat(parts, dim=axis).contiguous()
                yield n, cat
        buckets.clear()

    # 1) å…ˆå¤„ç†å…¨å±€ï¼ˆä¸åœ¨ layers.* ä¸‹çš„ï¼‰
    def is_global_key(n: str) -> bool:
        return (not n.startswith("layers.")) and n.endswith(".weight")
    for name, t in collect_and_yield(is_global_key):
        yield name, t

    # 2) é€å±‚å¤„ç†
    for L in range(N):
        prefix = f"layers.{L}."
        def is_layer_key(n: str) -> bool:
            return n.startswith(prefix) and n.endswith(".weight")
        for name, t in collect_and_yield(is_layer_key):
            yield name, t

def generate_from_shards_stream(ckpt_dir: str, raw_device: str,
                                meta_out: Optional[str] = None,
                                manifest_out: Optional[str] = None,
                                output_dir: Optional[str] = None,
                                header_reserve: int = 4*1024*1024,
                                auto_confirm: bool = False) -> None:
    """
    å…³é”®ï¼šé€šè¿‡ monkey-patch å°† pack_any_to_raw çš„â€œç›®å½•è¿­ä»£å™¨â€æ›¿æ¢ä¸ºæˆ‘ä»¬ä¸Šé¢çš„ streaming ç”Ÿæˆå™¨ï¼Œ
    è¿™æ · pack å°±ä¼šæŒ‰â€œé€å±‚åˆå¹¶â†’ç«‹åˆ»å†™ rawâ€çš„æ–¹å¼å·¥ä½œï¼Œå³°å€¼å†…å­˜ â‰ª å…¨æ¨¡å‹ã€‚
    """
    print(f"âš ï¸  è­¦å‘Š: è¿™å°†ä¼šè¦†ç›– {raw_device} ä¸Šçš„ç°æœ‰æ•°æ®!")
    if not auto_confirm:
        confirm = input(f"ç¡®è®¤è¦ç»§ç»­å—? (yes/no): ")
        if confirm.lower() != 'yes':
            print("âŒ æ“ä½œå·²å–æ¶ˆ"); return

    args = _load_model_args_from_dir(ckpt_dir)
    print(f"ğŸ“‹ æ¨¡å‹é…ç½®: dim={args['dim']}, n_layers={args['n_layers']}, n_heads={args['n_heads']}, n_kv_heads={args['n_kv_heads']}, head_dim={args['head_dim']}")

    # monkey-patch
    import llama3.weights_io_ssd_dram as wio
    orig_iter_dir = wio._iter_tensors_from_dir
    def patched_iter_dir(dir_path: str):
        print("ğŸ©¹ ä½¿ç”¨ streaming åˆ†ç‰‡è¿­ä»£å™¨ï¼ˆé€å±‚åˆå¹¶ï¼‰...")
        return _streaming_iter_from_sharded_dir(dir_path)
    wio._iter_tensors_from_dir = patched_iter_dir

    # æ­£å¸¸è°ƒç”¨ pack_any_to_rawï¼ˆå®ƒä¼šä»æˆ‘ä»¬æ›¿æ¢çš„è¿­ä»£å™¨æŒ‰é¡ºåºæ‹¿ tensor å¹¶å†™ rawï¼‰
    model_name = extract_model_name(ckpt_dir)
    if output_dir is None: output_dir = "/data1"
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    if meta_out is None:     meta_out = str(Path(output_dir) / f"{model_name}.shapes_meta.json")
    if manifest_out is None: manifest_out = str(Path(output_dir) / f"{model_name}.runtime_manifest.json")

    print(f"\nğŸ“¦ Step 1: æ‰“åŒ…æƒé‡åˆ° raw è®¾å¤‡ï¼ˆé€å±‚ streamingï¼‰...")
    shapes_meta_path = pack_any_to_raw(
        ckpt_dir, raw_device,
        shapes_meta_out=meta_out,
        header_reserve_bytes=header_reserve
    )
    print(f"âœ… shapes_meta ç”Ÿæˆ: {shapes_meta_path}")

    print(f"\nğŸ“ Step 2: ç”Ÿæˆ runtime manifest ...")
    build_runtime_manifest(shapes_meta_path, manifest_out)
    print(f"âœ… runtime_manifest ç”Ÿæˆ: {manifest_out}")

    # è¿˜åŸåŸ iteratorï¼ˆå¯é€‰ï¼‰
    wio._iter_tensors_from_dir = orig_iter_dir

# -------------------------
# CLI
# -------------------------

def main():
    parser = argparse.ArgumentParser(description="ç”ŸæˆLLaMA3æ¨ç†ç³»ç»Ÿ manifestï¼ˆå« from-shards-streamï¼šé€å±‚åˆå¹¶æ‰“åŒ…ï¼‰",
                                     formatter_class=argparse.RawDescriptionHelpFormatter,
                                     epilog=__doc__)
    subparsers = parser.add_subparsers(dest='command', help='å‘½ä»¤', required=True)

    # from-meta
    pm = subparsers.add_parser('from-meta', help='ä» shapes_meta.json ç”Ÿæˆ runtime_manifest.json')
    pm.add_argument('shapes_meta', type=str)
    pm.add_argument('--out', type=str, default=None)

    # from-checkpoint
    pc = subparsers.add_parser('from-checkpoint', help='ä»å•æ–‡ä»¶ checkpoint æ‰“åŒ…å¹¶ç”Ÿæˆ manifest')
    pc.add_argument('checkpoint', type=str)
    pc.add_argument('raw_device', type=str)
    pc.add_argument('--output-dir', type=str, default=None)
    pc.add_argument('--meta-out', type=str, default=None)
    pc.add_argument('--manifest-out', type=str, default=None)
    pc.add_argument('--header-reserve', type=int, default=4*1024*1024)
    pc.add_argument('--yes', action='store_true')

    # template
    pt = subparsers.add_parser('template', help='ç”Ÿæˆ manifest æ¨¡æ¿')
    pt.add_argument('--raw-device', type=str, required=True)
    pt.add_argument('--out', type=str, default=None)
    pt.add_argument('--model-name', type=str, default='llama-model')
    pt.add_argument('--layers', type=int, default=80)

    # from-shards-stream  â˜…
    pss = subparsers.add_parser('from-shards-stream', help='ä» Meta åˆ†ç‰‡ç›®å½•é€å±‚ streaming åˆå¹¶å¹¶æ‰“åŒ…ï¼ˆä½å†…å­˜ï¼‰')
    pss.add_argument('checkpoint_dir', type=str, help='åŒ…å« consolidated.00..NN.pth ä¸ params.json çš„ç›®å½•')
    pss.add_argument('raw_device', type=str)
    pss.add_argument('--output-dir', type=str, default=None)
    pss.add_argument('--meta-out', type=str, default=None)
    pss.add_argument('--manifest-out', type=str, default=None)
    pss.add_argument('--header-reserve', type=int, default=4*1024*1024)
    pss.add_argument('--yes', action='store_true')

    args = parser.parse_args()

    try:
        if args.command == 'from-meta':
            generate_from_shapes_meta(args.shapes_meta, args.out)

        elif args.command == 'from-checkpoint':
            check_raw_device(args.raw_device)
            generate_from_checkpoint(args.checkpoint, args.raw_device,
                                     meta_out=args.meta_out, manifest_out=args.manifest_out,
                                     output_dir=args.output_dir, header_reserve=args.header_reserve,
                                     auto_confirm=args.yes)

        elif args.command == 'template':
            generate_template(args.raw_device, args.out, n_layers=args.layers, model_name=args.model_name)

        elif args.command == 'from-shards-stream':
            check_raw_device(args.raw_device)
            generate_from_shards_stream(args.checkpoint_dir, args.raw_device,
                                        meta_out=args.meta_out, manifest_out=args.manifest_out,
                                        output_dir=args.output_dir, header_reserve=args.header_reserve,
                                        auto_confirm=args.yes)

        print("\nâœ… æ“ä½œå®Œæˆ!")

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback; traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()
