# FFN è®¡ç®—çš„ Event ä¾èµ–è¯¦è§£

## é—®é¢˜ï¼šFFN è®¡ç®—éœ€è¦ç­‰å¾…å“ªäº› Eventï¼Ÿ

FFN è®¡ç®—ç¡®å®éœ€è¦æ»¡è¶³**ä¸¤ä¸ªå‰ç½®æ¡ä»¶**ï¼š
1. âœ… **MHA è®¡ç®—å®Œæˆ** (æ•°æ®ä¾èµ–ï¼šFFN çš„è¾“å…¥æ˜¯ MHA çš„è¾“å‡º)
2. âœ… **FFN æƒé‡å°±ç»ª** (æƒé‡ä¾èµ–ï¼šFFN éœ€è¦ w1/w2/w3 åœ¨ GPU ä¸Š)

ä½†æ˜¯ï¼Œ**è¿™ä¸¤ä¸ªä¾èµ–æ˜¯é€šè¿‡ä¸åŒçš„æœºåˆ¶å®ç°çš„**ï¼

---

## ä¸€ã€å®é™…ä»£ç ä¸­çš„å®ç°æ–¹å¼

### æ–¹å¼ 1: MHA â†’ FFN çš„æ•°æ®ä¾èµ– (éšå¼ï¼Œé€šè¿‡æµä¾èµ–)

```python
# EncoderBlock.forward() ä¸­çš„å®é™…è°ƒç”¨

# === Step 1: MHA è®¡ç®— ===
with torch.cuda.stream(streams.compute_mha):  # MHA è®¡ç®—æµ
    attn_out = self.attention(x, start_pos, freqs_complex)
    # â†‘ åœ¨ compute_mha æµä¸Šæ‰§è¡Œ
    # â†“ è¾“å‡º: attn_out (åœ¨ GPU ä¸Š)

# === Step 2: æ®‹å·®è¿æ¥ + Norm ===
h = x + attn_out  # è¿™ä¸€è¡Œåœ¨å“ªä¸ªæµä¸Šï¼Ÿ
# â†‘ å…³é”®ï¼šPyTorch è‡ªåŠ¨é€‰æ‹©æµ
#   - å¦‚æœ attn_out åœ¨ compute_mha æµä¸Šæœ€åè¢«å†™å…¥
#   - è¿™ä¸ªåŠ æ³•ä¼šéšå¼åœ°åœ¨ compute_mha æµä¸Šæˆ–é»˜è®¤æµä¸Šæ‰§è¡Œ
#   - GPU ä¼šè‡ªåŠ¨ç­‰å¾… attn_out å°±ç»ª

h = self.ffn_norm(h)

# === Step 3: FFN è®¡ç®— ===
with torch.cuda.stream(streams.compute_ffn):  # FFN è®¡ç®—æµ
    ffn_out = self.feed_forward(h)
    # â†‘ é—®é¢˜ï¼šcompute_ffn æµå¦‚ä½•çŸ¥é“ h å·²ç»å°±ç»ªï¼Ÿ
```

**å…³é”®ç‚¹**:
- **æ²¡æœ‰æ˜¾å¼çš„ Eventï¼**
- **PyTorch çš„é»˜è®¤è¡Œä¸º**: å½“ä½ åœ¨ä¸åŒæµä¹‹é—´ä¼ é€’ tensor æ—¶ï¼ŒPyTorch ä¼šè‡ªåŠ¨æ’å…¥éšå¼çš„æµåŒæ­¥
- **å…·ä½“æœºåˆ¶**: PyTorch å†…éƒ¨ç»´æŠ¤äº† tensor çš„"æœ€åå†™å…¥æµ"è®°å½•ï¼Œè¯»å–æ—¶ä¼šè‡ªåŠ¨ç­‰å¾…

---

### æ–¹å¼ 2: FFN æƒé‡ä¾èµ– (æ˜¾å¼ï¼Œé€šè¿‡ Event)

```python
# FeedForward.forward() ä¸­çš„å®é™…ä»£ç 
# llama3/layers.py:1366-1381

def forward(self, x: torch.Tensor) -> torch.Tensor:
    wm = getattr(self, "weight_manager", None)
    compute_stream = getattr(self.streams, "compute_ffn", None)

    # â­â­â­ å…³é”®ä»£ç ï¼šç­‰å¾… FFN æƒé‡å°±ç»ª
    stream = compute_stream or torch.cuda.current_stream()
    evt = None

    # å°è¯•è·å– FFN æƒé‡çš„ ready Event
    try:
        if wm is not None and hasattr(wm, "get_group_ready_event"):
            evt = wm.get_group_ready_event(self.layer_id, "ffn")
            #    â†‘ è¿”å› weight_h2d_ffn æµä¸Šè®°å½•çš„ Event
    except Exception:
        evt = None

    if evt is not None:
        stream.wait_event(evt)  # â† compute_ffn æµç­‰å¾… FFN æƒé‡ H2D Event
        # â†‘ è¿™é‡Œåªç­‰å¾…æƒé‡ï¼Œä¸ç­‰å¾… MHA è®¡ç®—ï¼
    else:
        # æç«¯å…œåº•ï¼šè°ƒç”¨ wait_group_ready (å†…éƒ¨ä¹Ÿæ˜¯ wait_event)
        if wm is not None and hasattr(wm, "wait_group_ready"):
            wm.wait_group_ready(self.layer_id, "ffn", compute_stream=stream)

    # === FFN è®¡ç®— ===
    gate = self.w1(x)  # ä½¿ç”¨ FFN æƒé‡
    up = self.w3(x)
    gate = F.silu(gate, inplace=True)
    up.mul_(gate)
    result = self.w2(up)

    return result
```

**å…³é”®ç‚¹**:
- **åªæ˜¾å¼ç­‰å¾… FFN æƒé‡ Event**
- **ä¸æ˜¾å¼ç­‰å¾… MHA è®¡ç®—å®Œæˆ** (PyTorch è‡ªåŠ¨å¤„ç†)

---

## äºŒã€ä¸ºä»€ä¹ˆ FFN ä¸éœ€è¦æ˜¾å¼ç­‰å¾… MHA Eventï¼Ÿ

### PyTorch çš„è‡ªåŠ¨ä¾èµ–è·Ÿè¸ªæœºåˆ¶

PyTorch å†…éƒ¨ä¸ºæ¯ä¸ª tensor ç»´æŠ¤äº†ä¸€ä¸ª"æœ€åå†™å…¥æµ"çš„è®°å½•ï¼š

```python
# PyTorch å†…éƒ¨ä¼ªä»£ç 

class Tensor:
    data: pointer_to_gpu_memory
    last_write_stream: cuda.Stream  # æœ€åå†™å…¥æ­¤ tensor çš„æµ
    last_write_event: cuda.Event    # æœ€åå†™å…¥å®Œæˆçš„ Event

# å½“ä½ åœ¨ä¸åŒæµä¸Šè¯»å– tensor æ—¶:
def read_tensor_in_stream(tensor, read_stream):
    if tensor.last_write_stream != read_stream:
        # è‡ªåŠ¨æ’å…¥ç­‰å¾…
        read_stream.wait_event(tensor.last_write_event)

    # ç°åœ¨å®‰å…¨è¯»å–
    return tensor.data
```

### å®é™…æ¡ˆä¾‹åˆ†æ

```python
# === MHA é˜¶æ®µ ===
with torch.cuda.stream(streams.compute_mha):
    attn_out = attention(x, ...)
    # PyTorch å†…éƒ¨è®°å½•:
    #   attn_out.last_write_stream = streams.compute_mha
    #   attn_out.last_write_event  = <Event recorded on compute_mha>

# === æ®‹å·®è¿æ¥ (å¯èƒ½åœ¨é»˜è®¤æµæˆ– compute_mha æµ) ===
h = x + attn_out
# PyTorch è‡ªåŠ¨å¤„ç†:
#   - æ£€æŸ¥ attn_out.last_write_stream != current_stream?
#   - å¦‚æœæ˜¯ â†’ è‡ªåŠ¨ current_stream.wait_event(attn_out.last_write_event)

h = self.ffn_norm(h)
# PyTorch è®°å½•:
#   h.last_write_stream = current_stream
#   h.last_write_event  = <Event recorded on current_stream>

# === FFN è®¡ç®— ===
with torch.cuda.stream(streams.compute_ffn):
    gate = self.w1(h)  # â† è¯»å– h
    # PyTorch è‡ªåŠ¨å¤„ç†:
    #   - æ£€æŸ¥ h.last_write_stream != streams.compute_ffn?
    #   - å¦‚æœæ˜¯ â†’ è‡ªåŠ¨ streams.compute_ffn.wait_event(h.last_write_event)
    #   - ç­‰ä»·äº: streams.compute_ffn ç­‰å¾… MHA è®¡ç®—å®Œæˆï¼
```

---

## ä¸‰ã€å®Œæ•´çš„ä¾èµ–å›¾

```
æ—¶é—´çº¿ (GPU è§†è§’):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

weight_h2d_mha æµ:
â”œâ”€ [0-6.74ms]   cudaMemcpyAsync(MHA weights) â”€â”€â”€â”€â”
â””â”€ [6.74ms]     cudaEventRecord(mha_h2d_evt)     â”‚
                                                  â”‚
compute_mha æµ:                                  â”‚
â”œâ”€ [0ms]        wait_event(mha_h2d_evt) â—„â”€â”€â”€â”€â”€â”€â”€â”€â”˜ [ä¾èµ–1: MHAæƒé‡]
â”œâ”€ [6.74-159ms] SDPA kernel (MHAè®¡ç®—) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€ [159ms]      <éšå¼Event: attn_outå®Œæˆ>       â”‚ [äº§å‡º: attn_out]
                                                  â”‚
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

weight_h2d_ffn æµ:
â”œâ”€ [6.74-20.28] cudaMemcpyAsync(FFN weights) â”€â”€â”
â””â”€ [20.28ms]    cudaEventRecord(ffn_h2d_evt)   â”‚
                                                â”‚
compute_ffn æµ:                                â”‚
â”œâ”€ [20.28ms]    wait_event(ffn_h2d_evt) â—„â”€â”€â”€â”€â”€â”€â”˜ [ä¾èµ–2: FFNæƒé‡]
â”‚                                               â”‚
â”œâ”€ [159ms]      <éšå¼ç­‰å¾… attn_out> â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ [ä¾èµ–3: MHAæ•°æ®]
â”‚               â†‘ PyTorch è‡ªåŠ¨æ’å…¥
â”‚               â†‘ ç­‰ä»·äº: wait_event(attn_out.last_write_event)
â”‚
â””â”€ [159-185ms]  FFN kernel (FFNè®¡ç®—)

å…³é”®è·¯å¾„: max(20.28, 159) = 159ms (MHAè®¡ç®—å®Œæˆæ—¶é—´)
```

---

## å››ã€ä¸¤ç§ä¾èµ–çš„å¯¹æ¯”

| ä¾èµ–ç±»å‹ | å®ç°æ–¹å¼ | åœ¨ä»£ç ä¸­çš„ä½ç½® | Eventç±»å‹ | æ˜¯å¦æ˜¾å¼ |
|---------|---------|---------------|----------|---------|
| **FFN æƒé‡ä¾èµ–** | `stream.wait_event(ffn_h2d_evt)` | FeedForward.forward():1377 | æ˜¾å¼ Event | âœ… æ˜¾å¼ |
| **MHA æ•°æ®ä¾èµ–** | PyTorch è‡ªåŠ¨è·Ÿè¸ª tensor | è‡ªåŠ¨æ’å…¥ | éšå¼ Event | âŒ éšå¼ |

### ä¸ºä»€ä¹ˆæƒé‡ä¾èµ–æ˜¯æ˜¾å¼çš„ï¼Ÿ

```python
# åŸå› ï¼šæƒé‡ä¸æ˜¯ tensor çš„"æœ€åå†™å…¥æµ"æ¦‚å¿µ
# æƒé‡æ˜¯é€šè¿‡ weight_manager ç®¡ç†çš„ï¼Œè·¨è¶Šå¤šä¸ªæµ

# WeightStreamingManager çš„æµç¨‹:
weight_h2d_ffn æµ: ä¼ è¾“ FFN æƒé‡ â†’ record(ffn_h2d_evt)
compute_ffn æµ:    ä½¿ç”¨ FFN æƒé‡ â† å¿…é¡»æ˜¾å¼ wait_event(ffn_h2d_evt)

# å¦‚æœä¸æ˜¾å¼ç­‰å¾…:
compute_ffn æµ:    self.w1(x)  â† å¯èƒ½è¯»åˆ°æœªä¼ è¾“å®Œæˆçš„æƒé‡ï¼
                   â†“ ç»“æœé”™è¯¯
```

### ä¸ºä»€ä¹ˆ MHA æ•°æ®ä¾èµ–æ˜¯éšå¼çš„ï¼Ÿ

```python
# åŸå› ï¼šattn_out æ˜¯æ™®é€š tensorï¼ŒPyTorch è‡ªåŠ¨è·Ÿè¸ª

# PyTorch çš„æœºåˆ¶:
compute_mha æµ:  out = attention(...) â†’ out.last_write_stream = compute_mha
compute_ffn æµ:  gate = w1(out)       â†’ è‡ªåŠ¨ wait_event(out.last_write_event)

# ä¸éœ€è¦æ‰‹åŠ¨å†™:
# compute_ffn.wait_event(mha_done_event)  â† ä¸éœ€è¦ï¼PyTorch è‡ªåŠ¨å¤„ç†
```

---

## äº”ã€å¦‚æœæˆ‘ä»¬æ˜¾å¼ç­‰å¾… MHA Event ä¼šæ€æ ·ï¼Ÿ

### æ–¹æ¡ˆ A: æ˜¾å¼è®°å½•å’Œç­‰å¾… MHA Event (å†—ä½™ä½†å®‰å…¨)

```python
# === MHA é˜¶æ®µ ===
with torch.cuda.stream(streams.compute_mha):
    attn_out = self.attention(x, ...)

# æ˜¾å¼è®°å½• MHA å®Œæˆ Event
mha_done_evt = torch.cuda.Event()
mha_done_evt.record(streams.compute_mha)

# === FFN é˜¶æ®µ ===
def forward(self, x):
    # ç­‰å¾… MHA æ•°æ®
    streams.compute_ffn.wait_event(mha_done_evt)  # â† å†—ä½™ï¼

    # ç­‰å¾… FFN æƒé‡
    ffn_h2d_evt = wm.get_group_ready_event(self.layer_id, "ffn")
    streams.compute_ffn.wait_event(ffn_h2d_evt)

    # FFN è®¡ç®—
    gate = self.w1(x)
    ...
```

**ç»“æœ**:
- âœ… **æ­£ç¡®æ€§**: å®Œå…¨æ­£ç¡®
- âš ï¸ **æ€§èƒ½**: ä¸å½“å‰å®ç°ç›¸åŒ (å› ä¸º PyTorch å·²ç»éšå¼ç­‰å¾…äº†)
- âŒ **å¤æ‚åº¦**: å¢åŠ äº†ä»£ç å¤æ‚åº¦
- ğŸ’¡ **å»ºè®®**: **ä¸å¿…è¦ï¼** PyTorch å·²ç»å¤„ç†äº†

---

### æ–¹æ¡ˆ B: å½“å‰å®ç° (åªæ˜¾å¼ç­‰å¾…æƒé‡ï¼Œéšå¼ç­‰å¾…æ•°æ®)

```python
# === FFN é˜¶æ®µ ===
def forward(self, x):
    # åªæ˜¾å¼ç­‰å¾… FFN æƒé‡
    ffn_h2d_evt = wm.get_group_ready_event(self.layer_id, "ffn")
    streams.compute_ffn.wait_event(ffn_h2d_evt)

    # MHA æ•°æ®ä¾èµ–ç”± PyTorch è‡ªåŠ¨å¤„ç†
    # (å½“ compute_ffn æµè¯»å– x æ—¶ï¼Œè‡ªåŠ¨ç­‰å¾… x.last_write_event)

    gate = self.w1(x)
    ...
```

**ç»“æœ**:
- âœ… **æ­£ç¡®æ€§**: å®Œå…¨æ­£ç¡® (PyTorch ä¿è¯)
- âœ… **æ€§èƒ½**: æœ€ä¼˜ (æ— å†—ä½™ç­‰å¾…)
- âœ… **å¤æ‚åº¦**: ç®€æ´
- ğŸ’¡ **å»ºè®®**: **è¿™æ˜¯æœ€ä½³å®è·µï¼**

---

## å…­ã€éªŒè¯ï¼šå¦‚ä½•ç¡®è®¤ PyTorch çš„è‡ªåŠ¨ä¾èµ–ï¼Ÿ

### å®éªŒ 1: ç¦ç”¨è‡ªåŠ¨ä¾èµ– (ä¼šå‡ºé”™)

```python
# é”™è¯¯ç¤ºä¾‹ï¼šå¼ºåˆ¶ç»•è¿‡ PyTorch çš„è‡ªåŠ¨ä¾èµ–

# MHA é˜¶æ®µ
with torch.cuda.stream(streams.compute_mha):
    attn_out = attention(x, ...)

# ç«‹å³åœ¨ FFN æµä¸Šä½¿ç”¨ (å¼ºåˆ¶è¯»å–æœªå®Œæˆçš„æ•°æ®)
with torch.cuda.stream(streams.compute_ffn):
    # ä½¿ç”¨åº•å±‚ API ç»•è¿‡ PyTorch
    raw_ptr = attn_out.data_ptr()
    # ç›´æ¥ä¼ ç»™ kernel (ä¸ç»è¿‡ PyTorch çš„ä¾èµ–æ£€æŸ¥)
    custom_kernel(raw_ptr, ...)  # â† å¯èƒ½è¯»åˆ°è„æ•°æ®ï¼
```

**ç»“æœ**: æ•°æ®ç«äº‰ï¼Œè¾“å‡ºé”™è¯¯

---

### å®éªŒ 2: éªŒè¯éšå¼ä¾èµ–å­˜åœ¨

```python
import torch

# åˆ›å»ºä¸¤ä¸ªæµ
s1 = torch.cuda.Stream()
s2 = torch.cuda.Stream()

# åœ¨ s1 ä¸Šå†™å…¥ tensor
with torch.cuda.stream(s1):
    a = torch.ones(1000, 1000, device='cuda')
    a *= 2  # å†™å…¥æ“ä½œ

# åœ¨ s2 ä¸Šè¯»å– tensor (ä¸æ˜¾å¼ wait_event)
with torch.cuda.stream(s2):
    b = a + 1  # PyTorch ä¼šè‡ªåŠ¨ç­‰å¾… a çš„å†™å…¥å®Œæˆ

# éªŒè¯ï¼šç»“æœåº”è¯¥æ˜¯ 3
torch.cuda.synchronize()
print(b[0, 0])  # è¾“å‡º: 3.0 (æ­£ç¡®ï¼è¯´æ˜è‡ªåŠ¨ç­‰å¾…äº†)
```

**ç»“è®º**: PyTorch ç¡®å®è‡ªåŠ¨å¤„ç†äº†è·¨æµçš„ tensor ä¾èµ–

---

## ä¸ƒã€æ€»ç»“

### FFN è®¡ç®—éœ€è¦ç­‰å¾…çš„ Eventï¼š

| Event | ç±»å‹ | å®ç°æ–¹å¼ | æ˜¯å¦å¿…é¡»æ˜¾å¼ç­‰å¾… |
|-------|------|---------|----------------|
| **FFN æƒé‡ H2D Event** | æƒé‡ä¾èµ– | `stream.wait_event(ffn_h2d_evt)` | âœ… **å¿…é¡»æ˜¾å¼** |
| **MHA è®¡ç®—å®Œæˆ Event** | æ•°æ®ä¾èµ– | PyTorch è‡ªåŠ¨è·Ÿè¸ª | âŒ **éšå¼å¤„ç†** |

### å…³é”®ç†è§£ï¼š

1. **æƒé‡ä¾èµ–å¿…é¡»æ˜¾å¼**
   - åŸå› : æƒé‡é€šè¿‡ WeightStreamingManager ç®¡ç†ï¼Œè·¨è¶Šå¤šä¸ªæµ
   - ä½ç½®: `FeedForward.forward()` å¼€å¤´
   - ä»£ç : `stream.wait_event(ffn_h2d_evt)`

2. **æ•°æ®ä¾èµ–è‡ªåŠ¨éšå¼**
   - åŸå› : PyTorch è‡ªåŠ¨è·Ÿè¸ª tensor çš„ `last_write_stream`
   - ä½ç½®: æ— éœ€å†™ä»£ç ï¼ŒPyTorch å†…éƒ¨å¤„ç†
   - æœºåˆ¶: è¯»å– tensor æ—¶è‡ªåŠ¨æ’å…¥ `wait_event()`

3. **å½“å‰å®ç°æ˜¯æœ€ä½³å®è·µ**
   - åªæ˜¾å¼ç­‰å¾…æƒé‡ Event
   - è®© PyTorch å¤„ç†æ•°æ®ä¾èµ–
   - ä»£ç ç®€æ´ï¼Œæ€§èƒ½æœ€ä¼˜

4. **å¦‚æœæ˜¾å¼ç­‰å¾… MHA Event**
   - ä¸ä¼šå‡ºé”™ (å†—ä½™ä½†å®‰å…¨)
   - ä¸ä¼šæå‡æ€§èƒ½ (PyTorch å·²ç»ç­‰å¾…äº†)
   - å¢åŠ ä»£ç å¤æ‚åº¦ (ä¸æ¨è)

### æœ€ç»ˆç­”æ¡ˆï¼š

**FFN è®¡ç®—ç¡®å®éœ€è¦ä¸¤ä¸ªå‰ç½®æ¡ä»¶éƒ½æ»¡è¶³ï¼Œä½†åªéœ€è¦æ˜¾å¼ç­‰å¾… FFN æƒé‡ Eventï¼ŒMHA æ•°æ®ä¾èµ–ç”± PyTorch è‡ªåŠ¨å¤„ç†ã€‚**

---

## å…«ã€æ—¶é—´çº¿è¯¦è§£

```
å®Œæ•´çš„ Layer 0 æ—¶é—´çº¿ (åŒ…å«éšå¼ä¾èµ–):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[0ms] Layer 0 å¼€å§‹

weight_h2d_mha:
â”œâ”€ [0-6.74]     H2D(MHA weights) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€ [6.74]       record(mha_h2d_evt)                â”‚
                                                    â”‚
compute_mha:                                       â”‚
â”œâ”€ [0]          wait_event(mha_h2d_evt) â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ä¾èµ–1: MHAæƒé‡
â”œâ”€ [6.74-159]   SDPA kernel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                â”‚
â”‚               attn_out.last_write_stream = compute_mha
â”‚               attn_out.last_write_event = <è‡ªåŠ¨è®°å½•>
â”‚                                                â”‚
â””â”€ [159]        MHA å®Œæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                                  â”‚
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

weight_h2d_ffn:
â”œâ”€ [6.74-20.28] H2D(FFN weights) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€ [20.28]      record(ffn_h2d_evt)          â”‚
                                              â”‚
compute_ffn:                                 â”‚
â”œâ”€ [20.28]      wait_event(ffn_h2d_evt) â—„â”€â”€â”€â”€â”˜ ä¾èµ–2: FFNæƒé‡ (æ˜¾å¼)
â”‚
â”œâ”€ [159]        <è¯»å– attn_out>
â”‚               PyTorch æ£€æµ‹: attn_out.last_write_stream != compute_ffn
â”‚               è‡ªåŠ¨æ’å…¥: wait_event(attn_out.last_write_event)
â”‚               â†“ ç­‰å¾… MHA å®Œæˆ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ä¾èµ–3: MHAæ•°æ® (éšå¼)
â”‚
â””â”€ [159-185]    FFN kernel

[185ms] Layer 0 å®Œæˆ

å…³é”®è·¯å¾„: max(20.28, 159) = 159ms
```

**æ€»ç»“**: FFN è®¡ç®—åœ¨ 159ms å¼€å§‹ï¼Œæ­¤æ—¶ï¼š
- âœ… FFN æƒé‡å·²å°±ç»ª (20.28ms < 159msï¼Œæ˜¾å¼ç­‰å¾…)
- âœ… MHA æ•°æ®å·²å°±ç»ª (159ms = MHAå®Œæˆæ—¶é—´ï¼Œéšå¼ç­‰å¾…)

ä¸¤ä¸ªä¾èµ–éƒ½æ»¡è¶³ï¼ŒFFN å®‰å…¨æ‰§è¡Œï¼
